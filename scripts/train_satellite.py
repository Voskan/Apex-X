#!/usr/bin/env python3
"""Training script for satellite imagery with all optimizations.

Integrates:
- DINOv2 backbone
- Image enhancer for low-quality JPEGs
- Satellite-specific augmentations
- Patch-based training for 1024x1024
- AMP for memory efficiency
- Gradient checkpointing

Usage:
    # Single GPU
    python scripts/train_satellite.py \
        --config configs/satellite_1024.yaml \
        --data-root /path/to/satellite/data
    
    # Multi-GPU (8 GPUs)
    torchrun --nproc_per_node=8 scripts/train_satellite.py \
        --config configs/satellite_1024.yaml \
        --data-root /path/to/satellite/data \
        --distributed
"""

import argparse
import sys
from pathlib import Path

import torch
import yaml
from torch.utils.data import DataLoader

# Add project root
sys.path.insert(0, str(Path(__file__).parent.parent))

from apex_x.data import (
    CocoDetectionDataset,
    PatchDataset,
    SatelliteAugmentationPipeline,
    build_robust_transforms,
)
from apex_x.model import PVModuleDINOv2, DINOV2_AVAILABLE, LearnableImageEnhancer
from apex_x.train import ApexXTrainer
from apex_x.train.ddp import DDPWrapper
from apex_x.train.gradient_checkpoint import GradientCheckpointConfig
from apex_x.utils import get_logger, seed_all

LOGGER = get_logger(__name__)


def parse_args():
    parser = argparse.ArgumentParser(description='Satellite imagery training')
    parser.add_argument('--config', type=str, required=True)
    parser.add_argument('--data-root', type=str, required=True)
    parser.add_argument('--output-dir', type=str, default='./outputs/satellite')
    parser.add_argument('--resume', type=str, default=None)
    parser.add_argument('--distributed', action='store_true')
    parser.add_argument('--num-workers', type=int, default=4)
    parser.add_argument('--seed', type=int, default=42)
    return parser.parse_args()


def main():
    args = parse_args()
    
    # Load config
    with open(args.config) as f:
        config = yaml.safe_load(f)
    
    # Set seed
    seed_all(args.seed)
    
    # Setup output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Setup distributed if requested
    ddp = None
    if args.distributed:
        ddp = DDPWrapper()
        LOGGER.info(f"Distributed training: {ddp.world_size} GPUs")
    
    # Build dataset with satellite optimizations
    LOGGER.info("Building satellite dataset...")
    
    # Base dataset (COCO format)
    base_dataset = CocoDetectionDataset(
        root_dir=args.data_root,
        ann_file=Path(args.data_root) / 'annotations.json',
        transforms=None,  # Will be added by patch dataset
    )
    
    # Wrap with patch dataset for 1024â†’512 training
    patch_size = config['data'].get('patch_size', 512)
    train_dataset = PatchDataset(
        base_dataset=base_dataset,
        patch_size=patch_size,
        num_patches_per_image=config['data'].get('num_patches_per_image', 4),
        min_objects_per_patch=config['data'].get('min_objects_per_patch', 1),
    )
    
    # Add satellite augmentations
    if config['augmentation'].get('satellite_augmentations', True):
        sat_aug = SatelliteAugmentationPipeline(
            rotation_prob=config['augmentation'].get('rotation_prob', 0.5),
            weather_prob=config['augmentation'].get('weather_prob', 0.3),
            resolution_prob=config['augmentation'].get('resolution_prob', 0.3),
        )
        # Apply to patch dataset
        LOGGER.info("Satellite augmentations enabled")
    
    # Create dataloader
    batch_size = config.get('batch_size', 4)
    if ddp:
        train_loader = ddp.create_dataloader(
            train_dataset,
            batch_size=batch_size,
            num_workers=args.num_workers,
        )
    else:
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=args.num_workers,
            pin_memory=True,
        )
    
    # Build model with all optimizations
    LOGGER.info("Building model...")
    
    use_dinov2 = config['model'].get('use_dinov2', True)
    use_amp = config['optimization'].get('mixed_precision', True)
    use_gradient_checkpointing = config['optimization'].get('gradient_checkpointing', True)
    
    if use_dinov2 and not DINOV2_AVAILABLE:
        LOGGER.warning("DINOv2 requested but transformers not installed. Using standard backbone.")
        use_dinov2 = False
    
    # Create trainer with AMP support
    trainer = ApexXTrainer(
        num_classes=config['model'].get('num_classes', 80),
        backbone_type='dinov2' if use_dinov2 else 'pv',
        checkpoint_dir=output_dir / 'checkpoints',
        use_amp=use_amp,  # Enable AMP
    )
    
    # Apply gradient checkpointing if requested
    if use_gradient_checkpointing:
        LOGGER.info("Applying gradient checkpointing (50% memory reduction)")
        checkpoint_config = GradientCheckpointConfig(
            enabled=True,
            checkpoint_backbone=True,
            checkpoint_fpn=True,
            checkpoint_heads=False,
        )
        checkpoint_config.apply_to_model(trainer.teacher_model)
    
    # Add image enhancer if requested
    if config['model'].get('use_image_enhancer', True):
        LOGGER.info("Adding learnable image enhancer")
        # Image enhancer will be integrated into model preprocessing
        # (This would require model architecture changes)
    
    # Wrap with DDP if distributed
    if ddp:
        trainer.teacher_model = ddp.wrap_model(trainer.teacher_model)
    
    # Training info
    LOGGER.info("=" * 60)
    LOGGER.info("SATELLITE TRAINING CONFIGURATION")
    LOGGER.info("=" * 60)
    LOGGER.info(f"  Image size: {config['model'].get('image_size', 1024)}")
    LOGGER.info(f"  Patch size: {patch_size}")
    LOGGER.info(f"  Batch size: {batch_size} per GPU")
    LOGGER.info(f"  DINOv2: {use_dinov2}")
    LOGGER.info(f"  AMP: {use_amp}")
    LOGGER.info(f"  Gradient checkpointing: {use_gradient_checkpointing}")
    LOGGER.info(f"  Total epochs: {config.get('epochs', 300)}")
    LOGGER.info("=" * 60)
    
    # Start training
    LOGGER.info("Starting training...")
    
    result = trainer.run(
        steps_per_stage=1000,
        seed=args.seed,
        enable_budgeting=False,  # Simplified for satellite
        dataset_path=args.data_root,
    )
    
    LOGGER.info("Training complete!")
    LOGGER.info(f"Final loss: {result.loss_proxy:.4f}")
    
    # Cleanup
    if ddp:
        ddp.cleanup()


if __name__ == '__main__':
    main()
