name: GPU Perf Regression

on:
  workflow_dispatch:
    inputs:
      run_mode:
        description: "Run target. Use self-hosted-gpu to execute benchmark; skip is default safe mode."
        required: true
        default: "skip"
        type: choice
        options:
          - skip
          - self-hosted-gpu
      baseline_path:
        description: "Path to stored GPU baseline JSON in repository."
        required: true
        default: "scripts/perf_baseline_gpu.json"
        type: string
      trt_engine_path:
        description: "Optional TensorRT engine path for end-to-end TRT benchmark."
        required: false
        default: ""
        type: string
      trt_plugin_lib:
        description: "Optional explicit TensorRT plugin shared library path."
        required: false
        default: ""
        type: string
      python_version:
        description: "Python version"
        required: true
        default: "3.11"
        type: string
  schedule:
    - cron: "0 4 * * *"

permissions:
  contents: read

concurrency:
  group: perf-gpu-${{ github.ref }}
  cancel-in-progress: false

jobs:
  skipped:
    if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.run_mode != 'self-hosted-gpu' }}
    runs-on: ubuntu-latest
    steps:
      - name: Skipped by default
        run: |
          echo "GPU perf workflow is skipped by default on public runners."
          echo "To run, dispatch with run_mode=self-hosted-gpu on a trusted self-hosted GPU runner."

  gpu-perf-regression:
    if: >-
      ${{
        (github.event_name == 'workflow_dispatch' && github.event.inputs.run_mode == 'self-hosted-gpu') ||
        (github.event_name == 'schedule' && vars.APEXX_ENABLE_GPU_NIGHTLY == 'true')
      }}
    runs-on: [self-hosted, linux, x64, gpu]
    timeout-minutes: 60
    env:
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      BASELINE_PATH: ${{ github.event.inputs.baseline_path || 'scripts/perf_baseline_gpu.json' }}
      TRT_ENGINE_PATH: ${{ github.event.inputs.trt_engine_path || '' }}
      TRT_PLUGIN_LIB: ${{ github.event.inputs.trt_plugin_lib || '' }}
      PYTHON_VERSION_INPUT: ${{ github.event.inputs.python_version || '3.11' }}
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_INPUT }}
          cache: pip

      - name: Install dependencies (GPU environment)
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Preflight GPU check
        run: |
          python - <<'PY'
          import torch
          print(f"torch={torch.__version__}")
          print(f"cuda_available={torch.cuda.is_available()}")
          if not torch.cuda.is_available():
              raise SystemExit("CUDA is required for GPU perf regression workflow")
          print(f"device_count={torch.cuda.device_count()}")
          print(f"device_name={torch.cuda.get_device_name(0)}")
          print(f"capability={torch.cuda.get_device_capability(0)}")
          PY

      - name: Run GPU perf regression compare
        env:
          APEXX_TRT_PLUGIN_LIB: ${{ env.TRT_PLUGIN_LIB }}
        run: |
          python scripts/perf_regression_gpu.py \
            --compare \
            --baseline "${BASELINE_PATH}" \
            --output artifacts/perf_gpu_current_ci.json \
            --summary artifacts/perf_gpu_compare_ci.json \
            --warmup 5 \
            --iters 20 \
            --trt-engine-path "${TRT_ENGINE_PATH}" \
            --trt-plugin-lib "${TRT_PLUGIN_LIB}"

      - name: Upload GPU perf artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-gpu-regression-artifacts
          path: |
            artifacts/perf_gpu_current_ci.json
            artifacts/perf_gpu_compare_ci.json
