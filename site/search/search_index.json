{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apex-X Documentation Apex-X v4 is a dynamic vision compute graph with utility-based routing, continuous budgeting, and deterministic inference for production-friendly export/runtime workflows. Core Documents Product Requirements Document: PRD Engineering Specification: Engineering Spec Quantization Policy: QAT / PTQ FP8 Precision Policy: FP8 Policy Performance Regression: PERF Runtime Plugin Specification: Runtime Plugin Spec Runtime Capability Detection: Runtime Caps Runtime Parity Framework: Runtime Parity TensorRT Runtime Notes: TensorRT Scaffolding TensorRT Build Guide: TensorRT Build/Harness Triton Runtime Notes: Triton Fused Ops Triton TilePack: Triton TilePack Kernel Triton TileUnpack: Triton TileUnpack Kernel Triton FusionGate: Triton FusionGate Kernel Triton TileSSM Scan: Triton TileSSM Baseline Triton Fused Stage-1: Triton Fused Stage-1 Project Context Memory: Context Architecture Decisions: Decisions Active Worklist: TODO Build Documentation Locally python -m venv .venv source .venv/bin/activate pip install -e '.[docs]' mkdocs build --strict mkdocs serve Notes mkdocs build --strict is enforced in CI. Authoritative architecture content lives in docs/PRD.md and docs/ENGINEERING_SPEC.md .","title":"Home"},{"location":"#apex-x-documentation","text":"Apex-X v4 is a dynamic vision compute graph with utility-based routing, continuous budgeting, and deterministic inference for production-friendly export/runtime workflows.","title":"Apex-X Documentation"},{"location":"#core-documents","text":"Product Requirements Document: PRD Engineering Specification: Engineering Spec Quantization Policy: QAT / PTQ FP8 Precision Policy: FP8 Policy Performance Regression: PERF Runtime Plugin Specification: Runtime Plugin Spec Runtime Capability Detection: Runtime Caps Runtime Parity Framework: Runtime Parity TensorRT Runtime Notes: TensorRT Scaffolding TensorRT Build Guide: TensorRT Build/Harness Triton Runtime Notes: Triton Fused Ops Triton TilePack: Triton TilePack Kernel Triton TileUnpack: Triton TileUnpack Kernel Triton FusionGate: Triton FusionGate Kernel Triton TileSSM Scan: Triton TileSSM Baseline Triton Fused Stage-1: Triton Fused Stage-1 Project Context Memory: Context Architecture Decisions: Decisions Active Worklist: TODO","title":"Core Documents"},{"location":"#build-documentation-locally","text":"python -m venv .venv source .venv/bin/activate pip install -e '.[docs]' mkdocs build --strict mkdocs serve","title":"Build Documentation Locally"},{"location":"#notes","text":"mkdocs build --strict is enforced in CI. Authoritative architecture content lives in docs/PRD.md and docs/ENGINEERING_SPEC.md .","title":"Notes"},{"location":"CONTEXT/","text":"Apex-X Project Context (Persistent Memory) Authoritative Links (Mandatory) PRD: docs/PRD.md Engineering spec: docs/ENGINEERING_SPEC.md Runtime plugin spec: docs/runtime/PLUGIN_SPEC.md Decisions log: docs/DECISIONS.md Active worklist: docs/TODO.md Project Identity Name: apex-x Version: 0.1.0 License: Apache-2.0 Current baseline: CPU-only reference implementation Current Architecture Snapshot Dual-stream concept established in docs (PV dense + FF sparse) Utility-based router contracts defined Continuous and deterministic budgeting contracts defined Quadtree nesting policy defined ( L0/L1/L2 ) TilePack/TileUnpack and ordering contracts defined Tile-SSM placeholder behavior defined What Exists Right Now (2026-02-07) Repository scaffold created: apex_x/ , tests/ , docs/ , docs/runtime/ , examples/ , scripts/ , runtime/ , .github/workflows/ Governance/Open-source files added: LICENSE , CODE_OF_CONDUCT.md , CONTRIBUTING.md , SECURITY.md Authoritative docs added/updated: docs/PRD.md (full) docs/ENGINEERING_SPEC.md (full) docs/runtime/PLUGIN_SPEC.md CPU baseline code added: apex_x/config/schema.py apex_x/routing/core.py apex_x/tiles/ops.py apex_x/utils/ssm.py apex_x/model/core.py Validation assets added: tests/test_router.py tests/test_tile_ops.py tests/test_model.py scripts/perf_regression.py .github/workflows/ci.yml Tooling and developer workflow baseline: pyproject.toml now targets python>=3.11 runtime deps: torch , numpy , typer , rich , pydantic dev deps/tools: pytest , ruff , black , mypy , pre-commit pre-commit hooks: .pre-commit-config.yaml CI now runs lint + typecheck + tests on ubuntu-latest with CPU torch index Package skeleton and public API surfaces: Created package layout: apex_x/config/ , apex_x/model/ , apex_x/tiles/ , apex_x/routing/ , apex_x/losses/ , apex_x/train/ , apex_x/infer/ , apex_x/data/ , apex_x/export/ , apex_x/bench/ , apex_x/runtime/ , apex_x/utils/ Root API now exports required surfaces in apex_x/__init__.py : ApexXConfig , ApexXModel Router , BudgetController TilePack , TileUnpack Exporter Added import smoke coverage in tests/test_import_smoke.py Migrated baseline code into package modules and removed legacy flat modules to avoid namespace ambiguity Nested configuration system implemented: New nested config domains in apex_x/config/schema.py : ModelConfig (profiles, channels, strides, tile sizes, Kmax, nesting depth) RoutingConfig (budgets B/B1/B2/B3, costs, hysteresis/split thresholds) TrainConfig (curriculum, dual- mu parameters, distill weights, PCGrad++, QAT toggles) DataConfig (COCO paths and augmentation knobs) RuntimeConfig (precision profile, export/runtime toggles) Top-level ApexXConfig now nests all sections and performs cross-section validation Added YAML + CLI-style override support in apex_x/config/io.py : load_yaml_config(path, overrides=...) apply_overrides(cfg, [\\\"section.key=value\\\", ...]) Added config validation test coverage in tests/test_config.py + fixture tests/fixtures/apex_x_config.yaml Updated model to consume nested config fields in apex_x/model/core.py Added PyYAML + types-PyYAML to project dependencies for runtime + typing support Reproducibility and logging utilities implemented: Added apex_x/utils/repro.py : seed_all() set_deterministic_mode() deterministic_mode() context manager get_determinism_state() reproducibility_notes() (CPU vs CUDA behavior notes) Added apex_x/utils/logging.py : configure_logging() get_logger() shared apex_x.* logger namespace log_event() structured key/value logging with rich Wired shared logger usage in: apex_x/config/io.py apex_x/model/core.py Added determinism tests in tests/test_repro.py CLI surface implemented: Added Typer CLI entrypoint in apex_x/cli.py with commands: apex-x train apex-x eval apex-x predict apex-x bench apex-x ablate apex-x export All commands load config via YAML and support repeated --set section.key=value overrides Added console script entrypoint in pyproject.toml : [project.scripts] apex-x = \\\"apex_x.cli:main\\\" Added CLI parsing/behavior tests in tests/test_cli.py Documentation scaffold implemented: Added MkDocs config in mkdocs.yml Added docs home page in docs/index.md linking PRD/spec/runtime/context/decisions/TODO Added docs build instructions in docs/index.md and README.md Added docs dependency group in pyproject.toml : .[docs] with mkdocs Added CI docs build job in .github/workflows/ci.yml running: mkdocs build --strict Protocol typing standardization implemented: Added explicit protocol names and aliases for consistency: RouterProtocol BudgetControllerProtocol TilePackerProtocol RuntimeAdapterProtocol Kept backward-compatible aliases in existing modules ( Router , BudgetController , TilePack , etc.) Added runtime adapter interface + reference adapter: apex_x/runtime/interfaces.py apex_x/runtime/adapters.py ( NullRuntimeAdapter ) Updated model typing to consume protocol-based interfaces in apex_x/model/core.py Added minimal protocol-conformance tests: tests/test_protocols.py Updated import-smoke expectations in tests/test_import_smoke.py CPU smoke example added: Added examples/smoke_cpu.py that: loads YAML config instantiates ApexXModel stub runs one forward pass on random input Added examples/smoke_cpu.yaml default config for fast CPU smoke runs Added tests/test_smoke_cpu_example.py as a quick smoke pytest Documentation governance updates: Added initial convention ADRs in docs/DECISIONS.md for: naming conventions tensor shape contracts determinism rules Expanded docs/TODO.md with known future implementation tracks: full Triton fused kernels full TensorRT plugin stack ONNX Runtime custom-op sparse path and parity gates Strengthened CONTRIBUTING.md policy to require: docs/CONTEXT.md update in every significant PR docs/DECISIONS.md update when architectural/convention decisions change L0 tiling mapping implemented and validated: Added apex_x/tiles/mapping.py with explicit L0 mapping API: l0_grid_shape(feature_h, feature_w, tile_size) with strict divisibility checks l0_tile_to_index(ty, tx, grid_h, grid_w) and l0_index_to_tile(index, grid_h, grid_w) with bounds checks batched helpers: l0_indices_to_coords(indices[B,K], grid_h, grid_w) -> coords[B,K,2] l0_coords_to_indices(coords[B,K,2], grid_h, grid_w) -> indices[B,K] Wired tile ops grid sizing to strict mapping validation: apex_x/tiles/ops.py::tile_grid_shape now uses l0_grid_shape(...) Exported mapping API through apex_x/tiles/__init__.py Added focused tests in tests/test_tile_mapping.py : index/coord bijection batched [B,K] roundtrip invalid size/divisibility out-of-bounds and shape/dtype validation Verification status: python -m pytest -q passed ruff check . passed mypy passed Hilbert ordering implemented for coordinates and indices: Added apex_x/tiles/ordering.py with explicit Hilbert APIs: hilbert_distance(tx, ty, order_n) hilbert_order_coords(grid_h, grid_w) for full-grid coordinate traversal hilbert_order_indices(indices, grid_h, grid_w) for subset index ordering hilbert_full_indices(grid_h, grid_w) for complete index traversal Updated apex_x/tiles/ops.py : order_idx(..., mode=\\\"hilbert\\\") now uses hilbert_order_indices(...) Exported ordering APIs from apex_x/tiles/__init__.py Added fixtures: tests/fixtures/hilbert_2x2.json tests/fixtures/hilbert_4x4.json tests/fixtures/hilbert_8x8.json Added fixture-driven tests in tests/test_tile_hilbert.py : exact traversal match vs fixtures determinism across repeated calls full coverage of all coordinates/indices subset index ordering stability + parity with order_idx(..., mode=\\\"hilbert\\\") Verification status: python -m pytest -q passed ruff check . passed mypy passed Scan ordering variants and stable dispatcher implemented: Extended apex_x/tiles/ordering.py with scan modes and dispatcher utilities: Scan variants: l2r , r2l , u2d , d2u Stable dispatcher: order_tile_indices(indices, grid_h, grid_w, mode=...) Mode normalization and aliases: normalize_order_mode(...) supports scan_lr/scan_rl/scan_ud/scan_du + short aliases + canonical names Scan inverse mapping helper: inverse_scan_mode(...) Explicit scan ordering APIs: scan_order_coords(grid_h, grid_w, mode) scan_order_indices(indices, grid_h, grid_w, mode) Updated apex_x/tiles/ops.py : order_idx(...) now delegates to order_tile_indices(...) (single path for ordering semantics) Exported new ordering APIs from apex_x/tiles/__init__.py Added tests in tests/test_tile_scan_ordering.py : deterministic ordering for all scan variants alias/normalization correctness stable ordering behavior on duplicate indices reversible mapping checks: L2R <-> R2L by horizontal mirror U2D <-> D2U by vertical mirror dispatcher parity with order_idx(...) Verification status: python -m pytest -q passed ruff check . passed mypy passed L0->L1 quadtree mapping and metadata implemented: Added apex_x/tiles/quadtree.py with deterministic L0/L1 mapping APIs: l1_grid_shape_from_l0(l0_grid_h, l0_grid_w) l0_l1_grid_shapes_from_feature(feature_h, feature_w, tile_size_l0, tile_size_l1) l0_to_l1_children_coords(l0_ty, l0_tx, l0_grid_h, l0_grid_w) (TL, TR, BL, BR order) l0_to_l1_children_indices(l0_index, l0_grid_h, l0_grid_w) reverse mapping: l1_to_l0_parent_coord(l1_ty, l1_tx, l0_grid_h, l0_grid_w) l1_to_l0_parent_index(l1_index, l0_grid_h, l0_grid_w) metadata builder: build_l0_l1_quadtree_meta(parent_indices, l0_grid_h, l0_grid_w) -> L0L1QuadtreeMeta Exported new quadtree APIs in apex_x/tiles/__init__.py Added tests in tests/test_tile_quadtree.py : boundary tile mapping correctness (bottom-right L0 tile to L1 children) reverse parent mapping across full L1 grids multiple config coverage via (feature_h, feature_w, tile_size_l0, tile_size_l1) parametrization metadata shape/content checks invalid ratio/divisibility/out-of-bounds validation checks Verification status: python -m pytest -q passed ruff check . passed mypy passed L2 nesting and overlap priority contract implemented: Extended apex_x/tiles/quadtree.py with L1->L2 and combined depth-2 utilities: grid shapes: l2_grid_shape_from_l1(...) l1_l2_grid_shapes_from_feature(...) l0_l1_l2_grid_shapes_from_feature(...) mappings: l1_to_l2_children_coords(...) , l1_to_l2_children_indices(...) l2_to_l1_parent_coord(...) , l2_to_l1_parent_index(...) l0_to_l2_descendant_indices(...) metadata: L1L2QuadtreeMeta L0L1L2QuadtreeMeta build_l1_l2_quadtree_meta(...) build_l0_l1_l2_quadtree_meta(...) Defined explicit overlap priority tags and helper: OVERLAP_PRIORITY_L0 = 1 OVERLAP_PRIORITY_L1 = 2 OVERLAP_PRIORITY_L2 = 3 overlap_priority_for_level(...) contract enforces L2 > L1 > L0 Exported new L2 and priority APIs through apex_x/tiles/__init__.py Added/expanded tests: tests/test_tile_quadtree.py : L1->L2 mapping correctness (including boundary tiles) L2->L1 reverse mapping correctness L0->L2 descendant index correctness combined L0/L1/L2 metadata consistency priority tag contract checks multi-config and validation coverage for depth-2 shapes tests/test_tile_ops.py : overlap behavior check using unpack priorities confirming L2 overrides L1 and L0 Verification status: python -m pytest -q passed ruff check . passed mypy passed Tile selection debug dataclasses implemented: Added apex_x/tiles/selection.py with: TileSelection fields: level , indices , ordered_indices , meta , budgets_used validation: level constrained to l0/l1/l2 ordered_indices must be a permutation of indices budgets must be finite and non-negative JSON persistence: to_dict()/from_dict() save_json()/load_json() TileSelectionTrace for multi-level selection records fields: selections , run_meta helpers: to_dict()/from_dict() save_json()/load_json() for_level(level) JSON serialization includes recursive normalization of NumPy arrays/scalars for debug/ablation dumps. Exported APIs via apex_x/tiles/__init__.py : TileSelection TileSelectionTrace Added unit tests in tests/test_tile_selection.py : roundtrip dict serialization file save/load JSON validation error cases trace roundtrip and level lookup non-empty trace guard Verification status: python -m pytest -q passed ruff check . passed mypy passed Tile overlay visualization utility implemented: Added apex_x/utils/visualization.py with dependency-free overlay rendering: draw_selected_tiles_overlay(...) supports HWC , CHW , and batch-size-1 image inputs deterministic tile overlay rendering from selected tile indices + grid/tile size fill + border blending with deterministic integer alpha math save_overlay_ppm(...) saves overlay to .ppm for debug/ablation without extra image libraries draw_and_save_selected_tiles_overlay(...) convenience wrapper combining render + save Exported visualization utilities through apex_x/utils/__init__.py Added deterministic tests in tests/test_visualization.py : overlay output shape/dtype checks stable SHA256 hash checks for rendered overlay bytes stable SHA256 hash checks for saved PPM file bytes save-path extension validation for .ppm Verification status: python -m pytest -q passed ruff check . passed mypy passed Cost model interface and reference implementation added: Extended routing interfaces in apex_x/routing/interfaces.py : CostModelProtocol backward-compatible alias CostModel Added apex_x/routing/cost_model.py : LevelCost : per-level cost terms: c_cheap (C_c) c_heavy (C_h) pack_overhead unpack_overhead split_overhead (O_split) CalibrationRecord : stores empirical calibration measurements, blend factor, and apply flag StaticCostModel : level-aware cost computations: cheap_cost(...) heavy_cost(...) delta_cost(...) split_overhead(...) expected_level_cost(...) total_cost(...) optional empirical calibration hook: apply_empirical_calibration(level, measured_timings, blend, apply) stores records in calibration_history serialization: to_dict()/from_dict() save_json()/load_json() Exported cost model symbols through: apex_x/routing/__init__.py apex_x/__init__.py (public API now includes CostModel , CostModelProtocol , StaticCostModel ) Added tests in tests/test_cost_model.py : deterministic cost computations and totals calibration update behavior and history storage JSON serialization roundtrip validation/error paths protocol conformance check ( isinstance(..., CostModelProtocol) ) Verification status: python -m pytest -q passed ruff check . passed mypy passed Oracle set sampler implemented ( S sampling for utility oracle training): Added apex_x/routing/oracle_sampling.py : OracleSetSample dataclass: indices random_indices uncertainty_indices sample_oracle_set(u_hat, random_fraction, uncertainty_fraction, seed) : random fraction sampling over all tiles uncertainty-biased sampling over remaining tiles using PV uncertainty u_hat deterministic behavior under fixed seed Exported sampler APIs through apex_x/routing/__init__.py : OracleSetSample sample_oracle_set Added tests in tests/test_oracle_sampling.py : seed determinism checks count/uniqueness checks for random + uncertainty components uncertainty-biased distribution check across many seeds validation/error checks for fraction bounds and invalid uncertainty values Verification status: python -m pytest -q passed ruff check . passed mypy passed Stable tie-breaking helper for selections implemented: Updated apex_x/routing/core.py : added stable_rank_tile_ids(scores) with deterministic ordering policy: primary key: score descending secondary key: tile id ascending wired greedy_utility_per_cost(...) to use stable_rank_tile_ids(...) for score-ratio ranking Exported helper via apex_x/routing/__init__.py : stable_rank_tile_ids Added tests in tests/test_router.py : explicit tie-break behavior validation repeat-run determinism checks integration check that greedy selection follows stable tie-breaking under equal utility/cost ratios Verification status: python -m pytest -q passed ruff check . passed mypy passed PV->FF tile vector aggregation implemented: Added apex_x/routing/aggregation.py : compute_ff_tile_bounds_on_pv_grid(...) computes PV-aligned bounds for each FF tile on coarse PV maps aggregate_pv_maps_to_ff_tile_vectors(...) pools per-tile stats from PV maps aligned to FF grid supported stats: mean , max , var deterministic feature ordering using sorted PV map names PVTileAggregation dataclass: vectors ( [B, K, D] ) tile_bounds_pv ( [K,4] ) feature_layout (feature names per channel/stat/map) Exported aggregation APIs via apex_x/routing/__init__.py : PVTileAggregation compute_ff_tile_bounds_on_pv_grid aggregate_pv_maps_to_ff_tile_vectors Added tests in tests/test_pv_aggregation.py : alignment on simple integer scale mapping pooled stat correctness ( mean/max/var ) output shape and feature layout checks deterministic outputs across map ordering non-integer scale edge/boundary alignment validation error paths Verification status: python -m pytest -q passed ruff check . passed mypy passed RouterTinyMLP implemented with utility/split/temporal outputs: Added apex_x/routing/tiny_mlp.py : RouterTinyOutput dataclass carrying per-tile tensors: U ( [B,K] ) utility logits S ( [B,K] ) split utility logits optional T ( [B,K] ) temporal keep logits RouterTinyMLP(nn.Module) : configurable input_dim , hidden_dim , num_layers , temporal_head forward contract on x[B,K,D] with strict input validation compatibility method predict_utilities(...) for RouterProtocol usage ( input_dim=1 ) Exported in apex_x/routing/__init__.py : RouterTinyOutput RouterTinyMLP Added tests in tests/test_router_tiny_mlp.py : shape checks for U/S and optional T deterministic outputs for fixed seed/model initialization backward/gradient-flow checks through router outputs predict_utilities(...) behavior and validation paths runtime protocol conformance ( isinstance(..., RouterProtocol) ) Minor typing fix to keep strict typecheck green: apex_x/utils/visualization.py now uses an explicit typed cast in _as_hwc_uint8(...) Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Lightweight spline activation + RouterKANLike implemented: Added apex_x/routing/kan_like.py : LightweightSplineActivation : compact per-feature piecewise-linear spline on fixed knot grid identity initialization for stable startup explicit nan/inf sanitization + bounded input clamp RouterKANLike : small-parameter KAN-like router ( LayerNorm -> Linear -> Spline -> Linear ) outputs U , S , and optional T via RouterKANOutput bounded head logits via configurable logit_clip protocol-compatible predict_utilities(...) and parameter_count() Exported via apex_x/routing/__init__.py : LightweightSplineActivation RouterKANOutput RouterKANLike Added numerical stability tests in tests/test_router_kan_like.py : finite outputs under extreme/nonnumeric input values finite gradients for spline params and inputs router output shape + finite output checks at large input magnitudes deterministic initialization/output checks under fixed seeds small-parameter-count guard RouterProtocol conformance check Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Tensor STE gating implemented for continuous-budget training path: Added apex_x/routing/gating.py : sigmoid_probabilities(U, temperature) : computes p = sigmoid(U) with temperature support clamps/sanitizes extreme and non-finite logits for numerical stability ste_hard_gate(p, mode, threshold) : forward hard gate modes: threshold : g = 1[p >= threshold] bernoulli : g ~ Bernoulli(p) backward straight-through estimator via detach trick ( dg/dp = 1 ) ste_gate_from_utilities(U, ...) -> (p, g) convenience function Exported in apex_x/routing/__init__.py : GateMode sigmoid_probabilities ste_hard_gate ste_gate_from_utilities Added autograd and numerical tests in tests/test_ste_gating.py : non-zero and finite gradient checks w.r.t. U in threshold mode non-zero and finite gradient checks w.r.t. U in Bernoulli mode explicit gradient-form check against sigmoid derivative under linear loss finite probability checks under extreme/non-finite utility inputs Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed BudgetDualController implemented for continuous-budget dual optimization: Added apex_x/routing/dual_budget.py : BudgetDualController (stateful dual variable controller) with: expected cost: E[C] = sum_i(p_i * C_h + (1 - p_i) * C_c) via expected_cost(...) budget term: L_budget = mu * (E[C] - B) via budget_loss(...) projected/clamped dual update: mu <- clamp(mu + mu_lr * (E[C] - B), [mu_min, mu_max]) via update_mu(...) structured debug logging on each update ( event='dual_mu_update' ) including: expected_cost , budget , delta , mu_prev , mu_next , clamped support for both sequence and tensor probabilities in expected-cost path Exported in apex_x/routing/__init__.py : BudgetDualController Added tests in tests/test_budget_dual_controller.py : expected-cost and budget-loss formula checks mu moves in correct direction ( E[C] > B increases, E[C] < B decreases) mu clamp bounds respected ( mu_min / mu_max ) tensor-path budget loss backpropagates with finite non-zero gradients Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Deterministic greedy inference budgeting implemented with explicit Kmax buffer contract: Added apex_x/routing/inference_budget.py : deterministic_greedy_selection(...) : computes scores exactly as score_i = U_i / DeltaC_i deterministic ordering by: primary: score descending secondary: tile id ascending selects until budget exhausted and kmax reached returns GreedySelectionResult with: selected_indices spent_budget ordered_candidates scores kmax_buffer (fixed-length padded buffer) valid_count build_kmax_buffer(...) helper for fixed-size runtime buffers Wired existing public helper to the new deterministic path: apex_x/routing/core.py::greedy_utility_per_cost(...) now delegates to deterministic_greedy_selection(...) and preserves existing return signature Exported in apex_x/routing/__init__.py : GreedySelectionResult build_kmax_buffer deterministic_greedy_selection Added tests in tests/test_inference_budget.py : repeat-run determinism for ordering and selections budget enforcement and kmax cap behavior stable tie handling ( tile_id ascending under equal scores) Kmax buffer padding/truncation semantics Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Two-stage nesting selection implemented ( L0 under B1 , split under B2 ): Extended apex_x/routing/inference_budget.py : TwoStageSelectionResult dataclass carrying: l0 selection result split parent ordering/selection spent split budget generated L1 indices ordered L1 indices + fixed-size L1 Kmax buffer deterministic_two_stage_selection(...) : stage 1: deterministic L0 selection using U / DeltaC under budget_b1 + kmax_l0 stage 2: split parent ranking by S / O_split under budget_b2 expands selected L0 parents to L1 children via quadtree mapping enforces kmax_l1 capacity during split expansion applies deterministic L1 ordering via configured order mode (Hilbert/scan) Exported in apex_x/routing/__init__.py : TwoStageSelectionResult deterministic_two_stage_selection Added tests in tests/test_two_stage_selection.py : L0 selection under B1 split candidate selection under B2 with S/O_split deterministic tie handling on split parents ( tile_id ascending) L1 children generation and ordering behavior determinism across repeated runs kmax_l1 capacity/buffer enforcement Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Temporal hysteresis rollout and toggle analysis implemented: Extended apex_x/routing/core.py hysteresis APIs: hysteresis_update(...) now validates: theta_on > theta_off equal lengths for utilities_t and prev_mask added hysteresis_rollout(...) : applies rule over full time sequence with carried z(t-1) state added count_mask_toggles(...) : counts total 0/1 transitions across time (for anti-flicker evaluation) Exported in apex_x/routing/__init__.py : hysteresis_rollout count_mask_toggles Added tests in tests/test_hysteresis_temporal.py : deadband behavior preserves previous mask state ( z(t-1) ) when utility remains between thresholds synthetic noisy sequence shows reduced toggling vs single-threshold baseline validation/error-path checks for threshold ordering and shape consistency Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Routing diagnostics implemented and surfaced in model/train/infer paths: Added apex_x/routing/diagnostics.py : utility_histogram(...) for per-level utility histogram summaries build_routing_diagnostics(...) producing: selected ratios/counts per level utility histograms per level budget usage ( used , budget , ratio ) dual variable history ( mu_history ) Exported diagnostics APIs through apex_x/routing/__init__.py Integrated diagnostics into apex_x/model/core.py : model outputs now include routing_diagnostics dual controller ( BudgetDualController ) state tracked in mu_history optional dual update path enabled via forward(..., update_dual=True) structured logs now include selected ratio, budget usage ratio, and latest mu Updated apex_x/train/__init__.py and apex_x/infer/__init__.py : train placeholder logs/returns routing diagnostics summary fields infer placeholder returns diagnostics from model outputs Updated CLI integration in apex_x/cli.py : train now performs short warmup forwards with dual updates and reports diagnostics predict now reads infer diagnostics and logs budget usage ratio Added tests in tests/test_routing_diagnostics.py : diagnostics presence in inference outputs diagnostics propagation through train/infer placeholders mu_history progression when dual updates are enabled Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Config-driven feature toggles added for forced-off routing/training paths: Extended apex_x/config/schema.py : ModelConfig toggles: force_dense_routing (router off => dense tile activation) disable_nesting (effective nesting depth forced to 0) disable_ssm (bypass Tile-SSM mixing) TrainConfig toggles: disable_distill disable_pcgradpp Added helper methods: ModelConfig.effective_nesting_depth() ModelConfig.router_enabled() ModelConfig.ssm_enabled() TrainConfig.distill_enabled() TrainConfig.pcgradpp_enabled() Validation updated so disable_nesting=true can force nesting off without requiring manual kmax_l1/l2 and budget_b3 cleanup. Updated apex_x/model/core.py runtime behavior: router-off mode forces dense L0 selection and skips budget controller routing selection no-SSM mode bypasses tile_ssm_scan(...) and uses zero modulation/state no-nesting mode uses effective depth for diagnostics totals ( L1/L2 counts become zero) output now includes feature_toggles summary for debug/smoke assertions Updated apex_x/train/__init__.py + apex_x/cli.py : train placeholder now accepts config and reports effective distill/PCGrad++ enabled flags CLI train passes config through to preserve toggle behavior in logs/output paths Added smoke coverage: tests/test_feature_toggle_smoke.py validates override behavior for disable_nesting executes all 2^5=32 toggle combinations: router off / on no nesting / nesting no SSM / SSM no distill / distill no PCGrad++ / PCGrad++ asserts forward + train placeholder run without crashes and toggle semantics hold Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Torch tile packer implemented with contiguous output contract: Added apex_x/tiles/torch_ops.py : TilePackTorch.pack(...) : input: F[B,C,H,W] , idx[B,K] , tile_size output: P[B,K,C,t,t] , meta deterministic ordering via shared ordering dispatcher ( order_idx(...) ) strict shape/dtype/bounds validation guarantees contiguous packed tensor via .contiguous() pack_tiles_torch(...) convenience wrapper TorchTileMeta type alias ( dict[str, torch.Tensor] ) Exported through apex_x/tiles/__init__.py : TorchTileMeta TilePackTorch pack_tiles_torch Added tests in tests/test_tile_pack_torch.py : correctness vs NumPy reference pack_tiles(...) contiguous output assertion autograd gradient-flow check from packed output back to source feature map Verification status: python -m ruff check ... passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_tile_pack_torch.py tests/test_tile_ops.py passed Torch tile unpacker implemented with overlap priority modes: Extended apex_x/tiles/torch_ops.py : TileUnpackTorch.unpack(...) : input: base_map[B,C,H,W] , packed_out[B,K,C,t,t] , meta , level_priority , optional priority_map overlap modes: override : incoming tile values replace existing values where priority allows blend : weighted fusion out = (1-alpha)*current + alpha*incoming where priority allows priority contract preserved via per-pixel priority_map updates strict validation for tensor ranks/shapes, bounds, and mode/alpha values unpack_tiles_torch(...) convenience wrapper OverlapMode type alias Exported through apex_x/tiles/__init__.py : OverlapMode TileUnpackTorch unpack_tiles_torch Added tests in tests/test_tile_unpack_torch.py : override-mode parity vs NumPy unpack_tiles(...) overlap priority enforcement and blend-mode numeric behavior autograd gradient-flow checks for blend mode ( base and packed_out ) helper/function parity ( TileUnpackTorch().unpack vs unpack_tiles_torch ) Verification status: python -m ruff check apex_x/tiles/torch_ops.py apex_x/tiles/__init__.py tests/test_tile_unpack_torch.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_tile_pack_torch.py tests/test_tile_unpack_torch.py tests/test_tile_ops.py passed Fusion gate implemented with boundary/uncertainty-conditioned alpha: Added apex_x/model/fusion_gate.py : FusionGate(nn.Module) computes spatial gate: alpha [B,1,H,W] = sigmoid(w_b * boundary_proxy + w_u * uncertainty_proxy + bias) positive monotonic proxy weights enforced via softplus(...) outputs fused features: fused = base + alpha * (heavy - base) validates proxy shape contracts and aligns proxies to feature dtype/device Exported in apex_x/model/__init__.py : FusionGate Added tests in tests/test_fusion_gate.py : alpha shape/range and fusion formula correctness sensitivity checks: increasing boundary/uncertainty proxies increases mean alpha Verification status: python -m ruff check apex_x/model/fusion_gate.py tests/test_fusion_gate.py apex_x/model/__init__.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_fusion_gate.py passed Cheap block implemented ( 1x1 + norm + ReGLU + optional residual ): Added apex_x/model/cheap_block.py : CheapBlock(nn.Module) : path: Conv2d(1x1) -> GroupNorm -> ReGLU optional residual add automatic residual projection ( 1x1 ) when in_channels != out_channels validation for input channels and normalization group divisibility Exported in apex_x/model/__init__.py : CheapBlock Added tests in tests/test_cheap_block.py : shape checks with residual projection path residual identity behavior when main path is zeroed no-residual zero-output behavior when main path is zeroed gradient-flow checks for input and block parameters Verification status: python -m ruff check apex_x/model/cheap_block.py apex_x/model/__init__.py tests/test_cheap_block.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_cheap_block.py tests/test_model.py passed Tile refine block implemented for packed tiles: Added apex_x/model/tile_refine_block.py : TileRefineBlock(nn.Module) operating on packed tensors [B,K,C,t,t] local refine path per tile: depthwise conv ( k x k ) pointwise conv GroupNorm ReGLU activation optional residual (with automatic projection when channels differ) implementation flattens B*K for conv processing and restores [B,K,...] , preserving per-tile independence Exported in apex_x/model/__init__.py : TileRefineBlock Added tests in tests/test_tile_refine_block.py : shape/projection path checks residual identity when main path is zeroed per-tile independence (no cross-tile mixing) gradient-flow checks for inputs and parameters Verification status: python -m ruff check apex_x/model/tile_refine_block.py apex_x/model/__init__.py tests/test_tile_refine_block.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_tile_refine_block.py tests/test_cheap_block.py passed PV backbone implemented with P3/P4/P5 feature outputs: Added apex_x/model/pv_backbone.py : PVBackbone(nn.Module) returning feature dict: P3 (stride 8) P4 (stride 16) P5 (stride 32) uses lightweight staged downsampling + CheapBlock refinement per level validates input shape/channel contract and minimum spatial size Exported in apex_x/model/__init__.py : PVBackbone Added tests in tests/test_pv_backbone.py : parameterized shape checks across multiple input sizes gradient-flow check input validation checks Verification status: python -m ruff check apex_x/model/pv_backbone.py apex_x/model/__init__.py tests/test_pv_backbone.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_pv_backbone.py passed PV coarse heads implemented with explicit uncertainty proxy definition: Added apex_x/model/pv_coarse_heads.py : PVCoarseHeads(nn.Module) producing coarse PV maps from a selected backbone level (default P4 ): objectness_logits objectness = sigmoid(objectness_logits) boundary_proxy = sigmoid(boundary_logits) variance_proxy = softplus(variance_logits) uncertainty_proxy clear uncertainty definition: u_hat = 4 * p * (1 - p) where p = objectness normalized Bernoulli variance ( u_hat=1 at p=0.5 , u_hat=0 at p in {0,1} ) output typed via PVCoarseOutput dataclass Exported in apex_x/model/__init__.py : PVCoarseHeads PVCoarseOutput Added tests in tests/test_pv_coarse_heads.py : parameterized shape/range checks across multiple image sizes (via PVBackbone + P4 ) uncertainty sensitivity checks with controlled objectness logits direct uncertainty formula parity check gradient-flow check through backbone + heads Verification status: python -m ruff check apex_x/model/pv_coarse_heads.py apex_x/model/__init__.py tests/test_pv_coarse_heads.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_pv_coarse_heads.py passed PV module wired with backbone + coarse heads: Added apex_x/model/pv_module.py : PVModule composes: PVBackbone for P3/P4/P5 feature extraction PVCoarseHeads for coarse proxies PVModule.forward() now returns PVModuleOutput containing: features dict ( P3/P4/P5 ) coarse maps from selected level ( coarse_level : P3 / P4 / P5 ) proxy_maps dict for routing-facing signals: objectness uncertainty boundary variance Exported in apex_x/model/__init__.py : PVModule PVModuleOutput Added tests in tests/test_pv_module.py : shape checks across multiple input sizes coarse-level selection checks ( P3 vs P5 ) proxy_maps key/shape checks finite-output assertions Added dedicated CPU smoke test in tests/test_pv_module_smoke_cpu.py : validates CPU forward path and proxy-map outputs are finite with expected shapes Verification status: python -m ruff check apex_x/model/pv_module.py apex_x/model/__init__.py tests/test_pv_module.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_pv_module.py tests/test_pv_module_smoke_cpu.py tests/test_pv_backbone.py tests/test_pv_coarse_heads.py passed Stable state-space-like scan implemented with constrained parameters: Extended apex_x/utils/ssm.py : StableStateSpaceScan(nn.Module) with constrained recurrent parameters: decay constrained to (min_decay, max_decay) via sigmoid mapping positive input/output gains via softplus numerically safe token sanitization/clamp path SSMScanStats for explicit scan complexity accounting: steps recurrent_updates pairwise_updates tile_ssm_scan(...) now clamps alpha to stable bounds Exported in apex_x/utils/__init__.py : StableStateSpaceScan SSMScanStats Added tests in tests/test_stable_ssm_scan.py : no-NaN/finite checks on extreme inputs gradient-flow checks for inputs and scan parameters O(K) behavior checks via linear recurrent-update accounting and zero pairwise updates Verification status: python -m ruff check apex_x/utils/ssm.py apex_x/utils/__init__.py tests/test_stable_ssm_scan.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_stable_ssm_scan.py passed Bidirectional scan and merge gate added on top of stable scan: Extended apex_x/utils/ssm.py : StableBidirectionalStateSpaceScan(nn.Module) : forward-direction stable scan backward-direction stable scan (reverse sequence + reverse outputs back) channel-wise constrained merge gate: gate = sigmoid(merge_gate_logit) in [0,1] merged output: gate * y_fwd + (1 - gate) * y_bwd complexity stats preserved as linear-time recurrent updates (no pairwise updates) Exported in apex_x/utils/__init__.py : StableBidirectionalStateSpaceScan Added tests in tests/test_bidirectional_ssm_scan.py : finite/no-NaN checks with extreme inputs merge formula correctness vs explicit gate-weighted combination gradient-flow checks for inputs and parameters (including merge gate path) O(K) scaling checks from recurrent update counts Verification status: python -m ruff check apex_x/utils/ssm.py apex_x/utils/__init__.py tests/test_bidirectional_ssm_scan.py tests/test_stable_ssm_scan.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_stable_ssm_scan.py tests/test_bidirectional_ssm_scan.py passed FiLM modulation implemented from tokens to packed tiles: Added apex_x/model/film.py : TileFiLM(nn.Module) : computes FiLM parameters from tokens tokens[B,K,Ct] bounded gain path: gamma = tanh(gamma_raw) * gamma_limit shift path: beta applies modulation to packed tiles: out = (1 + gamma) * tiles + beta apply_film(...) functional helper with strict shape validation Updated apex_x/model/core.py : replaced additive-only packed modulation with FiLM-style modulation using SSM mixed tokens: gamma = tanh(mixed) beta = mixed packed_out = (1 + gamma) * packed + beta Exported in apex_x/model/__init__.py : TileFiLM apply_film Added tests in tests/test_film_modulation.py : formula and shape checks gamma range bound checks deterministic forward under fixed inputs gradient-flow checks through tokens, tiles, and module parameters Verification status: python -m ruff check apex_x/model/film.py apex_x/model/core.py apex_x/model/__init__.py tests/test_film_modulation.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_film_modulation.py tests/test_model.py passed FF heavy path implemented end-to-end with aligned detail map output: Added apex_x/model/ff_heavy_path.py : FFHeavyPath(nn.Module) pipeline: TilePackTorch gather on selected FF tile indices tile tokenization via spatial pooling: tokens[B,K,C] stable scan ( forward or bidirectional ) over tokens FiLM modulation ( gamma , beta ) from mixed tokens to packed tiles local packed-tile refine via TileRefineBlock TileUnpackTorch scatter back to dense map shape optional proxy-conditioned fusion gate ( FusionGate ) output contract via FFHeavyPathOutput : heavy_features[B,C,H,W] detail_map[B,C,H,W] aligned to dense features alpha[B,1,H,W] diagnostics tensors ( tokens , mixed_tokens , gamma , beta , state ) includes robust CPU behavior for empty tile selections ( K=0 ) with zero detail contribution. Updated exports in apex_x/model/__init__.py : FFHeavyPath FFHeavyPathOutput Added CPU tests in tests/test_ff_heavy_path.py : shape/alignment checks and finite outputs empty-selection behavior ( K=0 ) -> zero detail_map deterministic repeated forward and gradient-flow checks Verification status: python -m ruff check apex_x/model/ff_heavy_path.py apex_x/model/__init__.py tests/test_ff_heavy_path.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_ff_heavy_path.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q FF module implemented for train/infer routing execution: Added apex_x/model/ff_module.py : FFModule(nn.Module) with two explicit entrypoints: forward_train(...) : STE routing gates via ste_gate_from_utilities(...) expected-cost computation via BudgetDualController.expected_cost(...) budget loss term via BudgetDualController.budget_loss(...) optional dual- mu update with persistent mu_history routed L0 heavy execution through FFHeavyPath forward_infer(...) : deterministic L0 greedy selection under B1 / Kmax_l0 optional L0->L1 two-stage selection under B2 / Kmax_l1 optional nesting execution (L1 heavy pass) when split utilities provided diagnostics integrated in both paths through build_routing_diagnostics(...) : selected counts/ratios utility histograms per-budget usage ( b1/b2/b3/total ) mu_history output dataclasses: FFTrainOutput FFInferOutput Updated exports in apex_x/model/__init__.py : FFModule FFTrainOutput FFInferOutput Added tests in tests/test_ff_module.py : train path: STE + expected-cost + budget-loss outputs diagnostics presence dual- mu history update inference path: deterministic budgeted L0 selection optional nesting with deterministic L1 child selection under B2 diagnostics coverage Verification status: python -m ruff check apex_x/model/ff_module.py apex_x/model/__init__.py tests/test_ff_module.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_ff_module.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q Dual-path FPN implemented to combine PV low-res with FF high-res: Added apex_x/model/fpn.py : DualPathFPN(nn.Module) : inputs: PV features dict P3/P4/P5 FF high-res feature/detail map fusion path: lateral 1x1 projections for PV P3/P4/P5 lateral 1x1 projection for FF branch top-down FPN merge ( P5 -> P4 -> P3 ) explicit FF injection at P3 after spatial alignment smoothing with CheapBlock on P3/P4/P5 output contract via DualPathFPNOutput : fused pyramid dict P3/P4/P5 aligned FF feature map at P3 resolution ( ff_aligned ) Updated exports in apex_x/model/__init__.py : DualPathFPN DualPathFPNOutput Added tests in tests/test_fpn.py : CPU shape and finite-value checks FF-branch sensitivity (changing FF input changes fused P3 ) gradient-flow checks through PV inputs, FF input, and FPN params Verification status: python -m ruff check apex_x/model/fpn.py apex_x/model/__init__.py tests/test_fpn.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_fpn.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q DET head implemented over P3..P7 (cls/box/quality): Added apex_x/model/det_head.py : DetHead(nn.Module) : consumes pyramid features with required P3/P4/P5 supports optional provided P6/P7 ; auto-generates missing levels from P5 / P6 per-level outputs: cls_logits : [B, num_classes, H, W] box_reg : [B, 4, H, W] quality : [B, 1, H, W] uses shared tower structure for cls/box/quality with GroupNorm + SiLU and export-friendly conv outputs output contract via DetHeadOutput : per-level dicts for cls_logits , box_reg , quality normalized features dict for effective P3..P7 levels used by the head Updated exports in apex_x/model/__init__.py : DetHead DetHeadOutput Added tests in tests/test_det_head.py : shape checks across all output levels P3..P7 when only P3..P5 are provided behavior check that explicitly provided P6/P7 are used as-is gradient-flow checks through inputs and DET-head parameters Verification status: python -m ruff check apex_x/model/det_head.py apex_x/model/__init__.py tests/test_det_head.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_det_head.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q SimOTA/OTA cost components implemented for DET matching: Added apex_x/losses/simota.py : classification cost: classification_cost(...) with modes: BCE-based positive-class cost focal-based positive-class cost IoU cost: iou_cost(...) implementing 1 - IoU over pairwise GT/anchor boxes center prior cost: center_prior_cost(...) from GT center to anchor center distance (normalized by GT size) per-GT candidate generation: topk_center_candidates(...) selecting top-k nearest anchor centers per GT candidate_mask_from_indices(...) to build per-GT candidate masks integrated cost assembly: compute_simota_cost(...) combining weighted components and candidate masking SimOTACostOutput dataclass containing component matrices, combined cost, and candidates Updated exports in apex_x/losses/__init__.py : classification_cost , iou_cost , center_prior_cost topk_center_candidates , candidate_mask_from_indices compute_simota_cost , SimOTACostOutput , ClassificationCostType Added tests in tests/test_simota_cost.py : per-GT top-k center candidate selection correctness on synthetic anchors/GT classification-cost ranking behavior (higher positive logit -> lower cost) IoU cost sanity ( 1 - IoU ) combined SimOTA ranking on synthetic setup (reasonable anchor wins; non-candidates penalized) center-prior ranking preference for nearby anchor centers Verification status: python -m ruff check apex_x/losses/simota.py apex_x/losses/__init__.py tests/test_simota_cost.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_simota_cost.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q Dynamic-K SimOTA matching implemented (including conflict resolution): Extended apex_x/losses/simota.py : DynamicKMatchingOutput dataclass with: dynamic_ks matching_matrix foreground_mask matched_gt_indices assigned_cost num_foreground dynamic_k_from_top_ious(...) : computes per-GT dynamic_k from sum of top IoUs (with configurable topk and min_k ) supports optional candidate mask dynamic_k_matching(...) : selects dynamic_k anchors per GT by minimal total cost resolves multi-GT conflicts by keeping the minimal-cost GT assignment per anchor outputs deterministic one-to-one anchor-to-GT assignment for foreground anchors Updated exports in apex_x/losses/__init__.py : DynamicKMatchingOutput dynamic_k_from_top_ious dynamic_k_matching Expanded tests/test_simota_cost.py : verifies dynamic-k computation from top-IoU sums verifies crowded conflict resolution picks minimal-cost GT per anchor verifies candidate-mask constraints are respected in crowded settings Verification status: python -m ruff check apex_x/losses/simota.py apex_x/losses/__init__.py tests/test_simota_cost.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_simota_cost.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q SimOTA assignment integrated into DET loss with target generation: Added apex_x/losses/det_loss.py : build_simota_targets_for_anchors(...) : uses SimOTA cost + dynamic-k matching to select positive anchors builds per-anchor targets: cls_target (one-hot positives, zero background) box_target (assigned GT boxes) quality_target (matched IoU targets) returns SimOTATargets with matching diagnostics det_loss_with_simota(...) : computes DET loss from assignment targets: classification loss (BCE or focal) box loss ( 1 - IoU ) on positives quality BCE loss returns DetLossOutput with component losses and targets stability features: canonicalized box ordering for robust IoU math optional assignment on detached predictions small-object positive weighting with clipped inverse-sqrt area scaling dynamic-k conflict-resolved assignment for crowded scenes Updated exports in apex_x/losses/__init__.py : SimOTATargets DetLossOutput build_simota_targets_for_anchors det_loss_with_simota ClsLossType Added tests in tests/test_det_loss_simota.py : target generation and per-anchor labeling correctness finite/stable loss on tiny-object crowded synthetic setup toy optimization loop verifying DET loss decreases over training steps Verification status: python -m ruff check apex_x/losses/det_loss.py apex_x/losses/__init__.py tests/test_det_loss_simota.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_det_loss_simota.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q DET losses hardened for numerical stability and quality-focal support: Updated apex_x/losses/det_loss.py : added logit sanitization/clipping via _sanitize_logits(...) added QualityLossType with bce and qfl modes det_loss_with_simota(...) now supports: quality_loss_type=\\\"qfl\\\" quality_focal_beta logit_clip focal/BCE classification and QFL/BCE quality paths now run on sanitized logits for stable behavior under extreme values Updated exports in apex_x/losses/__init__.py : QualityLossType Expanded tests/test_det_loss_simota.py : toy decreasing-loss case now also exercises quality-focal path added extreme-logit + tiny-box finite test with backward gradient finiteness checks Verification status: python -m ruff check apex_x/losses/det_loss.py apex_x/losses/__init__.py tests/test_det_loss_simota.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_det_loss_simota.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q Deterministic DET decode + NMS implemented: Added apex_x/infer/detection.py : decode_anchor_free_candidates(...) : decodes anchor-free DetHeadOutput maps into per-image candidate tensors supports configurable level strides and image clipping applies stable candidate ranking with deterministic tie behavior deterministic_nms(...) : class-wise NMS with deterministic ordering tie-breaking policy: score desc, then candidate index asc batched_deterministic_nms(...) : fixed-shape batched outputs with padding and valid_counts decode_and_nms(...) : end-to-end decode + NMS convenience entrypoint output dataclasses: DetectionCandidates DetectionBatch Updated exports in apex_x/infer/__init__.py : DetectionCandidates DetectionBatch decode_anchor_free_candidates deterministic_nms batched_deterministic_nms decode_and_nms Added tests in tests/test_det_decode_nms.py : end-to-end decode + NMS determinism and class-wise suppression behavior deterministic tie handling in NMS (equal scores -> lower index first) cross-class overlap handling (no cross-class suppression) Verification status: python -m ruff check apex_x/infer/detection.py apex_x/infer/__init__.py tests/test_det_decode_nms.py passed python -m mypy --cache-dir=/dev/null apex_x/infer/detection.py apex_x/infer/__init__.py passed python -m pytest -q tests/test_det_decode_nms.py tests/test_import_smoke.py passed python -m pytest -q passed Prototype-based instance segmentation forward path and mask assembly implemented: Added apex_x/model/inst_seg_head.py : PrototypeInstanceSegHead(nn.Module) : prototype generator from feature maps ( prototypes: [B,M,Hp,Wp] ) per-instance coefficient prediction ( coefficients: [B,N,M] ) from: ROI mean-pooled feature regions derived from boxes_xyxy , or explicit instance_embeddings mask assembly by linear prototype combination: mask_logits_lowres = einsum(coefficients, prototypes) -> [B,N,Hp,Wp] output resizing to requested mask resolution optional box cropping with configurable fill value for stable masked logits per-instance mask_scores from masked probability averages helper functions: assemble_mask_logits_from_prototypes(...) rasterize_box_masks(...) output dataclass: InstanceSegOutput Updated exports in apex_x/model/__init__.py : PrototypeInstanceSegHead InstanceSegOutput assemble_mask_logits_from_prototypes rasterize_box_masks Added tests in tests/test_inst_seg_head.py : prototype-mask assembly correctness vs expected weighted combinations forward-path shape/range checks and finite outputs deterministic repeatability for same weights/inputs gradient-flow checks (features + instance embeddings + parameters) crop-to-box fill behavior outside ROI regions Verification status: python -m ruff check apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_head.py passed python -m mypy --cache-dir=/dev/null apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_head.py passed python -m pytest -q tests/test_inst_seg_head.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q passed Segmentation losses implemented (BCE + Dice + boundary DT surrogate): Added apex_x/losses/seg_loss.py : mask_bce_loss(...) : BCEWithLogits per mask with optional per-instance weighting [B,N] mask_dice_loss(...) : soft Dice loss over [B,N,H,W] masks with optional per-instance weighting soft_boundary_distance_transform(...) : differentiable approximation of boundary distance transform using iterative soft-min neighborhood propagation boundary_distance_transform_surrogate_loss(...) : boundary mismatch weighted by target soft distance transform instance_segmentation_losses(...) : combined BCE + Dice + boundary loss returning SegLossOutput Updated exports in apex_x/losses/__init__.py : SegLossOutput mask_bce_loss mask_dice_loss soft_boundary_distance_transform boundary_distance_transform_surrogate_loss instance_segmentation_losses Added tests in tests/test_seg_loss.py : BCE/Dice near-zero behavior for near-perfect logits soft boundary-DT monotonicity sanity check boundary surrogate sensitivity to shifted boundaries toy optimization loop showing combined loss decreases with finite gradients instance-weight support path Verification status: python -m ruff check apex_x/losses/seg_loss.py apex_x/losses/__init__.py tests/test_seg_loss.py passed python -m mypy --cache-dir=/dev/null apex_x/losses/seg_loss.py apex_x/losses/__init__.py tests/test_seg_loss.py passed python -m pytest -q tests/test_seg_loss.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q FF high-resolution tile-gated refinement hook implemented for instance masks: Updated apex_x/model/inst_seg_head.py : added FFTileRefinementHook(nn.Module) : inputs: mask_logits [B,N,H,W] , ff_highres_features [B,C,H,W] , active_tile_indices [B,K] packs only active tiles, applies FF-conditioned additive refinement on packed tiles, and unpacks back guarantees inactive tiles remain unchanged via tile-scatter semantics integrated optional hook into PrototypeInstanceSegHead : new init toggles: enable_ff_refine ff_refine_tile_size ff_refine_order_mode ff_refine_overlap_mode ff_refine_blend_alpha ff_refine_strength_init new forward args: ff_highres_features active_tile_indices refinement is applied only when enabled and both FF features + active indices are provided Updated exports in apex_x/model/__init__.py : FFTileRefinementHook Added tests in tests/test_inst_seg_refinement_hook.py : hook updates only selected tiles and leaves non-selected tiles exactly unchanged empty active-tile indices produce no-op behavior head-level integration verifies refinement delta is confined to active tile regions Verification status: python -m ruff check apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_refinement_hook.py passed python -m mypy --cache-dir=/dev/null apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_refinement_hook.py passed python -m pytest -q tests/test_inst_seg_refinement_hook.py tests/test_inst_seg_head.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Panoptic output generation implemented (semantic + instance fusion): Added apex_x/infer/panoptic.py : generate_panoptic_output(...) : combines semantic logits with instance masks into deterministic panoptic maps deterministic overlap fusion: thing instances fused first, sorted by (score desc, instance_index asc) overlap resolution keeps higher-ranked instance pixels deterministic thing/stuff rules: only classes in thing_class_ids are accepted as thing instances remaining unassigned pixels are filled by semantic stuff classes in ascending class-id order segment id 0 reserved as void/unassigned supports: mask threshold, score threshold minimum thing/stuff area filters optional masks_are_logits for instance-mask logits input dataclasses: PanopticSegmentInfo ( id , category_id , isthing , area , optional score/index) PanopticOutput ( panoptic_map , segments_info , semantic_labels ) Updated exports in apex_x/infer/__init__.py : PanopticSegmentInfo PanopticOutput generate_panoptic_output Added tests in tests/test_panoptic_generation.py : deterministic overlap resolution on synthetic overlapping thing instances thing/stuff rule verification (non-thing instances ignored, stuff preserved) output contract checks on synthetic batched scenes: map shape/type unique segment IDs per-segment area parity with panoptic map pixels Verification status: python -m ruff check apex_x/infer/panoptic.py apex_x/infer/__init__.py tests/test_panoptic_generation.py passed python -m mypy --cache-dir=/dev/null apex_x/infer/panoptic.py apex_x/infer/__init__.py tests/test_panoptic_generation.py passed python -m pytest -q tests/test_panoptic_generation.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Panoptic PQ evaluation wrapper implemented (official API + fallback): Added apex_x/infer/pq_eval.py : evaluate_panoptic_quality(...) : uses official panopticapi evaluator when available and paths are provided otherwise falls back to an in-memory deterministic minimal PQ implementation deterministic fallback behavior: per-category matching with IoU threshold one-to-one matches with deterministic tie handling computes per-class (PQ, SQ, RQ) and aggregate all/things/stuff metrics dataclasses: PQClassMetrics PQMetrics OfficialPQPaths Updated exports in apex_x/infer/__init__.py : PQClassMetrics PQMetrics OfficialPQPaths evaluate_panoptic_quality CLI integration hook added: updated apex_x/cli.py eval command with: --panoptic-pq flag runs panoptic PQ hook and prints panoptic_pq=<value> and source ( official / fallback ) Added fixtures: tests/fixtures/pq_case_perfect.json tests/fixtures/pq_case_partial.json Added tests: tests/test_pq_eval.py fixture-driven perfect and partial overlap PQ checks verifies official-path attempt cleanly falls back when official API is unavailable/invalid tests/test_cli.py new parse test for eval --panoptic-pq Verification status: python -m ruff check apex_x/infer/pq_eval.py apex_x/infer/__init__.py apex_x/cli.py tests/test_pq_eval.py tests/test_cli.py passed python -m mypy --cache-dir=/dev/null apex_x/infer/pq_eval.py apex_x/infer/__init__.py apex_x/cli.py tests/test_pq_eval.py tests/test_cli.py passed python -m pytest -q tests/test_pq_eval.py tests/test_cli.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Tracking embedding head and association interfaces implemented: Added apex_x/model/track_head.py : TrackEmbeddingHead : accepts feature tensor or feature dict projects features, ROI-pools detections, and emits L2-normalized embeddings TrackEmbeddingOutput dataclass with: embeddings raw_embeddings pooled_features Added apex_x/infer/tracking.py : TrackState dataclass (validated tracker state contract) AssociationResult dataclass AssociationProtocol interface deterministic GreedyCosineAssociator implementation compatibility aliases: TrackAssociatorProtocol TrackAssociator Updated exports: apex_x/model/__init__.py exports TrackEmbeddingHead , TrackEmbeddingOutput apex_x/infer/__init__.py exports tracking dataclasses/protocols/associator apex_x/__init__.py exports TrackState and AssociationProtocol Added tests: tests/test_track_head.py : output shape checks embedding unit-norm checks deterministic forward checks gradient flow checks tests/test_tracking_interfaces.py : TrackState.empty(...) contract protocol conformance checks deterministic greedy matching/new-track behavior aging/removal behavior with no detections Verification status: targeted checks passed: python -m ruff check apex_x/model/track_head.py apex_x/infer/tracking.py tests/test_track_head.py tests/test_tracking_interfaces.py python -m mypy --cache-dir=/dev/null apex_x/model/track_head.py apex_x/infer/tracking.py tests/test_track_head.py tests/test_tracking_interfaces.py python -m pytest -q tests/test_track_head.py tests/test_tracking_interfaces.py project checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q note: python -m black --check . currently reports unrelated pre-existing formatting diffs in legacy files not touched in this change: apex_x/routing/interfaces.py apex_x/train/__init__.py tests/test_pq_eval.py apex_x/config/schema.py apex_x/infer/pq_eval.py apex_x/losses/det_loss.py apex_x/model/inst_seg_head.py Hungarian association with lifecycle and memory-bank updates implemented: Updated apex_x/infer/tracking.py : added Hungarian solver utility: hungarian_assignment(...) added HungarianAssociator implementing: IoU + embedding-distance gating for candidate matches global cost minimization via Hungarian assignment track lifecycle ( init / update / terminate ) with max_age embedding memory-bank update per track with fixed bank size bank-size normalization for legacy states to keep runtime stable extended TrackState with optional lifecycle/memory fields: hit_counts memory_bank memory_counts extended AssociationResult with lifecycle debug outputs: terminated_track_indices terminated_track_ids created_track_ids kept backward compatibility: GreedyCosineAssociator now delegates to Hungarian with cosine-only cost protocol aliases preserved Updated apex_x/infer/__init__.py exports: HungarianAssociator hungarian_assignment Added tests in tests/test_tracking_hungarian.py : Hungarian global-optimum assignment on synthetic cost matrix IoU + embedding-distance gating behavior lifecycle termination after max_age memory-bank update/cap behavior multi-frame moving-object ID consistency across reordered detections Verification status: targeted checks passed: python -m ruff check apex_x/infer/tracking.py apex_x/infer/__init__.py tests/test_tracking_interfaces.py tests/test_tracking_hungarian.py python -m mypy --cache-dir=/dev/null apex_x/infer/tracking.py apex_x/infer/__init__.py tests/test_tracking_interfaces.py tests/test_tracking_hungarian.py python -m pytest -q tests/test_tracking_interfaces.py tests/test_tracking_hungarian.py tests/test_track_head.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q PCGrad++ shared-trunk conflict projection implemented: Added apex_x/train/pcgrad.py : canonical grouped loss ordering: det_cls , det_box , seg_mask , seg_boundary , then sorted extras LossGroup dataclass and group_loss_terms(...) apply_pcgradpp(...) : computes per-group gradients applies projection only to shared trunk parameter gradients when cos < 0 leaves task-head parameter gradients as standard total-loss gradients PCGradDiagnostics + diagnostics_to_dict(...) for logging/debug Updated exports in apex_x/train/__init__.py : DEFAULT_LOSS_GROUP_ORDER LossGroup PCGradDiagnostics group_loss_terms apply_pcgradpp diagnostics_to_dict Added tests in tests/test_pcgradpp.py : deterministic grouped ordering for canonical loss groups + extra loss terms tiny-network synthetic conflicting gradients test: confirms projection resolves shared-trunk conflict confirms head gradients match standard total-loss gradients (no projection on heads) Verification status: targeted checks passed: python -m ruff check apex_x/train/pcgrad.py apex_x/train/__init__.py tests/test_pcgradpp.py python -m mypy --cache-dir=/dev/null apex_x/train/pcgrad.py apex_x/train/__init__.py python -m pytest -q tests/test_pcgradpp.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Distillation losses implemented (logits KL + feature L2 + boundary distill): Added apex_x/losses/distill.py : logits_kl_distill(...) : KL divergence distillation on logits with temperature scaling ( T^2 factor) feature_l2_distill(...) : layer-selective feature L2 distillation with optional per-layer weights supports optional feature normalization before L2 boundary_distill_loss(...) : boundary-focused distillation using Sobel-based soft boundary maps teacher boundary distance-transform weighting distillation_losses(...) : combined wrapper returning DistillationLossOutput Updated exports in apex_x/losses/__init__.py : DistillationLossOutput logits_kl_distill feature_l2_distill boundary_distill_loss distillation_losses Added tests in tests/test_distill_loss.py : KL distill near-zero when student/teacher logits match feature L2 selected-layer behavior with layer weights boundary distill penalizes shifted boundaries more than aligned boundaries combined distillation loss decreases in toy optimization Verification status: targeted checks passed: python -m ruff check apex_x/losses/distill.py apex_x/losses/__init__.py tests/test_distill_loss.py python -m mypy --cache-dir=/dev/null apex_x/losses/distill.py apex_x/losses/__init__.py python -m pytest -q tests/test_distill_loss.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Oracle \u0394Loss targets and router utility supervision implemented: Added apex_x/routing/oracle_distill.py : compute_oracle_delta_targets(...) : computes sampled-tile oracle targets: \u0394_i = L_distill(cheap, teacher) - L_distill(heavy, teacher) supports sampled indices [S] or batched [B,S] returns detached (stop-grad) oracle targets optional clamping for outlier robustness utility_regression_loss(...) : regression loss ( l1 / mse / smooth_l1 ) between router utility logits and detached \u0394 targets utility_ranking_loss(...) : pairwise hinge ranking loss over sampled tiles to preserve oracle ordering utility_oracle_loss(...) : combined regression + ranking objective with diagnostics ( num_pairs ) dataclasses: OracleDeltaTargets UtilityOracleLossOutput Updated routing exports in apex_x/routing/__init__.py : RegressionLossType OracleDeltaTargets UtilityOracleLossOutput compute_oracle_delta_targets utility_regression_loss utility_ranking_loss utility_oracle_loss Added tests in tests/test_oracle_distill.py : sign sanity for \u0394 targets (positive when heavy distill loss is lower than cheap) stop-grad behavior (no gradients into cheap/heavy distill losses via utility supervision) ranking sign sanity (correct utility order yields lower ranking loss) sampled-index regression correctness (only sampled tiles influence loss) Verification status: targeted checks passed: python -m ruff check apex_x/routing/oracle_distill.py apex_x/routing/__init__.py tests/test_oracle_distill.py python -m mypy --cache-dir=/dev/null apex_x/routing/oracle_distill.py apex_x/routing/__init__.py python -m pytest -q tests/test_oracle_distill.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q TeacherModel implemented for full-compute distillation outputs with optional EMA: Added apex_x/model/teacher.py : TeacherModel : dense/full-compute teacher forward path (PV -> FPN -> DET) without sparse routing standardized distillation output bundle: flattened logits ( logits ) per-level logits ( logits_by_level ) selected feature layers ( features ) boundary proxy map aligned to input size ( boundaries ) optional EMA shadow modules: configurable ema_decay update_ema(...) for parameter/buffer updates runtime switch to use online or EMA weights in forward(...) TeacherDistillOutput dataclass flatten_logits_for_distill(...) helper with deterministic level order Updated exports in apex_x/model/__init__.py : TeacherModel TeacherDistillOutput flatten_logits_for_distill Added tests in tests/test_teacher_model.py : full-compute standardized output contract checks deterministic logits-flatten ordering checks EMA behavior checks: initial online/EMA parity EMA lag + update movement toward online model frozen EMA parameter requirements Verification status: targeted checks passed: python -m ruff check apex_x/model/teacher.py apex_x/model/__init__.py tests/test_teacher_model.py python -m mypy --cache-dir=/dev/null apex_x/model/teacher.py apex_x/model/__init__.py python -m pytest -q tests/test_teacher_model.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Staged trainer pipeline implemented and wired into CLI train flow: Added apex_x/train/trainer.py : ApexXTrainer with required stages: stage 0: baseline warmup stage 1: teacher training (full compute) stage 2: oracle bootstrapping stage 3: continuous budgeting with dual mu stage 4: deterministic inference emulation stage/result dataclasses: StageResult StagedTrainResult Updated exports in apex_x/train/__init__.py : ApexXTrainer StageResult StagedTrainResult Updated CLI train command in apex_x/cli.py : now runs staged trainer instead of placeholder-only loop new option: --steps-per-stage output includes stage_count=5 Added staged-train CPU smoke script: examples/train_stages_smoke.py Added validation coverage: tests/test_trainer_stages.py (stage completeness + seed repeatability) tests/test_train_stages_smoke.py (subprocess smoke run) updated tests/test_cli.py to assert staged output includes stage_count=5 Updated docs: README.md now includes staged trainer quickstart/dev commands Verification status: targeted checks passed: python -m ruff check apex_x/train/trainer.py apex_x/cli.py tests/test_trainer_stages.py tests/test_train_stages_smoke.py tests/test_cli.py examples/train_stages_smoke.py python -m mypy --cache-dir=/dev/null apex_x/train/trainer.py apex_x/cli.py python -m pytest -q tests/test_trainer_stages.py tests/test_train_stages_smoke.py tests/test_cli.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Exact COCO compatibility layer implemented with strict schema checks and mask parsing: Added apex_x/data/coco.py with a strict COCO loader: load_coco_dataset(path, strict=True, use_cache=True) strict top-level/record key validation ( images , annotations , categories ) type/range checks for ids, bbox, area, iscrowd, segmentation payloads referential integrity checks for annotation.image_id and annotation.category_id Added complete parsing contracts: bbox parsing into CocoBBox polygon segmentation parsing into CocoSegmentation(kind=\\\"polygon\\\") RLE parsing for uncompressed list counts and compressed string counts into CocoSegmentation(kind=\\\"rle\\\") deterministic mask decode path via segmentation_to_mask(...) Added category mapping + caching: CocoDataset.category_mapping() caches contiguous category mapping: original_to_contiguous contiguous_to_original category name lookup maps loader cache for parsed datasets with mtime/size cache key clear_coco_dataset_cache() helper Updated exports in apex_x/data/__init__.py for COCO dataclasses/helpers. Added fixture-based tests: tests/test_coco_compat.py fixtures: tests/fixtures/coco_valid_mixed.json tests/fixtures/coco_invalid_missing_top_keys.json tests/fixtures/coco_invalid_bad_rle.json Verification status: targeted checks passed: python -m ruff check apex_x/data/coco.py apex_x/data/__init__.py tests/test_coco_compat.py python -m mypy --cache-dir=/dev/null apex_x/data/coco.py apex_x/data/__init__.py tests/test_coco_compat.py python -m pytest -q tests/test_coco_compat.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Data transforms pipeline and Mosaic-v2 heuristic implemented: Added apex_x/data/transforms.py : shared sample contract: TransformSample (image + boxes_xyxy + class_ids + optional masks) pipeline + base transforms: TransformPipeline RandomHorizontalFlip ClipBoxesAndMasks sanitize_sample(...) for clipping/filtering invalid boxes/masks Mosaic-v2: MosaicV2(...) 4-image composition with configurable split jitter heuristic crop-origin policy to protect important instances (by area threshold fallback-to-largest instance) visibility-aware bbox filtering and mask-aware validity checks Updated exports in apex_x/data/__init__.py : TransformSample , Transform , TransformPipeline RandomHorizontalFlip , ClipBoxesAndMasks , MosaicV2 , sanitize_sample Added tests in tests/test_data_transforms.py : transform-pipeline bbox/mask validity checks mosaic output validity for bbox/mask contracts heuristic regression test showing protected mosaic keeps significantly more important-instance area than unprotected crop selection sanitize clipping behavior on out-of-bounds boxes Verification status: targeted checks passed: python -m ruff check apex_x/data/transforms.py apex_x/data/__init__.py tests/test_data_transforms.py python -m mypy --cache-dir=/dev/null apex_x/data/transforms.py apex_x/data/__init__.py tests/test_data_transforms.py python -m pytest -q tests/test_data_transforms.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Eval pipeline implemented for DET/INST-SEG/SEM-SEG/PANO with report emission: Added apex_x/infer/eval_metrics.py : metric computation for tiny-fixture evaluation: COCO-style mAP (DET) over IoU thresholds 0.50:0.05:0.95 COCO-style mask mAP (INST-SEG) over IoU thresholds 0.50:0.05:0.95 mIoU (SEM-SEG) PQ (PANO) via existing evaluate_panoptic_quality(...) fixture evaluators: evaluate_fixture_payload(...) evaluate_fixture_file(...) built-in fallback payload tiny_eval_fixture_payload() report writers: write_eval_reports(...) emitting JSON + Markdown structured summary dataclass EvalSummary Updated exports in apex_x/infer/__init__.py : EvalSummary , evaluate_fixture_file , evaluate_fixture_payload , tiny_eval_fixture_payload , write_eval_reports Updated CLI eval command in apex_x/cli.py : supports: --fixture (optional fixture JSON; defaults to built-in tiny payload) --report-json --report-md always emits metrics in stdout: det_map , mask_map , miou , panoptic_pq writes JSON and markdown report files per invocation keeps --panoptic-pq flag as compatibility no-op Added tiny fixture + tests: fixture: tests/fixtures/eval_tiny_fixture.json tests/test_eval_metrics.py : metric values on perfect tiny fixture JSON/Markdown report emission built-in tiny payload validation tests/test_cli.py : eval command smoke with fixture + output report paths Verification status: targeted checks passed: python -m ruff check apex_x/infer/eval_metrics.py apex_x/infer/__init__.py apex_x/cli.py tests/test_eval_metrics.py tests/test_cli.py python -m mypy --cache-dir=/dev/null apex_x/infer/eval_metrics.py apex_x/infer/__init__.py apex_x/cli.py tests/test_eval_metrics.py tests/test_cli.py python -m pytest -q tests/test_eval_metrics.py tests/test_cli.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Ablation grid runner implemented in CLI with toggle sweeps and reports: Added apex_x/train/ablation.py : toggle grid definitions for: router , budgeting , nesting , ssm , distill , pcgrad , qat , panoptic , tracking deterministic grid builder: build_ablation_grid(...) with per-toggle modes ( on/off/both ) and max-cap ablation execution: fixed-seed runs over grid combinations trainer invocation with enable_budgeting switch metrics aggregation (DET mAP, mask mAP, semantic mIoU, PQ, tracking consistency) routing stat aggregation (selected ratios, budget usage ratio, mu_last ) report writers: CSV aggregate report markdown summary report Updated ApexXTrainer.run(...) in apex_x/train/trainer.py : added enable_budgeting flag for explicit budgeting on/off ablations Updated exports in apex_x/train/__init__.py : ablation dataclasses/functions ( AblationToggleSet , grid runner, report writer) Updated CLI ablate command in apex_x/cli.py : added per-toggle mode flags ( --router/--budgeting/.../--tracking ) added fixed seed support via repeated --seed added --steps-per-stage , --max-experiments added report outputs: --output-csv --output-md command now runs grid + writes CSV/MD reports Added tests: tests/test_ablation.py : grid construction behavior smoke run + CSV/MD output assertions updated tests/test_cli.py : ablate command smoke with output artifact checks Verification status: targeted checks passed: python -m ruff check apex_x/train/ablation.py apex_x/train/__init__.py apex_x/train/trainer.py apex_x/cli.py tests/test_ablation.py tests/test_cli.py python -m mypy --cache-dir=/dev/null apex_x/train/ablation.py apex_x/train/__init__.py apex_x/train/trainer.py apex_x/cli.py tests/test_ablation.py tests/test_cli.py python -m pytest -q tests/test_ablation.py tests/test_cli.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q INT8 QAT + PTQ fallback path implemented with FP16 router/gating policy: Added apex_x/train/qat.py : activation observer + activation fake quant: ActivationObserver ActivationFakeQuant per-channel INT8 weight fake quant: WeightPerChannelFakeQuant wrapped train-time fake quant modules: FakeQuantConv2d FakeQuantLinear QAT/PTQ entrypoints: prepare_int8_qat(...) prepare_int8_ptq(...) calibrate_ptq(...) explicit wrapper traversal/state controls: iter_qat_wrappers(...) set_qat_state(...) Updated apex_x/train/trainer.py : quantization preparation step added before staged training: uses QAT when train.qat_enable && train.qat_int8 uses PTQ calibration fallback when runtime.precision_profile=edge and QAT is off added deterministic calibration batch builder for PTQ fallback added quantization diagnostics to train_summary[\"quantization\"] : mode , wrapped_modules , calibration_batches , router_gating_fp16 stage-3 routing gate path now keeps FP16 utility gating math and uses FP32 expected-cost accumulation Updated apex_x/train/__init__.py : exported QAT module types/functions for public train API surface Added tests in tests/test_qat.py : QAT wrapper conversion with router/gating skip policy PTQ calibration state transitions (observer off + fake quant on after calibration) trainer-level QAT/PTQ toggle smoke with finite loss/output checks Added documentation: docs/QAT.md with INT8 policy, module behavior, trainer integration, and validation scope linked in docs/index.md and mkdocs.yml Verification status: targeted checks passed: python -m ruff check apex_x/train/qat.py apex_x/train/trainer.py apex_x/train/__init__.py tests/test_qat.py python -m mypy --cache-dir=/dev/null apex_x/train/qat.py apex_x/train/trainer.py apex_x/train/__init__.py python -m pytest -q tests/test_qat.py tests/test_trainer_stages.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q FP8-ready precision policy implemented with safe FP16 fallback: Added apex_x/runtime/precision.py : precision policy dataclass: PrecisionPolicy runtime detection + resolution: resolve_precision_policy(...) conservative CUDA FP8 support gate ( sm90+ + torch FP8 dtype presence) dtype helpers: dtype_name(...) execution context helper: heavy_ops_autocast_context(...) FP16 autocast path on CPU/CUDA FP8-ready no-op context pending specialized kernels/plugins Updated apex_x/runtime/__init__.py exports: PrecisionPolicy , resolve_precision_policy , dtype_name , heavy_ops_autocast_context Updated apex_x/train/trainer.py : resolves precision policy at trainer init applies heavy-op autocast context during stage-1 teacher forward adds stage metrics: heavy_ops_dtype , fp8_enabled adds precision diagnostics in train_summary[\"precision\"] : profile , device , heavy_ops_dtype fp8_requested , fp8_enabled , fallback_reason router_dtype , kan_dtype Updated apex_x/train/qat.py : expanded INT8 wrapper skip policy to preserve FP16 for KAN-like modules: default skip tokens now include \"kan\" in addition to router/gating names Added tests in tests/test_precision_policy.py : CPU fallback smoke ( balanced -> FP16 fallback with explicit reason) mocked supported CUDA path enabling FP8 for heavy ops trainer summary smoke verifying fallback diagnostics Added docs: docs/FP8.md documenting FP8 request rules, support detection, fallback contract, and smoke coverage linked from docs/index.md and mkdocs.yml Verification status: targeted checks passed: python -m ruff check apex_x/runtime/precision.py apex_x/runtime/__init__.py apex_x/train/trainer.py apex_x/train/qat.py tests/test_precision_policy.py python -m mypy --cache-dir=/dev/null apex_x/runtime/precision.py apex_x/runtime/__init__.py apex_x/train/trainer.py python -m pytest -q tests/test_precision_policy.py tests/test_trainer_stages.py tests/test_qat.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Triton fused gather+gate+scatter path scaffolded with clean fallback to reference: Environment check result for this workspace: torch.cuda.is_available() == False Triton package not installed therefore Triton kernel implementation path is unavailable in this run Added apex_x/runtime/triton_fused.py : availability model: TritonAvailability get_triton_availability() fused-result contract: FusedTileScatterResult reference fused pipeline: gather_gate_scatter_reference(...) implements: gather selected heavy/base/proxy tiles per-pixel fusion gate application scatter with overlap priority semantics via TileUnpackTorch dispatch API: gather_gate_scatter(...) attempts Triton path when requested and available cleanly falls back to reference path when unavailable or stubbed explicit Triton kernel stub: _triton_fused_kernel_stub(...) raises NotImplementedError (by design in no-Triton env) Updated apex_x/runtime/__init__.py exports: get_triton_availability gather_gate_scatter_reference gather_gate_scatter availability/result/backend dataclasses/types Added microbenchmark script: scripts/triton_fused_bench.py compares reference path vs dispatched fused path reports backend, fallback reason, and speed ratio works on CPU fallback path Added tests: tests/test_triton_fused.py : reference fused path parity vs explicit reference composition dispatch fallback behavior in no-Triton/no-CUDA case forced Triton/no-fallback path raises stub error tests/test_triton_bench.py : CPU smoke for benchmark utility Added runtime docs: docs/runtime/TRITON.md describing dispatch contracts, fallback behavior, and benchmark usage linked in docs/index.md and mkdocs.yml Verification status: targeted checks passed: python -m ruff check apex_x/runtime/triton_fused.py apex_x/runtime/__init__.py tests/test_triton_fused.py tests/test_triton_bench.py scripts/triton_fused_bench.py python -m mypy --cache-dir=/dev/null apex_x/runtime/triton_fused.py apex_x/runtime/__init__.py python -m pytest -q tests/test_triton_fused.py tests/test_triton_bench.py tests/test_tile_pack_torch.py tests/test_tile_unpack_torch.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q CPU performance regression suite implemented with baseline comparison gates: Added reusable perf suite module: apex_x/bench/perf.py fixed-size infer benchmark ( ApexXModel.forward on [1,3,128,128] ) microbenchmarks: TilePackTorch TileUnpackTorch FusionGate report + compare helpers: run_cpu_perf_suite(...) compare_against_baseline(...) JSON read/write helpers Updated apex_x/bench/__init__.py exports: perf suite and compare utilities exposed Replaced scripts/perf_regression.py : runs suite and writes current JSON report optional baseline-template emit compare mode with pass/fail exit status for CI gating artifacts: current report JSON comparison summary JSON Added committed CPU baseline: scripts/perf_baseline_cpu.json per-metric tolerances via: max_regression_ratio max_regression_abs_ms Added tests: tests/test_perf_regression.py suite smoke coverage baseline compare pass/fail behavior Added documentation: docs/PERF.md linked from docs/index.md and mkdocs.yml Updated CI workflow: .github/workflows/ci.yml now includes perf-regression job on ubuntu-latest (CPU-only) job executes scripts/perf_regression.py --compare ... job uploads perf artifacts ( perf_current_ci.json , perf_compare_ci.json ) Verification status: targeted checks passed: python -m ruff check apex_x/bench/perf.py apex_x/bench/__init__.py scripts/perf_regression.py tests/test_perf_regression.py python -m mypy --cache-dir=/dev/null apex_x/bench/perf.py apex_x/bench/__init__.py scripts/perf_regression.py python -m pytest -q tests/test_perf_regression.py python scripts/perf_regression.py --compare --baseline scripts/perf_baseline_cpu.json --output artifacts/perf_current_test.json --summary artifacts/perf_compare_test.json --infer-iters 15 --micro-iters 25 --infer-warmup 3 --micro-warmup 3 full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q TensorRT + Go runtime scaffolding added (Task A/B): TensorRT C++ scaffold created under runtime/tensorrt/ : CMakeLists.txt with optional feature probes: APEXX_ENABLE_TENSORRT only when NvInfer.h is found APEXX_ENABLE_CUDA only when CUDA compiler is available stub plugin interfaces/sources: TilePack TileSSMScan TileUnpackFusion optional DecodeNMS utility binary: apexx_trt_plugin_info (prints build summary and plugin flags) TensorRT docs added: docs/runtime/TENSORRT.md contract mapping from plugin spec to scaffold classes guarded build instructions: cd runtime/tensorrt && cmake -S . -B build && cmake --build build -j Go microservice scaffold created under runtime/go/ : service entrypoint: runtime/go/cmd/apexx-runtime/main.go endpoints: POST /predict GET /health GET /metrics short-window batching queue with per-request budget profile support ( quality|balanced|edge ) adapters: ONNX Runtime CPU baseline scaffold ( ORTAdapter ) TensorRT CGO scaffold with build tags: //go:build tensorrt && cgo default fallback returns clear unavailable error containerization: runtime/go/Dockerfile runtime/go/docker-compose.yml tests: runtime/go/internal/service/batcher_test.go runtime/go/internal/service/http_test.go CI/docs integration updates: .github/workflows/ci.yml now includes go-runtime job ( go test ./... in runtime/go ) mkdocs.yml + docs/index.md now link docs/runtime/TENSORRT.md runtime/README.md and root README.md updated with runtime scaffold usage Build/run commands verified for scaffolds: Go tests: cd runtime/go && go test ./... Go service: cd runtime/go && go run ./cmd/apexx-runtime -addr :8080 -adapter onnxruntime TensorRT scaffold build commands documented (not executed in this environment due missing cmake ): cd runtime/tensorrt && cmake -S . -B build && cmake --build build -j Remaining work from this milestone: implement real TensorRT plugin classes ( IPluginV2DynamicExt ) + serialization replace ORT stub adapter with true ONNX Runtime session execution implement TensorRT CGO adapter bridge to compiled plugin/runtime binaries add optional gRPC server for the Go service (HTTP baseline is complete) Runtime capability detection module implemented: Added apex_x/runtime/caps.py with unified runtime probe object: RuntimeCaps cuda: CudaCaps triton: TritonCaps tensorrt: TensorRTCaps fp8: FP8Caps exported from apex_x/runtime/__init__.py Detection coverage: CUDA availability + device name + compute capability Triton availability + version TensorRT: Python package/module availability header availability ( NvInfer.h / NvInferRuntime.h ) via: explicit header_search_paths env hints ( TENSORRT_INCLUDE_DIR , TRT_INCLUDE_DIR , TENSORRT_ROOT , TRT_ROOT , CUDA_HOME , CUDA_PATH ) common include directories INT8 availability gate for TRT usage ( BuilderFlag.INT8 + CUDA) FP8 availability gate: torch FP8 dtype support CUDA presence compute capability sm90+ Added tests (CPU-safe, mock-driven): tests/test_caps_runtime.py tests/test_caps_tensorrt_fp8.py Added docs: docs/runtime/CAPS.md linked in docs/index.md and mkdocs.yml Usage instructions: basic: from apex_x.runtime import detect_runtime_caps caps = detect_runtime_caps() caps.to_dict() explicit TRT header path: detect_runtime_caps(header_search_paths=[\"/usr/local/TensorRT/include\"]) Validation status: python -m pytest -q tests/test_caps_runtime.py tests/test_caps_tensorrt_fp8.py python -m ruff check apex_x/runtime/caps.py tests/test_caps_runtime.py tests/test_caps_tensorrt_fp8.py python -m mypy --cache-dir=/dev/null apex_x/runtime/caps.py apex_x/runtime/__init__.py Runtime parity framework implemented: Added apex_x/runtime/parity.py with backend-agnostic parity APIs: ParityCase , run_parity_case(...) , evaluate_parity_outputs(...) tolerance controls: NumericTolerance ToleranceConfig ( default , fp16 , bf16 , int8 ) reporting objects: TensorParityStats ParityReport format_parity_report(...) Determinism contract: run_parity_case(...) calls seed_all(seed, deterministic=...) before input generation Metrics emitted per compared tensor: max_abs_err , mean_abs_err max_rel_err , mean_rel_err mismatch_count , total_count , mismatch_ratio pass/fail against configured tolerance + mismatch-ratio limit Exported from apex_x/runtime/__init__.py for direct runtime use Added tests: tests/test_parity_framework_core.py tests/test_parity_framework_tolerances.py tests are CPU-safe and use small shapes for CI speed Added documentation: docs/runtime/PARITY.md linked in docs/index.md and mkdocs.yml Usage instructions: create a ParityCase with input_factory , reference_fn , and candidate_fn run run_parity_case(case, seed=..., deterministic=True) serialize/report with report.to_dict() or format_parity_report(report) Validation status: python -m ruff check apex_x/runtime/parity.py apex_x/runtime/__init__.py tests/test_parity_framework_core.py tests/test_parity_framework_tolerances.py python -m mypy --cache-dir=/dev/null apex_x/runtime/parity.py apex_x/runtime/__init__.py python -m pytest -q tests/test_parity_framework_core.py tests/test_parity_framework_tolerances.py .venv/bin/mkdocs build --strict Triton TilePack gather kernel implemented with fallback dispatch: Added new kernel module: apex_x/kernels/triton/tilepack.py Added package exports: apex_x/kernels/__init__.py apex_x/kernels/triton/__init__.py Implemented Triton kernel contract: Input: F[B,C,H,W] contiguous NCHW Input indices: idx[B,K] integer ( int32 kernel path; int64 accepted and cast) Output: P[B,K,C,t,t] contiguous Kernel path support: fp16 , bf16 on CUDA no Python tile loops in kernel gather path Added vectorized reference fallback: tilepack_reference(...) uses tensor gather (no per-tile Python loops) Added dispatch behavior: tilepack_dispatch(...) falls back when Triton/CUDA unavailable falls back when requires_grad and inference_only=True reason: Triton path is inference-oriented without custom backward registration Added parity tests: tests/test_triton_tilepack_parity_dispatch.py tests/test_triton_tilepack_parity_gpu.py GPU parity auto-skips when Triton/CUDA unavailable Added benchmark: apex_x/bench/triton_tilepack_bench.py Added docs: docs/runtime/TRITON_TILEPACK.md docs/runtime/TRITON.md updated with TilePack status docs navigation updated in docs/index.md and mkdocs.yml Run commands: tests: python -m pytest -q tests/test_triton_tilepack_parity_dispatch.py tests/test_triton_tilepack_parity_gpu.py bench (module): python -m apex_x.bench.triton_tilepack_bench --iters 50 --warmup 10 --batch 1 --channels 128 --height 128 --width 128 --tile-size 8 --kmax 32 --dtype fp16 lint/type: python -m ruff check apex_x/kernels/triton/tilepack.py apex_x/bench/triton_tilepack_bench.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tilepack.py apex_x/bench/triton_tilepack_bench.py Validation status: python -m ruff check ... passed python -m mypy --cache-dir=/dev/null ... passed python -m pytest -q tests/test_triton_tilepack_parity_dispatch.py tests/test_triton_tilepack_parity_gpu.py passed (GPU tests skipped on CPU-only env) .venv/bin/mkdocs build --strict passed Triton TileUnpack scatter kernel extended to overlap + priority semantics: Added kernel module: apex_x/kernels/triton/tileunpack.py Added Triton kernel dispatch/availability API: get_triton_tileunpack_availability() tileunpack_reference(...) tileunpack_triton(...) tileunpack_dispatch(...) Added exports: apex_x/kernels/triton/__init__.py Implemented semantics: Inputs: F_base[B,C,H,W] , P_out[B,K,C,t,t] , and idx/meta Output: F_merged[B,C,H,W] deterministic overlap overwrite with priorities: per-tile levels[B,K] (higher level wins) or pre-sorted K-order ( assume_priority_sorted=True ) as implicit priority default mode: overlap_mode=\\\"override\\\" (priority overwrite) optional mode: overlap_mode=\\\"blend\\\" (currently reference fallback) Updated tests: tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py includes synthetic overlap fixtures and parity against reference behavior Updated microbenchmark: apex_x/bench/triton_tileunpack_bench.py supports overlap stress via --overlap-shift supports level-aware runs via default levels ( --no-levels to disable) Added docs: docs/runtime/TRITON_TILEUNPACK.md updated docs/runtime/TRITON.md updated docs nav ( docs/index.md , mkdocs.yml ) Run commands: parity tests: python -m pytest -q tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py microbench: python -m apex_x.bench.triton_tileunpack_bench --batch 1 --channels 128 --height 128 --width 128 --tile-size 8 --kmax 32 --overlap-shift 4 --warmup 10 --iters 50 --dtype fp16 lint/type: python -m ruff check apex_x/kernels/triton/tileunpack.py apex_x/bench/triton_tileunpack_bench.py tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tileunpack.py apex_x/bench/triton_tileunpack_bench.py Validation status: python -m ruff check ... passed python -m mypy --cache-dir=/dev/null ... passed python -m pytest -q tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py passed (GPU tests skipped on CPU-only env) python -m apex_x.bench.triton_tileunpack_bench --iters 3 --warmup 1 --batch 1 --channels 8 --height 32 --width 32 --tile-size 4 --kmax 4 --overlap-shift 2 --dtype fp16 executed successfully (reference backend on CPU) .venv/bin/mkdocs build --strict passed Triton FusionGate alpha/fusion kernels implemented with fallback dispatch: Added kernel module: apex_x/kernels/triton/fusiongate.py Added exports: apex_x/kernels/triton/__init__.py Implemented kernels and dispatch: alpha kernel: inputs: boundary/uncertainty proxies ( [B,1,H,W] or [B,H,W] ) output: alpha[B,1,H,W] formula: alpha = sigmoid(softplus(w_b) * boundary + softplus(w_u) * uncertainty + bias) optional fusion kernel: fused = base + alpha * (detail - base) supports optional in-place output in dispatch API fallback semantics: falls back to reference when Triton/CUDA unavailable falls back when autograd is requested and inference_only=True Added tests: tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py coverage: parity vs apex_x.model.FusionGate.compute_alpha (simplified alpha path) alpha range checks ( [0,1] ) optional fusion parity GPU parity auto-skip without CUDA+Triton Added microbenchmark: apex_x/bench/triton_fusiongate_bench.py measures: alpha reference vs dispatch alpha+fusion reference vs dispatch Added docs: docs/runtime/TRITON_FUSION.md updated: docs/runtime/TRITON.md docs/index.md mkdocs.yml Run commands: tests: python -m pytest -q tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py benchmark: python -m apex_x.bench.triton_fusiongate_bench --batch 1 --channels 128 --height 128 --width 128 --warmup 10 --iters 50 --dtype fp16 lint/type: python -m ruff check apex_x/kernels/triton/fusiongate.py apex_x/bench/triton_fusiongate_bench.py tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/fusiongate.py apex_x/bench/triton_fusiongate_bench.py Validation status: python -m ruff check ... passed python -m mypy --cache-dir=/dev/null ... passed python -m pytest -q tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py passed (GPU tests skipped on CPU-only env) python -m apex_x.bench.triton_fusiongate_bench --iters 3 --warmup 1 --batch 1 --channels 8 --height 32 --width 32 --dtype fp16 executed successfully (reference backend on CPU) .venv/bin/mkdocs build --strict passed Invariants to Preserve Deterministic inference tile selection under fixed config Fixed Kmax -buffer shape contract for runtime compatibility No Python-side dynamic control flow in future export graph path CPU baseline must remain runnable at all times Open Risks Tile-SSM is currently a placeholder scan, not final kernel-equivalent behavior Detection/segmentation heads are minimal baseline stubs Runtime plugins are currently specification-only, not implemented Immediate Next Steps Expand baseline heads to full DET + INST-SEG proto path per spec. Add explicit continuous-budget training loop example with dual mu update. Add deterministic quadtree L1/L2 split implementation and tests. Add export smoke tests (ONNX contract checks with fixed Kmax ). Add perf threshold config file and CI perf guard for CPU baseline. Add CLI entrypoints (Typer + Rich) for run , test , and perf commands. Add initial concrete losses/ , train/ , infer/ , and export/ implementations beyond placeholders. Add a typed Typer command for config validate --config path.yaml --set key=value using new loader/override utilities. Add logging configuration knobs into RuntimeConfig (log level/format) and route through configure_logging() . Add apex-x config validate and apex-x config dump subcommands for explicit config workflows. Add docs deployment workflow (e.g., GitHub Pages) after docs structure stabilizes. Add real TRT/ORT RuntimeAdapterProtocol implementations behind feature flags. Add smoke example invocation to README and optionally CI as a dedicated quick check. Add ADR template/checklist for future decisions to keep decision records uniform. Integrate L0 mapping helpers directly inside pack/unpack metadata paths (store tile (ty,tx) alongside pixel origins) for easier runtime plugin parity checks. Add non-square grid Hilbert fixture coverage (e.g., 3x5 , 5x3 ) to lock padded-power-of-two traversal behavior. Add explicit fixture snapshots for scan modes ( l2r/r2l/u2d/d2u ) on representative non-square grids and enforce them in CI. Connect split-budget selection ( B2/B3 ) in inference path to quadtree depth-2 mappings/metadata and add end-to-end selection tests. Integrate TileSelection / TileSelectionTrace emission into model inference outputs and add CLI flag to dump selection traces for ablations. Add optional CLI command to generate overlay images from stored TileSelectionTrace JSON for quick qualitative routing inspection. Wire StaticCostModel into routing/inference selection path so budgeting uses per-level C_c/C_h + pack/unpack/split overhead directly instead of scalar placeholders. Integrate sample_oracle_set(...) into training loops so oracle subset S is produced from random + uncertainty-biased policies directly from PV u_hat . Add budget-selection debug artifact that logs per-tile score/rank and final stable tie-break order for exact replay in ablation runs. Integrate PV aggregation output x_i into router training/inference path so utility heads consume pooled mean/max/var vectors instead of placeholder signals. Wire RouterTinyMLP into model/routing execution path as the default trainable router backend (with config-selectable fallback to IdentityRouter ). Add config switch for router backend ( identity / tiny_mlp / kan_like ) and wire RouterKANLike into inference/training stubs. Integrate ste_gate_from_utilities(...) into training stubs so router utilities produce p_i / g_i directly in continuous-budget examples. Wire BudgetDualController into training stubs so mu , E[C] , and budget term are tracked/updated per step with debug logs. Use GreedySelectionResult.kmax_buffer + valid_count directly in model inference outputs to mirror runtime plugin shape contracts. Integrate deterministic_two_stage_selection(...) into model inference path so B1/B2 and L1 routing are exercised end-to-end in CPU baseline outputs. Use hysteresis_rollout(...) in temporal/video inference stubs and log count_mask_toggles(...) as an anti-flicker metric. Extend routing diagnostics to include L1/L2 selection once two-stage routing is wired into the model forward path. Add optional artifact export for diagnostics snapshots (JSON + histogram plots) from CLI train/predict commands for ablation workflows. Wire toggle states into YAML examples/README config snippets so users can reproduce dense/no-SSM/no-nesting baselines quickly. Integrate torch tile pack/unpack path into runtime adapter abstractions and add a CPU fallback selection path for adapter-level smoke tests. Integrate FusionGate into the model forward path (or runtime adapter path) to replace direct heavy overwrite with proxy-conditioned fusion in CPU baseline experiments. Integrate CheapBlock into PV/FF cheap path stubs and add micro-benchmarks for block latency under CPU profiles. Integrate TileRefineBlock after Tile-SSM in the model forward path so packed-tile local refinement is exercised end-to-end in baseline outputs. Wire PVBackbone into model execution path as the canonical PV stream source ( P3/P4/P5 ) and align routing signals to these outputs. Integrate PVModule into ApexXModel.forward so routing and diagnostics consume PV coarse proxies instead of handcrafted tile-signal placeholders. Replace/augment NumPy tile_ssm_scan usage in ApexXModel.forward with StableStateSpaceScan in torch execution paths and add parity checks for inference outputs. Add ApexXModel config switch for scan direction mode ( forward vs bidirectional ) and wire merge-gated bidirectional scan into packed-tile path. Wire decode_and_nms(...) into model/inference outputs so DET head predictions use the new deterministic decode/NMS path in end-to-end CPU runs. Wire PrototypeInstanceSegHead into end-to-end model/infer path (using DET-selected instances) and expose assembled masks in CLI predict outputs. Integrate instance_segmentation_losses(...) into training stubs with mask/box matching targets and log BCE/Dice/boundary components in trainer diagnostics. Wire FFTileRefinementHook active-tile indices from routing outputs in model/infer path so refinement uses real FF-selected tiles end-to-end. Wire generate_panoptic_output(...) into inference/CLI outputs so panoptic maps and segments_info are emitted from DET + INST-SEG + SEM-SEG predictions end-to-end. Wire evaluate_panoptic_quality(...) into dataset evaluation loops so apex-x eval can consume real predicted/GT panoptic artifacts and report dataset-level PQ metrics. Integrate TrackEmbeddingHead into model/infer outputs and add config-controlled tracking head enable/disable behavior. Add a basic association loop wrapper in apex_x/infer that keeps TrackState across frames and emits stable track IDs in CLI predict . Add optional motion gating term into Hungarian cost/gate path (for video mode) and verify flicker reduction with temporal fixtures. Integrate apply_pcgradpp(...) into concrete training step codepath so DET/SEG grouped losses are projected on shared trunk params during optimization and surfaced in trainer diagnostics. Integrate distillation_losses(...) into the concrete training path with config-driven weights/temperature/feature-layer selection and expose per-component values in trainer diagnostics. Integrate compute_oracle_delta_targets(...) + utility_oracle_loss(...) into router training loops so sampled set S drives utility regression/ranking with detached oracle targets. Wire TeacherModel into train/eval loops so EMA updates, distill outputs, and student-teacher loss plumbing are exercised end-to-end with config toggles. Expand ApexXTrainer stage loop from smoke-level synthetic batches to dataset-backed dataloaders with checkpointing/resume support. Add stage-aware CLI logging/artifacts ( stage_metrics.json , mu_history.json ) for ablation reproducibility. Add CI smoke command for staged training CLI ( apex-x train --steps-per-stage 1 ) to guard regressions in train wiring. Wire TransformPipeline and MosaicV2 into an actual dataset/dataloader path controlled by DataConfig knobs ( flip_prob , mosaic_prob , scale range). Add serialization/debug helpers to visualize transformed boxes/masks and mosaic split/crop decisions for reproducible augmentation ablations. Add dataset-wide evaluation loops that consume real model predictions and emit the new eval report (JSON/MD) directly from inference artifacts, beyond tiny fixture mode. Extend ablation runner to ingest real dataset eval outputs (instead of tiny fixture metrics) and add per-toggle significance summaries across seeds. Expand QAT coverage beyond Conv/Linear wrappers to selected normalization-sensitive blocks with explicit parity gates versus FP16 baseline. Add runtime-backed FP8 kernel probe path (beyond capability check) and enforce parity/perf gates before enabling FP8-by-default on compatible GPUs. Implement real Triton fused gather+gate+scatter kernel and wire it under gather_gate_scatter(...) dispatch when CUDA+Triton are available; add parity + perf thresholds against reference path. Add dataset/profile-specific perf baselines (e.g., quality/balanced/edge configs) and split tolerances by CPU model class for stricter regression gates. Latest Update (2026-02-08): Triton Fused Stage-1 Pipeline Added a new practical fused Triton fast path module: apex_x/kernels/triton/fused_pack_op_unpack.py Implements gather -> pointwise affine + ReGLU-like gate -> scatter in one Triton kernel launch sequence. Added dispatch + fallback API: get_triton_fused_stage1_availability() fused_pack_op_unpack_reference(...) fused_pack_op_unpack_triton(...) fused_pack_op_unpack_dispatch(...) Determinism rule for Stage-1 path: requires unique tile indices per batch row to avoid overlap write races. Added exports: apex_x/kernels/triton/__init__.py now exports fused Stage-1 APIs. Added parity tests: tests/test_triton_fused_stage1_dispatch.py tests/test_triton_fused_stage1_gpu.py Added microbenchmark: apex_x/bench/triton_fused_stage1_bench.py compares: explicit reference composition ( pack -> op -> unpack ) separate dispatch composition ( TilePack dispatch -> op -> TileUnpack dispatch ) fused Stage-1 dispatch Added docs: docs/runtime/TRITON_FUSED_STAGE1.md updated: docs/runtime/TRITON.md docs/index.md mkdocs.yml Run Commands Tests: python -m pytest -q tests/test_triton_fused_stage1_dispatch.py tests/test_triton_fused_stage1_gpu.py Microbenchmark: python -m apex_x.bench.triton_fused_stage1_bench --batch 1 --channels 128 --height 128 --width 128 --tile-size 8 --kmax 32 --warmup 10 --iters 50 --dtype fp16 Lint/type quick checks: python -m ruff check apex_x/kernels/triton/fused_pack_op_unpack.py tests/test_triton_fused_stage1_dispatch.py tests/test_triton_fused_stage1_gpu.py apex_x/bench/triton_fused_stage1_bench.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/fused_pack_op_unpack.py apex_x/bench/triton_fused_stage1_bench.py Remaining Work Wire Stage-1 fused kernel into legacy runtime entrypoint: apex_x/runtime/triton_fused.py::gather_gate_scatter(...) Extend fused kernel beyond Stage-1 local transform: add overlap-priority semantics in-kernel where needed integrate Tile-SSM-related fused blocks (next stages) Add GPU CI perf threshold gates for speedup_separate_over_fused . Latest Update (2026-02-08): Triton TileSSM Scan Baseline Added Triton TileSSM scan module: apex_x/kernels/triton/tilessm_scan.py Forward-only recurrence scan over tokens tokens[B,K,C] with stable sanitization/clamping. Outputs: y[B,K,C] final_state[B,C] Added availability + dispatch API: get_triton_tilessm_availability() tilessm_scan_reference(...) tilessm_scan_triton(...) tilessm_scan_dispatch(...) Dispatch keeps training-safe behavior: inference_only=True falls back to reference when autograd is active. Exported TileSSM API: apex_x/kernels/triton/__init__.py Integrated inference path into model heavy FF scan: updated apex_x/model/ff_heavy_path.py new use_triton_inference_scan toggle eval mode uses tilessm_scan_dispatch(...) train mode keeps torch scan path ( StableStateSpaceScan / StableBidirectionalStateSpaceScan ) updated apex_x/model/ff_module.py routes RuntimeConfig.enable_runtime_plugins to FFHeavyPath(..., use_triton_inference_scan=...) Added tests: tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py Added throughput benchmark: apex_x/bench/triton_tilessm_bench.py Added docs: docs/runtime/TRITON_SSM.md updated: docs/runtime/TRITON.md docs/index.md mkdocs.yml Run Commands Tests: python -m pytest -q tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py tests/test_ff_heavy_path.py Benchmark: python -m apex_x.bench.triton_tilessm_bench --batch 2 --steps 256 --channels 128 --warmup 10 --iters 50 --dtype fp16 Lint/type checks: python -m ruff check apex_x/kernels/triton/tilessm_scan.py apex_x/model/ff_heavy_path.py apex_x/model/ff_module.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tilessm_scan.py apex_x/model/ff_heavy_path.py apex_x/model/ff_module.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py Docs: .venv/bin/mkdocs build --strict Validation Status ruff : passed on changed TileSSM files. mypy : passed on changed TileSSM files. pytest : passed for new parity/integration tests (GPU tests auto-skipped on CPU-only environment). benchmark smoke run: completed on CPU fallback path. docs build: passed with strict mode. Remaining Work Add multi-direction scan execution mode in Triton TileSSM path (current kernel is forward-only baseline). Add a fused TileSSM + tile-local refine path after this baseline. Add GPU CI lane for TileSSM parity/perf thresholds when CUDA runners are available. Latest Update (2026-02-08): Triton TileSSM Multi-Direction Extended apex_x/kernels/triton/tilessm_scan.py to support directional scanning: direction : forward , backward , bidirectional merge_mode for bidirectional: sum , avg , gated optional torch-computed merge_gate for gated merge ( [C] or [B,1,C] ) Added clean directional API: scan(tokens, direction=...) -> y routes through dispatch with fallback behavior Kept training/inference separation: training/backward still uses torch scan path inference dispatch can use Triton path ( inference_only=True ) Updated FF inference integration: apex_x/model/ff_heavy_path.py Triton inference path now uses directional dispatch ( forward and backward ) and applies learned torch gate for merge in bidirectional mode. Updated exports: apex_x/kernels/triton/__init__.py now exports: ScanDirection BidirectionalMergeMode scan Updated benchmark for multi-direction overhead: apex_x/bench/triton_tilessm_bench.py now reports forward/backward/bidirectional timings and overhead ratios vs forward. Updated tests: tests/test_triton_tilessm_parity_dispatch.py added backward parity vs torch manual recurrence added bidirectional parity for sum/avg/gated added clean API scan(...) test tests/test_triton_tilessm_parity_gpu.py added bidirectional avg parity test (GPU) Updated docs: docs/runtime/TRITON_SSM.md docs/runtime/TRITON.md Run Commands Tests: python -m pytest -q tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py tests/test_ff_heavy_path.py Benchmark: python -m apex_x.bench.triton_tilessm_bench --batch 2 --steps 256 --channels 128 --warmup 10 --iters 50 --dtype fp16 Lint/type: python -m ruff check apex_x/kernels/triton/tilessm_scan.py apex_x/kernels/triton/__init__.py apex_x/model/ff_heavy_path.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tilessm_scan.py apex_x/kernels/triton/__init__.py apex_x/model/ff_heavy_path.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py Docs: .venv/bin/mkdocs build --strict Validation Status ruff : passed mypy : passed pytest : passed (GPU tests skipped on CPU-only environment) benchmark smoke run: passed (reference fallback on CPU) docs build ( mkdocs --strict ): passed Latest Update (2026-02-08): TensorRT Build Hardening + Harness Read runtime specs from: docs/runtime/PLUGIN_SPEC.md (canonical) docs/runtime/TENSORRT.md Added alias page: docs/runtime/PLUGIN_SPECS.md -> points to canonical spec Hardened TensorRT CMake in runtime/tensorrt/CMakeLists.txt : shared plugin library support: apexx_trt_plugins (SHARED) added always-build static core: apexx_trt_plugin_core (STATIC, PIC) compile-guard behavior: shared plugin library builds only when TensorRT headers and CUDA compiler are found if TRT/CUDA unavailable, shared build is skipped cleanly and repo remains buildable plugin info target kept available: apexx_trt_plugin_info harness target added conditionally (only with shared build): apexx_trt_plugin_harness Added minimal plugin enqueue-like path for stubs: runtime/tensorrt/include/apexx_trt/plugin_stub.hpp DummyTensor , PluginEnqueueInputs , PluginEnqueueOutputs , PluginStub::enqueue(...) implemented enqueue methods in: runtime/tensorrt/src/tile_pack_plugin.cpp runtime/tensorrt/src/tile_ssm_scan_plugin.cpp runtime/tensorrt/src/tile_unpack_fusion_plugin.cpp runtime/tensorrt/src/decode_nms_plugin.cpp Added shared-library C ABI entrypoints in: runtime/tensorrt/include/apexx_trt/common.hpp runtime/tensorrt/src/common.cpp symbols: apexx_trt_abi_version() apexx_trt_build_summary_cstr() apexx_trt_invoke_minimal(...) Added minimal runtime harness executable source: runtime/tensorrt/tests/plugin_harness_main.cpp harness behavior: loads plugin shared library via dlopen / LoadLibrary resolves C ABI symbols creates dummy tensors invokes minimal plugin call path for TilePack/TileSSMScan/TileUnpackFusion/DecodeNMS Added build doc: docs/runtime/TENSORRT_BUILD.md Updated docs navigation and runtime note cross-links: mkdocs.yml docs/index.md docs/runtime/TENSORRT.md Exact Build Commands Auto-detect build: cd runtime/tensorrt cmake -S . -B build cmake --build build -j ./build/apexx_trt_plugin_info Explicit TRT/CUDA paths: cd runtime/tensorrt cmake -S . -B build -DTENSORRT_INCLUDE_DIR=\\\"${TENSORRT_ROOT}/include\\\" -DCMAKE_CUDA_COMPILER=\\\"${CUDA_HOME}/bin/nvcc\\\" cmake --build build -j Force skip shared plugin build (portable/CI machines): cd runtime/tensorrt cmake -S . -B build -DAPEXX_ENABLE_TENSORRT=OFF -DAPEXX_ENABLE_CUDA=OFF -DAPEXX_BUILD_PLUGIN_TEST_HARNESS=OFF cmake --build build -j Harness run (when shared plugin target is built): ./build/apexx_trt_plugin_harness ./build/libapexx_trt_plugins.so or: export APEXX_TRT_PLUGIN_LIB=./build/libapexx_trt_plugins.so ./build/apexx_trt_plugin_harness Environment Variables TENSORRT_ROOT : TensorRT install root (optional) CUDA_HOME : CUDA root (optional) CMAKE_PREFIX_PATH : dependency discovery override (optional) APEXX_TRT_PLUGIN_LIB : path to shared plugin library for harness runtime loading Validation Status mkdocs build --strict : passed pytest tests/test_import_smoke.py : passed Local CMake configure/build execution could not be run in this environment because cmake binary is not installed ( command not found ). Update Protocol (Every Significant Change) Update this file with: what changed why it changed what to do next If architecture changed, also update docs/DECISIONS.md If requirements changed, update PRD/spec first, then code","title":"Context"},{"location":"CONTEXT/#apex-x-project-context-persistent-memory","text":"","title":"Apex-X Project Context (Persistent Memory)"},{"location":"CONTEXT/#authoritative-links-mandatory","text":"PRD: docs/PRD.md Engineering spec: docs/ENGINEERING_SPEC.md Runtime plugin spec: docs/runtime/PLUGIN_SPEC.md Decisions log: docs/DECISIONS.md Active worklist: docs/TODO.md","title":"Authoritative Links (Mandatory)"},{"location":"CONTEXT/#project-identity","text":"Name: apex-x Version: 0.1.0 License: Apache-2.0 Current baseline: CPU-only reference implementation","title":"Project Identity"},{"location":"CONTEXT/#current-architecture-snapshot","text":"Dual-stream concept established in docs (PV dense + FF sparse) Utility-based router contracts defined Continuous and deterministic budgeting contracts defined Quadtree nesting policy defined ( L0/L1/L2 ) TilePack/TileUnpack and ordering contracts defined Tile-SSM placeholder behavior defined","title":"Current Architecture Snapshot"},{"location":"CONTEXT/#what-exists-right-now-2026-02-07","text":"Repository scaffold created: apex_x/ , tests/ , docs/ , docs/runtime/ , examples/ , scripts/ , runtime/ , .github/workflows/ Governance/Open-source files added: LICENSE , CODE_OF_CONDUCT.md , CONTRIBUTING.md , SECURITY.md Authoritative docs added/updated: docs/PRD.md (full) docs/ENGINEERING_SPEC.md (full) docs/runtime/PLUGIN_SPEC.md CPU baseline code added: apex_x/config/schema.py apex_x/routing/core.py apex_x/tiles/ops.py apex_x/utils/ssm.py apex_x/model/core.py Validation assets added: tests/test_router.py tests/test_tile_ops.py tests/test_model.py scripts/perf_regression.py .github/workflows/ci.yml Tooling and developer workflow baseline: pyproject.toml now targets python>=3.11 runtime deps: torch , numpy , typer , rich , pydantic dev deps/tools: pytest , ruff , black , mypy , pre-commit pre-commit hooks: .pre-commit-config.yaml CI now runs lint + typecheck + tests on ubuntu-latest with CPU torch index Package skeleton and public API surfaces: Created package layout: apex_x/config/ , apex_x/model/ , apex_x/tiles/ , apex_x/routing/ , apex_x/losses/ , apex_x/train/ , apex_x/infer/ , apex_x/data/ , apex_x/export/ , apex_x/bench/ , apex_x/runtime/ , apex_x/utils/ Root API now exports required surfaces in apex_x/__init__.py : ApexXConfig , ApexXModel Router , BudgetController TilePack , TileUnpack Exporter Added import smoke coverage in tests/test_import_smoke.py Migrated baseline code into package modules and removed legacy flat modules to avoid namespace ambiguity Nested configuration system implemented: New nested config domains in apex_x/config/schema.py : ModelConfig (profiles, channels, strides, tile sizes, Kmax, nesting depth) RoutingConfig (budgets B/B1/B2/B3, costs, hysteresis/split thresholds) TrainConfig (curriculum, dual- mu parameters, distill weights, PCGrad++, QAT toggles) DataConfig (COCO paths and augmentation knobs) RuntimeConfig (precision profile, export/runtime toggles) Top-level ApexXConfig now nests all sections and performs cross-section validation Added YAML + CLI-style override support in apex_x/config/io.py : load_yaml_config(path, overrides=...) apply_overrides(cfg, [\\\"section.key=value\\\", ...]) Added config validation test coverage in tests/test_config.py + fixture tests/fixtures/apex_x_config.yaml Updated model to consume nested config fields in apex_x/model/core.py Added PyYAML + types-PyYAML to project dependencies for runtime + typing support Reproducibility and logging utilities implemented: Added apex_x/utils/repro.py : seed_all() set_deterministic_mode() deterministic_mode() context manager get_determinism_state() reproducibility_notes() (CPU vs CUDA behavior notes) Added apex_x/utils/logging.py : configure_logging() get_logger() shared apex_x.* logger namespace log_event() structured key/value logging with rich Wired shared logger usage in: apex_x/config/io.py apex_x/model/core.py Added determinism tests in tests/test_repro.py CLI surface implemented: Added Typer CLI entrypoint in apex_x/cli.py with commands: apex-x train apex-x eval apex-x predict apex-x bench apex-x ablate apex-x export All commands load config via YAML and support repeated --set section.key=value overrides Added console script entrypoint in pyproject.toml : [project.scripts] apex-x = \\\"apex_x.cli:main\\\" Added CLI parsing/behavior tests in tests/test_cli.py Documentation scaffold implemented: Added MkDocs config in mkdocs.yml Added docs home page in docs/index.md linking PRD/spec/runtime/context/decisions/TODO Added docs build instructions in docs/index.md and README.md Added docs dependency group in pyproject.toml : .[docs] with mkdocs Added CI docs build job in .github/workflows/ci.yml running: mkdocs build --strict Protocol typing standardization implemented: Added explicit protocol names and aliases for consistency: RouterProtocol BudgetControllerProtocol TilePackerProtocol RuntimeAdapterProtocol Kept backward-compatible aliases in existing modules ( Router , BudgetController , TilePack , etc.) Added runtime adapter interface + reference adapter: apex_x/runtime/interfaces.py apex_x/runtime/adapters.py ( NullRuntimeAdapter ) Updated model typing to consume protocol-based interfaces in apex_x/model/core.py Added minimal protocol-conformance tests: tests/test_protocols.py Updated import-smoke expectations in tests/test_import_smoke.py CPU smoke example added: Added examples/smoke_cpu.py that: loads YAML config instantiates ApexXModel stub runs one forward pass on random input Added examples/smoke_cpu.yaml default config for fast CPU smoke runs Added tests/test_smoke_cpu_example.py as a quick smoke pytest Documentation governance updates: Added initial convention ADRs in docs/DECISIONS.md for: naming conventions tensor shape contracts determinism rules Expanded docs/TODO.md with known future implementation tracks: full Triton fused kernels full TensorRT plugin stack ONNX Runtime custom-op sparse path and parity gates Strengthened CONTRIBUTING.md policy to require: docs/CONTEXT.md update in every significant PR docs/DECISIONS.md update when architectural/convention decisions change L0 tiling mapping implemented and validated: Added apex_x/tiles/mapping.py with explicit L0 mapping API: l0_grid_shape(feature_h, feature_w, tile_size) with strict divisibility checks l0_tile_to_index(ty, tx, grid_h, grid_w) and l0_index_to_tile(index, grid_h, grid_w) with bounds checks batched helpers: l0_indices_to_coords(indices[B,K], grid_h, grid_w) -> coords[B,K,2] l0_coords_to_indices(coords[B,K,2], grid_h, grid_w) -> indices[B,K] Wired tile ops grid sizing to strict mapping validation: apex_x/tiles/ops.py::tile_grid_shape now uses l0_grid_shape(...) Exported mapping API through apex_x/tiles/__init__.py Added focused tests in tests/test_tile_mapping.py : index/coord bijection batched [B,K] roundtrip invalid size/divisibility out-of-bounds and shape/dtype validation Verification status: python -m pytest -q passed ruff check . passed mypy passed Hilbert ordering implemented for coordinates and indices: Added apex_x/tiles/ordering.py with explicit Hilbert APIs: hilbert_distance(tx, ty, order_n) hilbert_order_coords(grid_h, grid_w) for full-grid coordinate traversal hilbert_order_indices(indices, grid_h, grid_w) for subset index ordering hilbert_full_indices(grid_h, grid_w) for complete index traversal Updated apex_x/tiles/ops.py : order_idx(..., mode=\\\"hilbert\\\") now uses hilbert_order_indices(...) Exported ordering APIs from apex_x/tiles/__init__.py Added fixtures: tests/fixtures/hilbert_2x2.json tests/fixtures/hilbert_4x4.json tests/fixtures/hilbert_8x8.json Added fixture-driven tests in tests/test_tile_hilbert.py : exact traversal match vs fixtures determinism across repeated calls full coverage of all coordinates/indices subset index ordering stability + parity with order_idx(..., mode=\\\"hilbert\\\") Verification status: python -m pytest -q passed ruff check . passed mypy passed Scan ordering variants and stable dispatcher implemented: Extended apex_x/tiles/ordering.py with scan modes and dispatcher utilities: Scan variants: l2r , r2l , u2d , d2u Stable dispatcher: order_tile_indices(indices, grid_h, grid_w, mode=...) Mode normalization and aliases: normalize_order_mode(...) supports scan_lr/scan_rl/scan_ud/scan_du + short aliases + canonical names Scan inverse mapping helper: inverse_scan_mode(...) Explicit scan ordering APIs: scan_order_coords(grid_h, grid_w, mode) scan_order_indices(indices, grid_h, grid_w, mode) Updated apex_x/tiles/ops.py : order_idx(...) now delegates to order_tile_indices(...) (single path for ordering semantics) Exported new ordering APIs from apex_x/tiles/__init__.py Added tests in tests/test_tile_scan_ordering.py : deterministic ordering for all scan variants alias/normalization correctness stable ordering behavior on duplicate indices reversible mapping checks: L2R <-> R2L by horizontal mirror U2D <-> D2U by vertical mirror dispatcher parity with order_idx(...) Verification status: python -m pytest -q passed ruff check . passed mypy passed L0->L1 quadtree mapping and metadata implemented: Added apex_x/tiles/quadtree.py with deterministic L0/L1 mapping APIs: l1_grid_shape_from_l0(l0_grid_h, l0_grid_w) l0_l1_grid_shapes_from_feature(feature_h, feature_w, tile_size_l0, tile_size_l1) l0_to_l1_children_coords(l0_ty, l0_tx, l0_grid_h, l0_grid_w) (TL, TR, BL, BR order) l0_to_l1_children_indices(l0_index, l0_grid_h, l0_grid_w) reverse mapping: l1_to_l0_parent_coord(l1_ty, l1_tx, l0_grid_h, l0_grid_w) l1_to_l0_parent_index(l1_index, l0_grid_h, l0_grid_w) metadata builder: build_l0_l1_quadtree_meta(parent_indices, l0_grid_h, l0_grid_w) -> L0L1QuadtreeMeta Exported new quadtree APIs in apex_x/tiles/__init__.py Added tests in tests/test_tile_quadtree.py : boundary tile mapping correctness (bottom-right L0 tile to L1 children) reverse parent mapping across full L1 grids multiple config coverage via (feature_h, feature_w, tile_size_l0, tile_size_l1) parametrization metadata shape/content checks invalid ratio/divisibility/out-of-bounds validation checks Verification status: python -m pytest -q passed ruff check . passed mypy passed L2 nesting and overlap priority contract implemented: Extended apex_x/tiles/quadtree.py with L1->L2 and combined depth-2 utilities: grid shapes: l2_grid_shape_from_l1(...) l1_l2_grid_shapes_from_feature(...) l0_l1_l2_grid_shapes_from_feature(...) mappings: l1_to_l2_children_coords(...) , l1_to_l2_children_indices(...) l2_to_l1_parent_coord(...) , l2_to_l1_parent_index(...) l0_to_l2_descendant_indices(...) metadata: L1L2QuadtreeMeta L0L1L2QuadtreeMeta build_l1_l2_quadtree_meta(...) build_l0_l1_l2_quadtree_meta(...) Defined explicit overlap priority tags and helper: OVERLAP_PRIORITY_L0 = 1 OVERLAP_PRIORITY_L1 = 2 OVERLAP_PRIORITY_L2 = 3 overlap_priority_for_level(...) contract enforces L2 > L1 > L0 Exported new L2 and priority APIs through apex_x/tiles/__init__.py Added/expanded tests: tests/test_tile_quadtree.py : L1->L2 mapping correctness (including boundary tiles) L2->L1 reverse mapping correctness L0->L2 descendant index correctness combined L0/L1/L2 metadata consistency priority tag contract checks multi-config and validation coverage for depth-2 shapes tests/test_tile_ops.py : overlap behavior check using unpack priorities confirming L2 overrides L1 and L0 Verification status: python -m pytest -q passed ruff check . passed mypy passed Tile selection debug dataclasses implemented: Added apex_x/tiles/selection.py with: TileSelection fields: level , indices , ordered_indices , meta , budgets_used validation: level constrained to l0/l1/l2 ordered_indices must be a permutation of indices budgets must be finite and non-negative JSON persistence: to_dict()/from_dict() save_json()/load_json() TileSelectionTrace for multi-level selection records fields: selections , run_meta helpers: to_dict()/from_dict() save_json()/load_json() for_level(level) JSON serialization includes recursive normalization of NumPy arrays/scalars for debug/ablation dumps. Exported APIs via apex_x/tiles/__init__.py : TileSelection TileSelectionTrace Added unit tests in tests/test_tile_selection.py : roundtrip dict serialization file save/load JSON validation error cases trace roundtrip and level lookup non-empty trace guard Verification status: python -m pytest -q passed ruff check . passed mypy passed Tile overlay visualization utility implemented: Added apex_x/utils/visualization.py with dependency-free overlay rendering: draw_selected_tiles_overlay(...) supports HWC , CHW , and batch-size-1 image inputs deterministic tile overlay rendering from selected tile indices + grid/tile size fill + border blending with deterministic integer alpha math save_overlay_ppm(...) saves overlay to .ppm for debug/ablation without extra image libraries draw_and_save_selected_tiles_overlay(...) convenience wrapper combining render + save Exported visualization utilities through apex_x/utils/__init__.py Added deterministic tests in tests/test_visualization.py : overlay output shape/dtype checks stable SHA256 hash checks for rendered overlay bytes stable SHA256 hash checks for saved PPM file bytes save-path extension validation for .ppm Verification status: python -m pytest -q passed ruff check . passed mypy passed Cost model interface and reference implementation added: Extended routing interfaces in apex_x/routing/interfaces.py : CostModelProtocol backward-compatible alias CostModel Added apex_x/routing/cost_model.py : LevelCost : per-level cost terms: c_cheap (C_c) c_heavy (C_h) pack_overhead unpack_overhead split_overhead (O_split) CalibrationRecord : stores empirical calibration measurements, blend factor, and apply flag StaticCostModel : level-aware cost computations: cheap_cost(...) heavy_cost(...) delta_cost(...) split_overhead(...) expected_level_cost(...) total_cost(...) optional empirical calibration hook: apply_empirical_calibration(level, measured_timings, blend, apply) stores records in calibration_history serialization: to_dict()/from_dict() save_json()/load_json() Exported cost model symbols through: apex_x/routing/__init__.py apex_x/__init__.py (public API now includes CostModel , CostModelProtocol , StaticCostModel ) Added tests in tests/test_cost_model.py : deterministic cost computations and totals calibration update behavior and history storage JSON serialization roundtrip validation/error paths protocol conformance check ( isinstance(..., CostModelProtocol) ) Verification status: python -m pytest -q passed ruff check . passed mypy passed Oracle set sampler implemented ( S sampling for utility oracle training): Added apex_x/routing/oracle_sampling.py : OracleSetSample dataclass: indices random_indices uncertainty_indices sample_oracle_set(u_hat, random_fraction, uncertainty_fraction, seed) : random fraction sampling over all tiles uncertainty-biased sampling over remaining tiles using PV uncertainty u_hat deterministic behavior under fixed seed Exported sampler APIs through apex_x/routing/__init__.py : OracleSetSample sample_oracle_set Added tests in tests/test_oracle_sampling.py : seed determinism checks count/uniqueness checks for random + uncertainty components uncertainty-biased distribution check across many seeds validation/error checks for fraction bounds and invalid uncertainty values Verification status: python -m pytest -q passed ruff check . passed mypy passed Stable tie-breaking helper for selections implemented: Updated apex_x/routing/core.py : added stable_rank_tile_ids(scores) with deterministic ordering policy: primary key: score descending secondary key: tile id ascending wired greedy_utility_per_cost(...) to use stable_rank_tile_ids(...) for score-ratio ranking Exported helper via apex_x/routing/__init__.py : stable_rank_tile_ids Added tests in tests/test_router.py : explicit tie-break behavior validation repeat-run determinism checks integration check that greedy selection follows stable tie-breaking under equal utility/cost ratios Verification status: python -m pytest -q passed ruff check . passed mypy passed PV->FF tile vector aggregation implemented: Added apex_x/routing/aggregation.py : compute_ff_tile_bounds_on_pv_grid(...) computes PV-aligned bounds for each FF tile on coarse PV maps aggregate_pv_maps_to_ff_tile_vectors(...) pools per-tile stats from PV maps aligned to FF grid supported stats: mean , max , var deterministic feature ordering using sorted PV map names PVTileAggregation dataclass: vectors ( [B, K, D] ) tile_bounds_pv ( [K,4] ) feature_layout (feature names per channel/stat/map) Exported aggregation APIs via apex_x/routing/__init__.py : PVTileAggregation compute_ff_tile_bounds_on_pv_grid aggregate_pv_maps_to_ff_tile_vectors Added tests in tests/test_pv_aggregation.py : alignment on simple integer scale mapping pooled stat correctness ( mean/max/var ) output shape and feature layout checks deterministic outputs across map ordering non-integer scale edge/boundary alignment validation error paths Verification status: python -m pytest -q passed ruff check . passed mypy passed RouterTinyMLP implemented with utility/split/temporal outputs: Added apex_x/routing/tiny_mlp.py : RouterTinyOutput dataclass carrying per-tile tensors: U ( [B,K] ) utility logits S ( [B,K] ) split utility logits optional T ( [B,K] ) temporal keep logits RouterTinyMLP(nn.Module) : configurable input_dim , hidden_dim , num_layers , temporal_head forward contract on x[B,K,D] with strict input validation compatibility method predict_utilities(...) for RouterProtocol usage ( input_dim=1 ) Exported in apex_x/routing/__init__.py : RouterTinyOutput RouterTinyMLP Added tests in tests/test_router_tiny_mlp.py : shape checks for U/S and optional T deterministic outputs for fixed seed/model initialization backward/gradient-flow checks through router outputs predict_utilities(...) behavior and validation paths runtime protocol conformance ( isinstance(..., RouterProtocol) ) Minor typing fix to keep strict typecheck green: apex_x/utils/visualization.py now uses an explicit typed cast in _as_hwc_uint8(...) Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Lightweight spline activation + RouterKANLike implemented: Added apex_x/routing/kan_like.py : LightweightSplineActivation : compact per-feature piecewise-linear spline on fixed knot grid identity initialization for stable startup explicit nan/inf sanitization + bounded input clamp RouterKANLike : small-parameter KAN-like router ( LayerNorm -> Linear -> Spline -> Linear ) outputs U , S , and optional T via RouterKANOutput bounded head logits via configurable logit_clip protocol-compatible predict_utilities(...) and parameter_count() Exported via apex_x/routing/__init__.py : LightweightSplineActivation RouterKANOutput RouterKANLike Added numerical stability tests in tests/test_router_kan_like.py : finite outputs under extreme/nonnumeric input values finite gradients for spline params and inputs router output shape + finite output checks at large input magnitudes deterministic initialization/output checks under fixed seeds small-parameter-count guard RouterProtocol conformance check Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Tensor STE gating implemented for continuous-budget training path: Added apex_x/routing/gating.py : sigmoid_probabilities(U, temperature) : computes p = sigmoid(U) with temperature support clamps/sanitizes extreme and non-finite logits for numerical stability ste_hard_gate(p, mode, threshold) : forward hard gate modes: threshold : g = 1[p >= threshold] bernoulli : g ~ Bernoulli(p) backward straight-through estimator via detach trick ( dg/dp = 1 ) ste_gate_from_utilities(U, ...) -> (p, g) convenience function Exported in apex_x/routing/__init__.py : GateMode sigmoid_probabilities ste_hard_gate ste_gate_from_utilities Added autograd and numerical tests in tests/test_ste_gating.py : non-zero and finite gradient checks w.r.t. U in threshold mode non-zero and finite gradient checks w.r.t. U in Bernoulli mode explicit gradient-form check against sigmoid derivative under linear loss finite probability checks under extreme/non-finite utility inputs Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed BudgetDualController implemented for continuous-budget dual optimization: Added apex_x/routing/dual_budget.py : BudgetDualController (stateful dual variable controller) with: expected cost: E[C] = sum_i(p_i * C_h + (1 - p_i) * C_c) via expected_cost(...) budget term: L_budget = mu * (E[C] - B) via budget_loss(...) projected/clamped dual update: mu <- clamp(mu + mu_lr * (E[C] - B), [mu_min, mu_max]) via update_mu(...) structured debug logging on each update ( event='dual_mu_update' ) including: expected_cost , budget , delta , mu_prev , mu_next , clamped support for both sequence and tensor probabilities in expected-cost path Exported in apex_x/routing/__init__.py : BudgetDualController Added tests in tests/test_budget_dual_controller.py : expected-cost and budget-loss formula checks mu moves in correct direction ( E[C] > B increases, E[C] < B decreases) mu clamp bounds respected ( mu_min / mu_max ) tensor-path budget loss backpropagates with finite non-zero gradients Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Deterministic greedy inference budgeting implemented with explicit Kmax buffer contract: Added apex_x/routing/inference_budget.py : deterministic_greedy_selection(...) : computes scores exactly as score_i = U_i / DeltaC_i deterministic ordering by: primary: score descending secondary: tile id ascending selects until budget exhausted and kmax reached returns GreedySelectionResult with: selected_indices spent_budget ordered_candidates scores kmax_buffer (fixed-length padded buffer) valid_count build_kmax_buffer(...) helper for fixed-size runtime buffers Wired existing public helper to the new deterministic path: apex_x/routing/core.py::greedy_utility_per_cost(...) now delegates to deterministic_greedy_selection(...) and preserves existing return signature Exported in apex_x/routing/__init__.py : GreedySelectionResult build_kmax_buffer deterministic_greedy_selection Added tests in tests/test_inference_budget.py : repeat-run determinism for ordering and selections budget enforcement and kmax cap behavior stable tie handling ( tile_id ascending under equal scores) Kmax buffer padding/truncation semantics Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Two-stage nesting selection implemented ( L0 under B1 , split under B2 ): Extended apex_x/routing/inference_budget.py : TwoStageSelectionResult dataclass carrying: l0 selection result split parent ordering/selection spent split budget generated L1 indices ordered L1 indices + fixed-size L1 Kmax buffer deterministic_two_stage_selection(...) : stage 1: deterministic L0 selection using U / DeltaC under budget_b1 + kmax_l0 stage 2: split parent ranking by S / O_split under budget_b2 expands selected L0 parents to L1 children via quadtree mapping enforces kmax_l1 capacity during split expansion applies deterministic L1 ordering via configured order mode (Hilbert/scan) Exported in apex_x/routing/__init__.py : TwoStageSelectionResult deterministic_two_stage_selection Added tests in tests/test_two_stage_selection.py : L0 selection under B1 split candidate selection under B2 with S/O_split deterministic tie handling on split parents ( tile_id ascending) L1 children generation and ordering behavior determinism across repeated runs kmax_l1 capacity/buffer enforcement Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Temporal hysteresis rollout and toggle analysis implemented: Extended apex_x/routing/core.py hysteresis APIs: hysteresis_update(...) now validates: theta_on > theta_off equal lengths for utilities_t and prev_mask added hysteresis_rollout(...) : applies rule over full time sequence with carried z(t-1) state added count_mask_toggles(...) : counts total 0/1 transitions across time (for anti-flicker evaluation) Exported in apex_x/routing/__init__.py : hysteresis_rollout count_mask_toggles Added tests in tests/test_hysteresis_temporal.py : deadband behavior preserves previous mask state ( z(t-1) ) when utility remains between thresholds synthetic noisy sequence shows reduced toggling vs single-threshold baseline validation/error-path checks for threshold ordering and shape consistency Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Routing diagnostics implemented and surfaced in model/train/infer paths: Added apex_x/routing/diagnostics.py : utility_histogram(...) for per-level utility histogram summaries build_routing_diagnostics(...) producing: selected ratios/counts per level utility histograms per level budget usage ( used , budget , ratio ) dual variable history ( mu_history ) Exported diagnostics APIs through apex_x/routing/__init__.py Integrated diagnostics into apex_x/model/core.py : model outputs now include routing_diagnostics dual controller ( BudgetDualController ) state tracked in mu_history optional dual update path enabled via forward(..., update_dual=True) structured logs now include selected ratio, budget usage ratio, and latest mu Updated apex_x/train/__init__.py and apex_x/infer/__init__.py : train placeholder logs/returns routing diagnostics summary fields infer placeholder returns diagnostics from model outputs Updated CLI integration in apex_x/cli.py : train now performs short warmup forwards with dual updates and reports diagnostics predict now reads infer diagnostics and logs budget usage ratio Added tests in tests/test_routing_diagnostics.py : diagnostics presence in inference outputs diagnostics propagation through train/infer placeholders mu_history progression when dual updates are enabled Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Config-driven feature toggles added for forced-off routing/training paths: Extended apex_x/config/schema.py : ModelConfig toggles: force_dense_routing (router off => dense tile activation) disable_nesting (effective nesting depth forced to 0) disable_ssm (bypass Tile-SSM mixing) TrainConfig toggles: disable_distill disable_pcgradpp Added helper methods: ModelConfig.effective_nesting_depth() ModelConfig.router_enabled() ModelConfig.ssm_enabled() TrainConfig.distill_enabled() TrainConfig.pcgradpp_enabled() Validation updated so disable_nesting=true can force nesting off without requiring manual kmax_l1/l2 and budget_b3 cleanup. Updated apex_x/model/core.py runtime behavior: router-off mode forces dense L0 selection and skips budget controller routing selection no-SSM mode bypasses tile_ssm_scan(...) and uses zero modulation/state no-nesting mode uses effective depth for diagnostics totals ( L1/L2 counts become zero) output now includes feature_toggles summary for debug/smoke assertions Updated apex_x/train/__init__.py + apex_x/cli.py : train placeholder now accepts config and reports effective distill/PCGrad++ enabled flags CLI train passes config through to preserve toggle behavior in logs/output paths Added smoke coverage: tests/test_feature_toggle_smoke.py validates override behavior for disable_nesting executes all 2^5=32 toggle combinations: router off / on no nesting / nesting no SSM / SSM no distill / distill no PCGrad++ / PCGrad++ asserts forward + train placeholder run without crashes and toggle semantics hold Verification status: python -m ruff check . passed python -m pytest -q passed ./.venv/bin/python -m mypy passed Torch tile packer implemented with contiguous output contract: Added apex_x/tiles/torch_ops.py : TilePackTorch.pack(...) : input: F[B,C,H,W] , idx[B,K] , tile_size output: P[B,K,C,t,t] , meta deterministic ordering via shared ordering dispatcher ( order_idx(...) ) strict shape/dtype/bounds validation guarantees contiguous packed tensor via .contiguous() pack_tiles_torch(...) convenience wrapper TorchTileMeta type alias ( dict[str, torch.Tensor] ) Exported through apex_x/tiles/__init__.py : TorchTileMeta TilePackTorch pack_tiles_torch Added tests in tests/test_tile_pack_torch.py : correctness vs NumPy reference pack_tiles(...) contiguous output assertion autograd gradient-flow check from packed output back to source feature map Verification status: python -m ruff check ... passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_tile_pack_torch.py tests/test_tile_ops.py passed Torch tile unpacker implemented with overlap priority modes: Extended apex_x/tiles/torch_ops.py : TileUnpackTorch.unpack(...) : input: base_map[B,C,H,W] , packed_out[B,K,C,t,t] , meta , level_priority , optional priority_map overlap modes: override : incoming tile values replace existing values where priority allows blend : weighted fusion out = (1-alpha)*current + alpha*incoming where priority allows priority contract preserved via per-pixel priority_map updates strict validation for tensor ranks/shapes, bounds, and mode/alpha values unpack_tiles_torch(...) convenience wrapper OverlapMode type alias Exported through apex_x/tiles/__init__.py : OverlapMode TileUnpackTorch unpack_tiles_torch Added tests in tests/test_tile_unpack_torch.py : override-mode parity vs NumPy unpack_tiles(...) overlap priority enforcement and blend-mode numeric behavior autograd gradient-flow checks for blend mode ( base and packed_out ) helper/function parity ( TileUnpackTorch().unpack vs unpack_tiles_torch ) Verification status: python -m ruff check apex_x/tiles/torch_ops.py apex_x/tiles/__init__.py tests/test_tile_unpack_torch.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_tile_pack_torch.py tests/test_tile_unpack_torch.py tests/test_tile_ops.py passed Fusion gate implemented with boundary/uncertainty-conditioned alpha: Added apex_x/model/fusion_gate.py : FusionGate(nn.Module) computes spatial gate: alpha [B,1,H,W] = sigmoid(w_b * boundary_proxy + w_u * uncertainty_proxy + bias) positive monotonic proxy weights enforced via softplus(...) outputs fused features: fused = base + alpha * (heavy - base) validates proxy shape contracts and aligns proxies to feature dtype/device Exported in apex_x/model/__init__.py : FusionGate Added tests in tests/test_fusion_gate.py : alpha shape/range and fusion formula correctness sensitivity checks: increasing boundary/uncertainty proxies increases mean alpha Verification status: python -m ruff check apex_x/model/fusion_gate.py tests/test_fusion_gate.py apex_x/model/__init__.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_fusion_gate.py passed Cheap block implemented ( 1x1 + norm + ReGLU + optional residual ): Added apex_x/model/cheap_block.py : CheapBlock(nn.Module) : path: Conv2d(1x1) -> GroupNorm -> ReGLU optional residual add automatic residual projection ( 1x1 ) when in_channels != out_channels validation for input channels and normalization group divisibility Exported in apex_x/model/__init__.py : CheapBlock Added tests in tests/test_cheap_block.py : shape checks with residual projection path residual identity behavior when main path is zeroed no-residual zero-output behavior when main path is zeroed gradient-flow checks for input and block parameters Verification status: python -m ruff check apex_x/model/cheap_block.py apex_x/model/__init__.py tests/test_cheap_block.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_cheap_block.py tests/test_model.py passed Tile refine block implemented for packed tiles: Added apex_x/model/tile_refine_block.py : TileRefineBlock(nn.Module) operating on packed tensors [B,K,C,t,t] local refine path per tile: depthwise conv ( k x k ) pointwise conv GroupNorm ReGLU activation optional residual (with automatic projection when channels differ) implementation flattens B*K for conv processing and restores [B,K,...] , preserving per-tile independence Exported in apex_x/model/__init__.py : TileRefineBlock Added tests in tests/test_tile_refine_block.py : shape/projection path checks residual identity when main path is zeroed per-tile independence (no cross-tile mixing) gradient-flow checks for inputs and parameters Verification status: python -m ruff check apex_x/model/tile_refine_block.py apex_x/model/__init__.py tests/test_tile_refine_block.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_tile_refine_block.py tests/test_cheap_block.py passed PV backbone implemented with P3/P4/P5 feature outputs: Added apex_x/model/pv_backbone.py : PVBackbone(nn.Module) returning feature dict: P3 (stride 8) P4 (stride 16) P5 (stride 32) uses lightweight staged downsampling + CheapBlock refinement per level validates input shape/channel contract and minimum spatial size Exported in apex_x/model/__init__.py : PVBackbone Added tests in tests/test_pv_backbone.py : parameterized shape checks across multiple input sizes gradient-flow check input validation checks Verification status: python -m ruff check apex_x/model/pv_backbone.py apex_x/model/__init__.py tests/test_pv_backbone.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_pv_backbone.py passed PV coarse heads implemented with explicit uncertainty proxy definition: Added apex_x/model/pv_coarse_heads.py : PVCoarseHeads(nn.Module) producing coarse PV maps from a selected backbone level (default P4 ): objectness_logits objectness = sigmoid(objectness_logits) boundary_proxy = sigmoid(boundary_logits) variance_proxy = softplus(variance_logits) uncertainty_proxy clear uncertainty definition: u_hat = 4 * p * (1 - p) where p = objectness normalized Bernoulli variance ( u_hat=1 at p=0.5 , u_hat=0 at p in {0,1} ) output typed via PVCoarseOutput dataclass Exported in apex_x/model/__init__.py : PVCoarseHeads PVCoarseOutput Added tests in tests/test_pv_coarse_heads.py : parameterized shape/range checks across multiple image sizes (via PVBackbone + P4 ) uncertainty sensitivity checks with controlled objectness logits direct uncertainty formula parity check gradient-flow check through backbone + heads Verification status: python -m ruff check apex_x/model/pv_coarse_heads.py apex_x/model/__init__.py tests/test_pv_coarse_heads.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_pv_coarse_heads.py passed PV module wired with backbone + coarse heads: Added apex_x/model/pv_module.py : PVModule composes: PVBackbone for P3/P4/P5 feature extraction PVCoarseHeads for coarse proxies PVModule.forward() now returns PVModuleOutput containing: features dict ( P3/P4/P5 ) coarse maps from selected level ( coarse_level : P3 / P4 / P5 ) proxy_maps dict for routing-facing signals: objectness uncertainty boundary variance Exported in apex_x/model/__init__.py : PVModule PVModuleOutput Added tests in tests/test_pv_module.py : shape checks across multiple input sizes coarse-level selection checks ( P3 vs P5 ) proxy_maps key/shape checks finite-output assertions Added dedicated CPU smoke test in tests/test_pv_module_smoke_cpu.py : validates CPU forward path and proxy-map outputs are finite with expected shapes Verification status: python -m ruff check apex_x/model/pv_module.py apex_x/model/__init__.py tests/test_pv_module.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_pv_module.py tests/test_pv_module_smoke_cpu.py tests/test_pv_backbone.py tests/test_pv_coarse_heads.py passed Stable state-space-like scan implemented with constrained parameters: Extended apex_x/utils/ssm.py : StableStateSpaceScan(nn.Module) with constrained recurrent parameters: decay constrained to (min_decay, max_decay) via sigmoid mapping positive input/output gains via softplus numerically safe token sanitization/clamp path SSMScanStats for explicit scan complexity accounting: steps recurrent_updates pairwise_updates tile_ssm_scan(...) now clamps alpha to stable bounds Exported in apex_x/utils/__init__.py : StableStateSpaceScan SSMScanStats Added tests in tests/test_stable_ssm_scan.py : no-NaN/finite checks on extreme inputs gradient-flow checks for inputs and scan parameters O(K) behavior checks via linear recurrent-update accounting and zero pairwise updates Verification status: python -m ruff check apex_x/utils/ssm.py apex_x/utils/__init__.py tests/test_stable_ssm_scan.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_stable_ssm_scan.py passed Bidirectional scan and merge gate added on top of stable scan: Extended apex_x/utils/ssm.py : StableBidirectionalStateSpaceScan(nn.Module) : forward-direction stable scan backward-direction stable scan (reverse sequence + reverse outputs back) channel-wise constrained merge gate: gate = sigmoid(merge_gate_logit) in [0,1] merged output: gate * y_fwd + (1 - gate) * y_bwd complexity stats preserved as linear-time recurrent updates (no pairwise updates) Exported in apex_x/utils/__init__.py : StableBidirectionalStateSpaceScan Added tests in tests/test_bidirectional_ssm_scan.py : finite/no-NaN checks with extreme inputs merge formula correctness vs explicit gate-weighted combination gradient-flow checks for inputs and parameters (including merge gate path) O(K) scaling checks from recurrent update counts Verification status: python -m ruff check apex_x/utils/ssm.py apex_x/utils/__init__.py tests/test_bidirectional_ssm_scan.py tests/test_stable_ssm_scan.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_stable_ssm_scan.py tests/test_bidirectional_ssm_scan.py passed FiLM modulation implemented from tokens to packed tiles: Added apex_x/model/film.py : TileFiLM(nn.Module) : computes FiLM parameters from tokens tokens[B,K,Ct] bounded gain path: gamma = tanh(gamma_raw) * gamma_limit shift path: beta applies modulation to packed tiles: out = (1 + gamma) * tiles + beta apply_film(...) functional helper with strict shape validation Updated apex_x/model/core.py : replaced additive-only packed modulation with FiLM-style modulation using SSM mixed tokens: gamma = tanh(mixed) beta = mixed packed_out = (1 + gamma) * packed + beta Exported in apex_x/model/__init__.py : TileFiLM apply_film Added tests in tests/test_film_modulation.py : formula and shape checks gamma range bound checks deterministic forward under fixed inputs gradient-flow checks through tokens, tiles, and module parameters Verification status: python -m ruff check apex_x/model/film.py apex_x/model/core.py apex_x/model/__init__.py tests/test_film_modulation.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_film_modulation.py tests/test_model.py passed FF heavy path implemented end-to-end with aligned detail map output: Added apex_x/model/ff_heavy_path.py : FFHeavyPath(nn.Module) pipeline: TilePackTorch gather on selected FF tile indices tile tokenization via spatial pooling: tokens[B,K,C] stable scan ( forward or bidirectional ) over tokens FiLM modulation ( gamma , beta ) from mixed tokens to packed tiles local packed-tile refine via TileRefineBlock TileUnpackTorch scatter back to dense map shape optional proxy-conditioned fusion gate ( FusionGate ) output contract via FFHeavyPathOutput : heavy_features[B,C,H,W] detail_map[B,C,H,W] aligned to dense features alpha[B,1,H,W] diagnostics tensors ( tokens , mixed_tokens , gamma , beta , state ) includes robust CPU behavior for empty tile selections ( K=0 ) with zero detail contribution. Updated exports in apex_x/model/__init__.py : FFHeavyPath FFHeavyPathOutput Added CPU tests in tests/test_ff_heavy_path.py : shape/alignment checks and finite outputs empty-selection behavior ( K=0 ) -> zero detail_map deterministic repeated forward and gradient-flow checks Verification status: python -m ruff check apex_x/model/ff_heavy_path.py apex_x/model/__init__.py tests/test_ff_heavy_path.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_ff_heavy_path.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q FF module implemented for train/infer routing execution: Added apex_x/model/ff_module.py : FFModule(nn.Module) with two explicit entrypoints: forward_train(...) : STE routing gates via ste_gate_from_utilities(...) expected-cost computation via BudgetDualController.expected_cost(...) budget loss term via BudgetDualController.budget_loss(...) optional dual- mu update with persistent mu_history routed L0 heavy execution through FFHeavyPath forward_infer(...) : deterministic L0 greedy selection under B1 / Kmax_l0 optional L0->L1 two-stage selection under B2 / Kmax_l1 optional nesting execution (L1 heavy pass) when split utilities provided diagnostics integrated in both paths through build_routing_diagnostics(...) : selected counts/ratios utility histograms per-budget usage ( b1/b2/b3/total ) mu_history output dataclasses: FFTrainOutput FFInferOutput Updated exports in apex_x/model/__init__.py : FFModule FFTrainOutput FFInferOutput Added tests in tests/test_ff_module.py : train path: STE + expected-cost + budget-loss outputs diagnostics presence dual- mu history update inference path: deterministic budgeted L0 selection optional nesting with deterministic L1 child selection under B2 diagnostics coverage Verification status: python -m ruff check apex_x/model/ff_module.py apex_x/model/__init__.py tests/test_ff_module.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_ff_module.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q Dual-path FPN implemented to combine PV low-res with FF high-res: Added apex_x/model/fpn.py : DualPathFPN(nn.Module) : inputs: PV features dict P3/P4/P5 FF high-res feature/detail map fusion path: lateral 1x1 projections for PV P3/P4/P5 lateral 1x1 projection for FF branch top-down FPN merge ( P5 -> P4 -> P3 ) explicit FF injection at P3 after spatial alignment smoothing with CheapBlock on P3/P4/P5 output contract via DualPathFPNOutput : fused pyramid dict P3/P4/P5 aligned FF feature map at P3 resolution ( ff_aligned ) Updated exports in apex_x/model/__init__.py : DualPathFPN DualPathFPNOutput Added tests in tests/test_fpn.py : CPU shape and finite-value checks FF-branch sensitivity (changing FF input changes fused P3 ) gradient-flow checks through PV inputs, FF input, and FPN params Verification status: python -m ruff check apex_x/model/fpn.py apex_x/model/__init__.py tests/test_fpn.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_fpn.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q DET head implemented over P3..P7 (cls/box/quality): Added apex_x/model/det_head.py : DetHead(nn.Module) : consumes pyramid features with required P3/P4/P5 supports optional provided P6/P7 ; auto-generates missing levels from P5 / P6 per-level outputs: cls_logits : [B, num_classes, H, W] box_reg : [B, 4, H, W] quality : [B, 1, H, W] uses shared tower structure for cls/box/quality with GroupNorm + SiLU and export-friendly conv outputs output contract via DetHeadOutput : per-level dicts for cls_logits , box_reg , quality normalized features dict for effective P3..P7 levels used by the head Updated exports in apex_x/model/__init__.py : DetHead DetHeadOutput Added tests in tests/test_det_head.py : shape checks across all output levels P3..P7 when only P3..P5 are provided behavior check that explicitly provided P6/P7 are used as-is gradient-flow checks through inputs and DET-head parameters Verification status: python -m ruff check apex_x/model/det_head.py apex_x/model/__init__.py tests/test_det_head.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_det_head.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q SimOTA/OTA cost components implemented for DET matching: Added apex_x/losses/simota.py : classification cost: classification_cost(...) with modes: BCE-based positive-class cost focal-based positive-class cost IoU cost: iou_cost(...) implementing 1 - IoU over pairwise GT/anchor boxes center prior cost: center_prior_cost(...) from GT center to anchor center distance (normalized by GT size) per-GT candidate generation: topk_center_candidates(...) selecting top-k nearest anchor centers per GT candidate_mask_from_indices(...) to build per-GT candidate masks integrated cost assembly: compute_simota_cost(...) combining weighted components and candidate masking SimOTACostOutput dataclass containing component matrices, combined cost, and candidates Updated exports in apex_x/losses/__init__.py : classification_cost , iou_cost , center_prior_cost topk_center_candidates , candidate_mask_from_indices compute_simota_cost , SimOTACostOutput , ClassificationCostType Added tests in tests/test_simota_cost.py : per-GT top-k center candidate selection correctness on synthetic anchors/GT classification-cost ranking behavior (higher positive logit -> lower cost) IoU cost sanity ( 1 - IoU ) combined SimOTA ranking on synthetic setup (reasonable anchor wins; non-candidates penalized) center-prior ranking preference for nearby anchor centers Verification status: python -m ruff check apex_x/losses/simota.py apex_x/losses/__init__.py tests/test_simota_cost.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_simota_cost.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q Dynamic-K SimOTA matching implemented (including conflict resolution): Extended apex_x/losses/simota.py : DynamicKMatchingOutput dataclass with: dynamic_ks matching_matrix foreground_mask matched_gt_indices assigned_cost num_foreground dynamic_k_from_top_ious(...) : computes per-GT dynamic_k from sum of top IoUs (with configurable topk and min_k ) supports optional candidate mask dynamic_k_matching(...) : selects dynamic_k anchors per GT by minimal total cost resolves multi-GT conflicts by keeping the minimal-cost GT assignment per anchor outputs deterministic one-to-one anchor-to-GT assignment for foreground anchors Updated exports in apex_x/losses/__init__.py : DynamicKMatchingOutput dynamic_k_from_top_ious dynamic_k_matching Expanded tests/test_simota_cost.py : verifies dynamic-k computation from top-IoU sums verifies crowded conflict resolution picks minimal-cost GT per anchor verifies candidate-mask constraints are respected in crowded settings Verification status: python -m ruff check apex_x/losses/simota.py apex_x/losses/__init__.py tests/test_simota_cost.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_simota_cost.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q SimOTA assignment integrated into DET loss with target generation: Added apex_x/losses/det_loss.py : build_simota_targets_for_anchors(...) : uses SimOTA cost + dynamic-k matching to select positive anchors builds per-anchor targets: cls_target (one-hot positives, zero background) box_target (assigned GT boxes) quality_target (matched IoU targets) returns SimOTATargets with matching diagnostics det_loss_with_simota(...) : computes DET loss from assignment targets: classification loss (BCE or focal) box loss ( 1 - IoU ) on positives quality BCE loss returns DetLossOutput with component losses and targets stability features: canonicalized box ordering for robust IoU math optional assignment on detached predictions small-object positive weighting with clipped inverse-sqrt area scaling dynamic-k conflict-resolved assignment for crowded scenes Updated exports in apex_x/losses/__init__.py : SimOTATargets DetLossOutput build_simota_targets_for_anchors det_loss_with_simota ClsLossType Added tests in tests/test_det_loss_simota.py : target generation and per-anchor labeling correctness finite/stable loss on tiny-object crowded synthetic setup toy optimization loop verifying DET loss decreases over training steps Verification status: python -m ruff check apex_x/losses/det_loss.py apex_x/losses/__init__.py tests/test_det_loss_simota.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_det_loss_simota.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q DET losses hardened for numerical stability and quality-focal support: Updated apex_x/losses/det_loss.py : added logit sanitization/clipping via _sanitize_logits(...) added QualityLossType with bce and qfl modes det_loss_with_simota(...) now supports: quality_loss_type=\\\"qfl\\\" quality_focal_beta logit_clip focal/BCE classification and QFL/BCE quality paths now run on sanitized logits for stable behavior under extreme values Updated exports in apex_x/losses/__init__.py : QualityLossType Expanded tests/test_det_loss_simota.py : toy decreasing-loss case now also exercises quality-focal path added extreme-logit + tiny-box finite test with backward gradient finiteness checks Verification status: python -m ruff check apex_x/losses/det_loss.py apex_x/losses/__init__.py tests/test_det_loss_simota.py passed ./.venv/bin/python -m mypy passed python -m pytest -q tests/test_det_loss_simota.py passed full project checks passed: python -m ruff check . ./.venv/bin/python -m mypy python -m pytest -q Deterministic DET decode + NMS implemented: Added apex_x/infer/detection.py : decode_anchor_free_candidates(...) : decodes anchor-free DetHeadOutput maps into per-image candidate tensors supports configurable level strides and image clipping applies stable candidate ranking with deterministic tie behavior deterministic_nms(...) : class-wise NMS with deterministic ordering tie-breaking policy: score desc, then candidate index asc batched_deterministic_nms(...) : fixed-shape batched outputs with padding and valid_counts decode_and_nms(...) : end-to-end decode + NMS convenience entrypoint output dataclasses: DetectionCandidates DetectionBatch Updated exports in apex_x/infer/__init__.py : DetectionCandidates DetectionBatch decode_anchor_free_candidates deterministic_nms batched_deterministic_nms decode_and_nms Added tests in tests/test_det_decode_nms.py : end-to-end decode + NMS determinism and class-wise suppression behavior deterministic tie handling in NMS (equal scores -> lower index first) cross-class overlap handling (no cross-class suppression) Verification status: python -m ruff check apex_x/infer/detection.py apex_x/infer/__init__.py tests/test_det_decode_nms.py passed python -m mypy --cache-dir=/dev/null apex_x/infer/detection.py apex_x/infer/__init__.py passed python -m pytest -q tests/test_det_decode_nms.py tests/test_import_smoke.py passed python -m pytest -q passed Prototype-based instance segmentation forward path and mask assembly implemented: Added apex_x/model/inst_seg_head.py : PrototypeInstanceSegHead(nn.Module) : prototype generator from feature maps ( prototypes: [B,M,Hp,Wp] ) per-instance coefficient prediction ( coefficients: [B,N,M] ) from: ROI mean-pooled feature regions derived from boxes_xyxy , or explicit instance_embeddings mask assembly by linear prototype combination: mask_logits_lowres = einsum(coefficients, prototypes) -> [B,N,Hp,Wp] output resizing to requested mask resolution optional box cropping with configurable fill value for stable masked logits per-instance mask_scores from masked probability averages helper functions: assemble_mask_logits_from_prototypes(...) rasterize_box_masks(...) output dataclass: InstanceSegOutput Updated exports in apex_x/model/__init__.py : PrototypeInstanceSegHead InstanceSegOutput assemble_mask_logits_from_prototypes rasterize_box_masks Added tests in tests/test_inst_seg_head.py : prototype-mask assembly correctness vs expected weighted combinations forward-path shape/range checks and finite outputs deterministic repeatability for same weights/inputs gradient-flow checks (features + instance embeddings + parameters) crop-to-box fill behavior outside ROI regions Verification status: python -m ruff check apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_head.py passed python -m mypy --cache-dir=/dev/null apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_head.py passed python -m pytest -q tests/test_inst_seg_head.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q passed Segmentation losses implemented (BCE + Dice + boundary DT surrogate): Added apex_x/losses/seg_loss.py : mask_bce_loss(...) : BCEWithLogits per mask with optional per-instance weighting [B,N] mask_dice_loss(...) : soft Dice loss over [B,N,H,W] masks with optional per-instance weighting soft_boundary_distance_transform(...) : differentiable approximation of boundary distance transform using iterative soft-min neighborhood propagation boundary_distance_transform_surrogate_loss(...) : boundary mismatch weighted by target soft distance transform instance_segmentation_losses(...) : combined BCE + Dice + boundary loss returning SegLossOutput Updated exports in apex_x/losses/__init__.py : SegLossOutput mask_bce_loss mask_dice_loss soft_boundary_distance_transform boundary_distance_transform_surrogate_loss instance_segmentation_losses Added tests in tests/test_seg_loss.py : BCE/Dice near-zero behavior for near-perfect logits soft boundary-DT monotonicity sanity check boundary surrogate sensitivity to shifted boundaries toy optimization loop showing combined loss decreases with finite gradients instance-weight support path Verification status: python -m ruff check apex_x/losses/seg_loss.py apex_x/losses/__init__.py tests/test_seg_loss.py passed python -m mypy --cache-dir=/dev/null apex_x/losses/seg_loss.py apex_x/losses/__init__.py tests/test_seg_loss.py passed python -m pytest -q tests/test_seg_loss.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q FF high-resolution tile-gated refinement hook implemented for instance masks: Updated apex_x/model/inst_seg_head.py : added FFTileRefinementHook(nn.Module) : inputs: mask_logits [B,N,H,W] , ff_highres_features [B,C,H,W] , active_tile_indices [B,K] packs only active tiles, applies FF-conditioned additive refinement on packed tiles, and unpacks back guarantees inactive tiles remain unchanged via tile-scatter semantics integrated optional hook into PrototypeInstanceSegHead : new init toggles: enable_ff_refine ff_refine_tile_size ff_refine_order_mode ff_refine_overlap_mode ff_refine_blend_alpha ff_refine_strength_init new forward args: ff_highres_features active_tile_indices refinement is applied only when enabled and both FF features + active indices are provided Updated exports in apex_x/model/__init__.py : FFTileRefinementHook Added tests in tests/test_inst_seg_refinement_hook.py : hook updates only selected tiles and leaves non-selected tiles exactly unchanged empty active-tile indices produce no-op behavior head-level integration verifies refinement delta is confined to active tile regions Verification status: python -m ruff check apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_refinement_hook.py passed python -m mypy --cache-dir=/dev/null apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_refinement_hook.py passed python -m pytest -q tests/test_inst_seg_refinement_hook.py tests/test_inst_seg_head.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Panoptic output generation implemented (semantic + instance fusion): Added apex_x/infer/panoptic.py : generate_panoptic_output(...) : combines semantic logits with instance masks into deterministic panoptic maps deterministic overlap fusion: thing instances fused first, sorted by (score desc, instance_index asc) overlap resolution keeps higher-ranked instance pixels deterministic thing/stuff rules: only classes in thing_class_ids are accepted as thing instances remaining unassigned pixels are filled by semantic stuff classes in ascending class-id order segment id 0 reserved as void/unassigned supports: mask threshold, score threshold minimum thing/stuff area filters optional masks_are_logits for instance-mask logits input dataclasses: PanopticSegmentInfo ( id , category_id , isthing , area , optional score/index) PanopticOutput ( panoptic_map , segments_info , semantic_labels ) Updated exports in apex_x/infer/__init__.py : PanopticSegmentInfo PanopticOutput generate_panoptic_output Added tests in tests/test_panoptic_generation.py : deterministic overlap resolution on synthetic overlapping thing instances thing/stuff rule verification (non-thing instances ignored, stuff preserved) output contract checks on synthetic batched scenes: map shape/type unique segment IDs per-segment area parity with panoptic map pixels Verification status: python -m ruff check apex_x/infer/panoptic.py apex_x/infer/__init__.py tests/test_panoptic_generation.py passed python -m mypy --cache-dir=/dev/null apex_x/infer/panoptic.py apex_x/infer/__init__.py tests/test_panoptic_generation.py passed python -m pytest -q tests/test_panoptic_generation.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Panoptic PQ evaluation wrapper implemented (official API + fallback): Added apex_x/infer/pq_eval.py : evaluate_panoptic_quality(...) : uses official panopticapi evaluator when available and paths are provided otherwise falls back to an in-memory deterministic minimal PQ implementation deterministic fallback behavior: per-category matching with IoU threshold one-to-one matches with deterministic tie handling computes per-class (PQ, SQ, RQ) and aggregate all/things/stuff metrics dataclasses: PQClassMetrics PQMetrics OfficialPQPaths Updated exports in apex_x/infer/__init__.py : PQClassMetrics PQMetrics OfficialPQPaths evaluate_panoptic_quality CLI integration hook added: updated apex_x/cli.py eval command with: --panoptic-pq flag runs panoptic PQ hook and prints panoptic_pq=<value> and source ( official / fallback ) Added fixtures: tests/fixtures/pq_case_perfect.json tests/fixtures/pq_case_partial.json Added tests: tests/test_pq_eval.py fixture-driven perfect and partial overlap PQ checks verifies official-path attempt cleanly falls back when official API is unavailable/invalid tests/test_cli.py new parse test for eval --panoptic-pq Verification status: python -m ruff check apex_x/infer/pq_eval.py apex_x/infer/__init__.py apex_x/cli.py tests/test_pq_eval.py tests/test_cli.py passed python -m mypy --cache-dir=/dev/null apex_x/infer/pq_eval.py apex_x/infer/__init__.py apex_x/cli.py tests/test_pq_eval.py tests/test_cli.py passed python -m pytest -q tests/test_pq_eval.py tests/test_cli.py passed full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Tracking embedding head and association interfaces implemented: Added apex_x/model/track_head.py : TrackEmbeddingHead : accepts feature tensor or feature dict projects features, ROI-pools detections, and emits L2-normalized embeddings TrackEmbeddingOutput dataclass with: embeddings raw_embeddings pooled_features Added apex_x/infer/tracking.py : TrackState dataclass (validated tracker state contract) AssociationResult dataclass AssociationProtocol interface deterministic GreedyCosineAssociator implementation compatibility aliases: TrackAssociatorProtocol TrackAssociator Updated exports: apex_x/model/__init__.py exports TrackEmbeddingHead , TrackEmbeddingOutput apex_x/infer/__init__.py exports tracking dataclasses/protocols/associator apex_x/__init__.py exports TrackState and AssociationProtocol Added tests: tests/test_track_head.py : output shape checks embedding unit-norm checks deterministic forward checks gradient flow checks tests/test_tracking_interfaces.py : TrackState.empty(...) contract protocol conformance checks deterministic greedy matching/new-track behavior aging/removal behavior with no detections Verification status: targeted checks passed: python -m ruff check apex_x/model/track_head.py apex_x/infer/tracking.py tests/test_track_head.py tests/test_tracking_interfaces.py python -m mypy --cache-dir=/dev/null apex_x/model/track_head.py apex_x/infer/tracking.py tests/test_track_head.py tests/test_tracking_interfaces.py python -m pytest -q tests/test_track_head.py tests/test_tracking_interfaces.py project checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q note: python -m black --check . currently reports unrelated pre-existing formatting diffs in legacy files not touched in this change: apex_x/routing/interfaces.py apex_x/train/__init__.py tests/test_pq_eval.py apex_x/config/schema.py apex_x/infer/pq_eval.py apex_x/losses/det_loss.py apex_x/model/inst_seg_head.py Hungarian association with lifecycle and memory-bank updates implemented: Updated apex_x/infer/tracking.py : added Hungarian solver utility: hungarian_assignment(...) added HungarianAssociator implementing: IoU + embedding-distance gating for candidate matches global cost minimization via Hungarian assignment track lifecycle ( init / update / terminate ) with max_age embedding memory-bank update per track with fixed bank size bank-size normalization for legacy states to keep runtime stable extended TrackState with optional lifecycle/memory fields: hit_counts memory_bank memory_counts extended AssociationResult with lifecycle debug outputs: terminated_track_indices terminated_track_ids created_track_ids kept backward compatibility: GreedyCosineAssociator now delegates to Hungarian with cosine-only cost protocol aliases preserved Updated apex_x/infer/__init__.py exports: HungarianAssociator hungarian_assignment Added tests in tests/test_tracking_hungarian.py : Hungarian global-optimum assignment on synthetic cost matrix IoU + embedding-distance gating behavior lifecycle termination after max_age memory-bank update/cap behavior multi-frame moving-object ID consistency across reordered detections Verification status: targeted checks passed: python -m ruff check apex_x/infer/tracking.py apex_x/infer/__init__.py tests/test_tracking_interfaces.py tests/test_tracking_hungarian.py python -m mypy --cache-dir=/dev/null apex_x/infer/tracking.py apex_x/infer/__init__.py tests/test_tracking_interfaces.py tests/test_tracking_hungarian.py python -m pytest -q tests/test_tracking_interfaces.py tests/test_tracking_hungarian.py tests/test_track_head.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q PCGrad++ shared-trunk conflict projection implemented: Added apex_x/train/pcgrad.py : canonical grouped loss ordering: det_cls , det_box , seg_mask , seg_boundary , then sorted extras LossGroup dataclass and group_loss_terms(...) apply_pcgradpp(...) : computes per-group gradients applies projection only to shared trunk parameter gradients when cos < 0 leaves task-head parameter gradients as standard total-loss gradients PCGradDiagnostics + diagnostics_to_dict(...) for logging/debug Updated exports in apex_x/train/__init__.py : DEFAULT_LOSS_GROUP_ORDER LossGroup PCGradDiagnostics group_loss_terms apply_pcgradpp diagnostics_to_dict Added tests in tests/test_pcgradpp.py : deterministic grouped ordering for canonical loss groups + extra loss terms tiny-network synthetic conflicting gradients test: confirms projection resolves shared-trunk conflict confirms head gradients match standard total-loss gradients (no projection on heads) Verification status: targeted checks passed: python -m ruff check apex_x/train/pcgrad.py apex_x/train/__init__.py tests/test_pcgradpp.py python -m mypy --cache-dir=/dev/null apex_x/train/pcgrad.py apex_x/train/__init__.py python -m pytest -q tests/test_pcgradpp.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Distillation losses implemented (logits KL + feature L2 + boundary distill): Added apex_x/losses/distill.py : logits_kl_distill(...) : KL divergence distillation on logits with temperature scaling ( T^2 factor) feature_l2_distill(...) : layer-selective feature L2 distillation with optional per-layer weights supports optional feature normalization before L2 boundary_distill_loss(...) : boundary-focused distillation using Sobel-based soft boundary maps teacher boundary distance-transform weighting distillation_losses(...) : combined wrapper returning DistillationLossOutput Updated exports in apex_x/losses/__init__.py : DistillationLossOutput logits_kl_distill feature_l2_distill boundary_distill_loss distillation_losses Added tests in tests/test_distill_loss.py : KL distill near-zero when student/teacher logits match feature L2 selected-layer behavior with layer weights boundary distill penalizes shifted boundaries more than aligned boundaries combined distillation loss decreases in toy optimization Verification status: targeted checks passed: python -m ruff check apex_x/losses/distill.py apex_x/losses/__init__.py tests/test_distill_loss.py python -m mypy --cache-dir=/dev/null apex_x/losses/distill.py apex_x/losses/__init__.py python -m pytest -q tests/test_distill_loss.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Oracle \u0394Loss targets and router utility supervision implemented: Added apex_x/routing/oracle_distill.py : compute_oracle_delta_targets(...) : computes sampled-tile oracle targets: \u0394_i = L_distill(cheap, teacher) - L_distill(heavy, teacher) supports sampled indices [S] or batched [B,S] returns detached (stop-grad) oracle targets optional clamping for outlier robustness utility_regression_loss(...) : regression loss ( l1 / mse / smooth_l1 ) between router utility logits and detached \u0394 targets utility_ranking_loss(...) : pairwise hinge ranking loss over sampled tiles to preserve oracle ordering utility_oracle_loss(...) : combined regression + ranking objective with diagnostics ( num_pairs ) dataclasses: OracleDeltaTargets UtilityOracleLossOutput Updated routing exports in apex_x/routing/__init__.py : RegressionLossType OracleDeltaTargets UtilityOracleLossOutput compute_oracle_delta_targets utility_regression_loss utility_ranking_loss utility_oracle_loss Added tests in tests/test_oracle_distill.py : sign sanity for \u0394 targets (positive when heavy distill loss is lower than cheap) stop-grad behavior (no gradients into cheap/heavy distill losses via utility supervision) ranking sign sanity (correct utility order yields lower ranking loss) sampled-index regression correctness (only sampled tiles influence loss) Verification status: targeted checks passed: python -m ruff check apex_x/routing/oracle_distill.py apex_x/routing/__init__.py tests/test_oracle_distill.py python -m mypy --cache-dir=/dev/null apex_x/routing/oracle_distill.py apex_x/routing/__init__.py python -m pytest -q tests/test_oracle_distill.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q TeacherModel implemented for full-compute distillation outputs with optional EMA: Added apex_x/model/teacher.py : TeacherModel : dense/full-compute teacher forward path (PV -> FPN -> DET) without sparse routing standardized distillation output bundle: flattened logits ( logits ) per-level logits ( logits_by_level ) selected feature layers ( features ) boundary proxy map aligned to input size ( boundaries ) optional EMA shadow modules: configurable ema_decay update_ema(...) for parameter/buffer updates runtime switch to use online or EMA weights in forward(...) TeacherDistillOutput dataclass flatten_logits_for_distill(...) helper with deterministic level order Updated exports in apex_x/model/__init__.py : TeacherModel TeacherDistillOutput flatten_logits_for_distill Added tests in tests/test_teacher_model.py : full-compute standardized output contract checks deterministic logits-flatten ordering checks EMA behavior checks: initial online/EMA parity EMA lag + update movement toward online model frozen EMA parameter requirements Verification status: targeted checks passed: python -m ruff check apex_x/model/teacher.py apex_x/model/__init__.py tests/test_teacher_model.py python -m mypy --cache-dir=/dev/null apex_x/model/teacher.py apex_x/model/__init__.py python -m pytest -q tests/test_teacher_model.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Staged trainer pipeline implemented and wired into CLI train flow: Added apex_x/train/trainer.py : ApexXTrainer with required stages: stage 0: baseline warmup stage 1: teacher training (full compute) stage 2: oracle bootstrapping stage 3: continuous budgeting with dual mu stage 4: deterministic inference emulation stage/result dataclasses: StageResult StagedTrainResult Updated exports in apex_x/train/__init__.py : ApexXTrainer StageResult StagedTrainResult Updated CLI train command in apex_x/cli.py : now runs staged trainer instead of placeholder-only loop new option: --steps-per-stage output includes stage_count=5 Added staged-train CPU smoke script: examples/train_stages_smoke.py Added validation coverage: tests/test_trainer_stages.py (stage completeness + seed repeatability) tests/test_train_stages_smoke.py (subprocess smoke run) updated tests/test_cli.py to assert staged output includes stage_count=5 Updated docs: README.md now includes staged trainer quickstart/dev commands Verification status: targeted checks passed: python -m ruff check apex_x/train/trainer.py apex_x/cli.py tests/test_trainer_stages.py tests/test_train_stages_smoke.py tests/test_cli.py examples/train_stages_smoke.py python -m mypy --cache-dir=/dev/null apex_x/train/trainer.py apex_x/cli.py python -m pytest -q tests/test_trainer_stages.py tests/test_train_stages_smoke.py tests/test_cli.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Exact COCO compatibility layer implemented with strict schema checks and mask parsing: Added apex_x/data/coco.py with a strict COCO loader: load_coco_dataset(path, strict=True, use_cache=True) strict top-level/record key validation ( images , annotations , categories ) type/range checks for ids, bbox, area, iscrowd, segmentation payloads referential integrity checks for annotation.image_id and annotation.category_id Added complete parsing contracts: bbox parsing into CocoBBox polygon segmentation parsing into CocoSegmentation(kind=\\\"polygon\\\") RLE parsing for uncompressed list counts and compressed string counts into CocoSegmentation(kind=\\\"rle\\\") deterministic mask decode path via segmentation_to_mask(...) Added category mapping + caching: CocoDataset.category_mapping() caches contiguous category mapping: original_to_contiguous contiguous_to_original category name lookup maps loader cache for parsed datasets with mtime/size cache key clear_coco_dataset_cache() helper Updated exports in apex_x/data/__init__.py for COCO dataclasses/helpers. Added fixture-based tests: tests/test_coco_compat.py fixtures: tests/fixtures/coco_valid_mixed.json tests/fixtures/coco_invalid_missing_top_keys.json tests/fixtures/coco_invalid_bad_rle.json Verification status: targeted checks passed: python -m ruff check apex_x/data/coco.py apex_x/data/__init__.py tests/test_coco_compat.py python -m mypy --cache-dir=/dev/null apex_x/data/coco.py apex_x/data/__init__.py tests/test_coco_compat.py python -m pytest -q tests/test_coco_compat.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Data transforms pipeline and Mosaic-v2 heuristic implemented: Added apex_x/data/transforms.py : shared sample contract: TransformSample (image + boxes_xyxy + class_ids + optional masks) pipeline + base transforms: TransformPipeline RandomHorizontalFlip ClipBoxesAndMasks sanitize_sample(...) for clipping/filtering invalid boxes/masks Mosaic-v2: MosaicV2(...) 4-image composition with configurable split jitter heuristic crop-origin policy to protect important instances (by area threshold fallback-to-largest instance) visibility-aware bbox filtering and mask-aware validity checks Updated exports in apex_x/data/__init__.py : TransformSample , Transform , TransformPipeline RandomHorizontalFlip , ClipBoxesAndMasks , MosaicV2 , sanitize_sample Added tests in tests/test_data_transforms.py : transform-pipeline bbox/mask validity checks mosaic output validity for bbox/mask contracts heuristic regression test showing protected mosaic keeps significantly more important-instance area than unprotected crop selection sanitize clipping behavior on out-of-bounds boxes Verification status: targeted checks passed: python -m ruff check apex_x/data/transforms.py apex_x/data/__init__.py tests/test_data_transforms.py python -m mypy --cache-dir=/dev/null apex_x/data/transforms.py apex_x/data/__init__.py tests/test_data_transforms.py python -m pytest -q tests/test_data_transforms.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Eval pipeline implemented for DET/INST-SEG/SEM-SEG/PANO with report emission: Added apex_x/infer/eval_metrics.py : metric computation for tiny-fixture evaluation: COCO-style mAP (DET) over IoU thresholds 0.50:0.05:0.95 COCO-style mask mAP (INST-SEG) over IoU thresholds 0.50:0.05:0.95 mIoU (SEM-SEG) PQ (PANO) via existing evaluate_panoptic_quality(...) fixture evaluators: evaluate_fixture_payload(...) evaluate_fixture_file(...) built-in fallback payload tiny_eval_fixture_payload() report writers: write_eval_reports(...) emitting JSON + Markdown structured summary dataclass EvalSummary Updated exports in apex_x/infer/__init__.py : EvalSummary , evaluate_fixture_file , evaluate_fixture_payload , tiny_eval_fixture_payload , write_eval_reports Updated CLI eval command in apex_x/cli.py : supports: --fixture (optional fixture JSON; defaults to built-in tiny payload) --report-json --report-md always emits metrics in stdout: det_map , mask_map , miou , panoptic_pq writes JSON and markdown report files per invocation keeps --panoptic-pq flag as compatibility no-op Added tiny fixture + tests: fixture: tests/fixtures/eval_tiny_fixture.json tests/test_eval_metrics.py : metric values on perfect tiny fixture JSON/Markdown report emission built-in tiny payload validation tests/test_cli.py : eval command smoke with fixture + output report paths Verification status: targeted checks passed: python -m ruff check apex_x/infer/eval_metrics.py apex_x/infer/__init__.py apex_x/cli.py tests/test_eval_metrics.py tests/test_cli.py python -m mypy --cache-dir=/dev/null apex_x/infer/eval_metrics.py apex_x/infer/__init__.py apex_x/cli.py tests/test_eval_metrics.py tests/test_cli.py python -m pytest -q tests/test_eval_metrics.py tests/test_cli.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Ablation grid runner implemented in CLI with toggle sweeps and reports: Added apex_x/train/ablation.py : toggle grid definitions for: router , budgeting , nesting , ssm , distill , pcgrad , qat , panoptic , tracking deterministic grid builder: build_ablation_grid(...) with per-toggle modes ( on/off/both ) and max-cap ablation execution: fixed-seed runs over grid combinations trainer invocation with enable_budgeting switch metrics aggregation (DET mAP, mask mAP, semantic mIoU, PQ, tracking consistency) routing stat aggregation (selected ratios, budget usage ratio, mu_last ) report writers: CSV aggregate report markdown summary report Updated ApexXTrainer.run(...) in apex_x/train/trainer.py : added enable_budgeting flag for explicit budgeting on/off ablations Updated exports in apex_x/train/__init__.py : ablation dataclasses/functions ( AblationToggleSet , grid runner, report writer) Updated CLI ablate command in apex_x/cli.py : added per-toggle mode flags ( --router/--budgeting/.../--tracking ) added fixed seed support via repeated --seed added --steps-per-stage , --max-experiments added report outputs: --output-csv --output-md command now runs grid + writes CSV/MD reports Added tests: tests/test_ablation.py : grid construction behavior smoke run + CSV/MD output assertions updated tests/test_cli.py : ablate command smoke with output artifact checks Verification status: targeted checks passed: python -m ruff check apex_x/train/ablation.py apex_x/train/__init__.py apex_x/train/trainer.py apex_x/cli.py tests/test_ablation.py tests/test_cli.py python -m mypy --cache-dir=/dev/null apex_x/train/ablation.py apex_x/train/__init__.py apex_x/train/trainer.py apex_x/cli.py tests/test_ablation.py tests/test_cli.py python -m pytest -q tests/test_ablation.py tests/test_cli.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q INT8 QAT + PTQ fallback path implemented with FP16 router/gating policy: Added apex_x/train/qat.py : activation observer + activation fake quant: ActivationObserver ActivationFakeQuant per-channel INT8 weight fake quant: WeightPerChannelFakeQuant wrapped train-time fake quant modules: FakeQuantConv2d FakeQuantLinear QAT/PTQ entrypoints: prepare_int8_qat(...) prepare_int8_ptq(...) calibrate_ptq(...) explicit wrapper traversal/state controls: iter_qat_wrappers(...) set_qat_state(...) Updated apex_x/train/trainer.py : quantization preparation step added before staged training: uses QAT when train.qat_enable && train.qat_int8 uses PTQ calibration fallback when runtime.precision_profile=edge and QAT is off added deterministic calibration batch builder for PTQ fallback added quantization diagnostics to train_summary[\"quantization\"] : mode , wrapped_modules , calibration_batches , router_gating_fp16 stage-3 routing gate path now keeps FP16 utility gating math and uses FP32 expected-cost accumulation Updated apex_x/train/__init__.py : exported QAT module types/functions for public train API surface Added tests in tests/test_qat.py : QAT wrapper conversion with router/gating skip policy PTQ calibration state transitions (observer off + fake quant on after calibration) trainer-level QAT/PTQ toggle smoke with finite loss/output checks Added documentation: docs/QAT.md with INT8 policy, module behavior, trainer integration, and validation scope linked in docs/index.md and mkdocs.yml Verification status: targeted checks passed: python -m ruff check apex_x/train/qat.py apex_x/train/trainer.py apex_x/train/__init__.py tests/test_qat.py python -m mypy --cache-dir=/dev/null apex_x/train/qat.py apex_x/train/trainer.py apex_x/train/__init__.py python -m pytest -q tests/test_qat.py tests/test_trainer_stages.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q FP8-ready precision policy implemented with safe FP16 fallback: Added apex_x/runtime/precision.py : precision policy dataclass: PrecisionPolicy runtime detection + resolution: resolve_precision_policy(...) conservative CUDA FP8 support gate ( sm90+ + torch FP8 dtype presence) dtype helpers: dtype_name(...) execution context helper: heavy_ops_autocast_context(...) FP16 autocast path on CPU/CUDA FP8-ready no-op context pending specialized kernels/plugins Updated apex_x/runtime/__init__.py exports: PrecisionPolicy , resolve_precision_policy , dtype_name , heavy_ops_autocast_context Updated apex_x/train/trainer.py : resolves precision policy at trainer init applies heavy-op autocast context during stage-1 teacher forward adds stage metrics: heavy_ops_dtype , fp8_enabled adds precision diagnostics in train_summary[\"precision\"] : profile , device , heavy_ops_dtype fp8_requested , fp8_enabled , fallback_reason router_dtype , kan_dtype Updated apex_x/train/qat.py : expanded INT8 wrapper skip policy to preserve FP16 for KAN-like modules: default skip tokens now include \"kan\" in addition to router/gating names Added tests in tests/test_precision_policy.py : CPU fallback smoke ( balanced -> FP16 fallback with explicit reason) mocked supported CUDA path enabling FP8 for heavy ops trainer summary smoke verifying fallback diagnostics Added docs: docs/FP8.md documenting FP8 request rules, support detection, fallback contract, and smoke coverage linked from docs/index.md and mkdocs.yml Verification status: targeted checks passed: python -m ruff check apex_x/runtime/precision.py apex_x/runtime/__init__.py apex_x/train/trainer.py apex_x/train/qat.py tests/test_precision_policy.py python -m mypy --cache-dir=/dev/null apex_x/runtime/precision.py apex_x/runtime/__init__.py apex_x/train/trainer.py python -m pytest -q tests/test_precision_policy.py tests/test_trainer_stages.py tests/test_qat.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q Triton fused gather+gate+scatter path scaffolded with clean fallback to reference: Environment check result for this workspace: torch.cuda.is_available() == False Triton package not installed therefore Triton kernel implementation path is unavailable in this run Added apex_x/runtime/triton_fused.py : availability model: TritonAvailability get_triton_availability() fused-result contract: FusedTileScatterResult reference fused pipeline: gather_gate_scatter_reference(...) implements: gather selected heavy/base/proxy tiles per-pixel fusion gate application scatter with overlap priority semantics via TileUnpackTorch dispatch API: gather_gate_scatter(...) attempts Triton path when requested and available cleanly falls back to reference path when unavailable or stubbed explicit Triton kernel stub: _triton_fused_kernel_stub(...) raises NotImplementedError (by design in no-Triton env) Updated apex_x/runtime/__init__.py exports: get_triton_availability gather_gate_scatter_reference gather_gate_scatter availability/result/backend dataclasses/types Added microbenchmark script: scripts/triton_fused_bench.py compares reference path vs dispatched fused path reports backend, fallback reason, and speed ratio works on CPU fallback path Added tests: tests/test_triton_fused.py : reference fused path parity vs explicit reference composition dispatch fallback behavior in no-Triton/no-CUDA case forced Triton/no-fallback path raises stub error tests/test_triton_bench.py : CPU smoke for benchmark utility Added runtime docs: docs/runtime/TRITON.md describing dispatch contracts, fallback behavior, and benchmark usage linked in docs/index.md and mkdocs.yml Verification status: targeted checks passed: python -m ruff check apex_x/runtime/triton_fused.py apex_x/runtime/__init__.py tests/test_triton_fused.py tests/test_triton_bench.py scripts/triton_fused_bench.py python -m mypy --cache-dir=/dev/null apex_x/runtime/triton_fused.py apex_x/runtime/__init__.py python -m pytest -q tests/test_triton_fused.py tests/test_triton_bench.py tests/test_tile_pack_torch.py tests/test_tile_unpack_torch.py full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q CPU performance regression suite implemented with baseline comparison gates: Added reusable perf suite module: apex_x/bench/perf.py fixed-size infer benchmark ( ApexXModel.forward on [1,3,128,128] ) microbenchmarks: TilePackTorch TileUnpackTorch FusionGate report + compare helpers: run_cpu_perf_suite(...) compare_against_baseline(...) JSON read/write helpers Updated apex_x/bench/__init__.py exports: perf suite and compare utilities exposed Replaced scripts/perf_regression.py : runs suite and writes current JSON report optional baseline-template emit compare mode with pass/fail exit status for CI gating artifacts: current report JSON comparison summary JSON Added committed CPU baseline: scripts/perf_baseline_cpu.json per-metric tolerances via: max_regression_ratio max_regression_abs_ms Added tests: tests/test_perf_regression.py suite smoke coverage baseline compare pass/fail behavior Added documentation: docs/PERF.md linked from docs/index.md and mkdocs.yml Updated CI workflow: .github/workflows/ci.yml now includes perf-regression job on ubuntu-latest (CPU-only) job executes scripts/perf_regression.py --compare ... job uploads perf artifacts ( perf_current_ci.json , perf_compare_ci.json ) Verification status: targeted checks passed: python -m ruff check apex_x/bench/perf.py apex_x/bench/__init__.py scripts/perf_regression.py tests/test_perf_regression.py python -m mypy --cache-dir=/dev/null apex_x/bench/perf.py apex_x/bench/__init__.py scripts/perf_regression.py python -m pytest -q tests/test_perf_regression.py python scripts/perf_regression.py --compare --baseline scripts/perf_baseline_cpu.json --output artifacts/perf_current_test.json --summary artifacts/perf_compare_test.json --infer-iters 15 --micro-iters 25 --infer-warmup 3 --micro-warmup 3 full checks passed: python -m ruff check . python -m mypy --cache-dir=/dev/null python -m pytest -q TensorRT + Go runtime scaffolding added (Task A/B): TensorRT C++ scaffold created under runtime/tensorrt/ : CMakeLists.txt with optional feature probes: APEXX_ENABLE_TENSORRT only when NvInfer.h is found APEXX_ENABLE_CUDA only when CUDA compiler is available stub plugin interfaces/sources: TilePack TileSSMScan TileUnpackFusion optional DecodeNMS utility binary: apexx_trt_plugin_info (prints build summary and plugin flags) TensorRT docs added: docs/runtime/TENSORRT.md contract mapping from plugin spec to scaffold classes guarded build instructions: cd runtime/tensorrt && cmake -S . -B build && cmake --build build -j Go microservice scaffold created under runtime/go/ : service entrypoint: runtime/go/cmd/apexx-runtime/main.go endpoints: POST /predict GET /health GET /metrics short-window batching queue with per-request budget profile support ( quality|balanced|edge ) adapters: ONNX Runtime CPU baseline scaffold ( ORTAdapter ) TensorRT CGO scaffold with build tags: //go:build tensorrt && cgo default fallback returns clear unavailable error containerization: runtime/go/Dockerfile runtime/go/docker-compose.yml tests: runtime/go/internal/service/batcher_test.go runtime/go/internal/service/http_test.go CI/docs integration updates: .github/workflows/ci.yml now includes go-runtime job ( go test ./... in runtime/go ) mkdocs.yml + docs/index.md now link docs/runtime/TENSORRT.md runtime/README.md and root README.md updated with runtime scaffold usage Build/run commands verified for scaffolds: Go tests: cd runtime/go && go test ./... Go service: cd runtime/go && go run ./cmd/apexx-runtime -addr :8080 -adapter onnxruntime TensorRT scaffold build commands documented (not executed in this environment due missing cmake ): cd runtime/tensorrt && cmake -S . -B build && cmake --build build -j Remaining work from this milestone: implement real TensorRT plugin classes ( IPluginV2DynamicExt ) + serialization replace ORT stub adapter with true ONNX Runtime session execution implement TensorRT CGO adapter bridge to compiled plugin/runtime binaries add optional gRPC server for the Go service (HTTP baseline is complete) Runtime capability detection module implemented: Added apex_x/runtime/caps.py with unified runtime probe object: RuntimeCaps cuda: CudaCaps triton: TritonCaps tensorrt: TensorRTCaps fp8: FP8Caps exported from apex_x/runtime/__init__.py Detection coverage: CUDA availability + device name + compute capability Triton availability + version TensorRT: Python package/module availability header availability ( NvInfer.h / NvInferRuntime.h ) via: explicit header_search_paths env hints ( TENSORRT_INCLUDE_DIR , TRT_INCLUDE_DIR , TENSORRT_ROOT , TRT_ROOT , CUDA_HOME , CUDA_PATH ) common include directories INT8 availability gate for TRT usage ( BuilderFlag.INT8 + CUDA) FP8 availability gate: torch FP8 dtype support CUDA presence compute capability sm90+ Added tests (CPU-safe, mock-driven): tests/test_caps_runtime.py tests/test_caps_tensorrt_fp8.py Added docs: docs/runtime/CAPS.md linked in docs/index.md and mkdocs.yml Usage instructions: basic: from apex_x.runtime import detect_runtime_caps caps = detect_runtime_caps() caps.to_dict() explicit TRT header path: detect_runtime_caps(header_search_paths=[\"/usr/local/TensorRT/include\"]) Validation status: python -m pytest -q tests/test_caps_runtime.py tests/test_caps_tensorrt_fp8.py python -m ruff check apex_x/runtime/caps.py tests/test_caps_runtime.py tests/test_caps_tensorrt_fp8.py python -m mypy --cache-dir=/dev/null apex_x/runtime/caps.py apex_x/runtime/__init__.py Runtime parity framework implemented: Added apex_x/runtime/parity.py with backend-agnostic parity APIs: ParityCase , run_parity_case(...) , evaluate_parity_outputs(...) tolerance controls: NumericTolerance ToleranceConfig ( default , fp16 , bf16 , int8 ) reporting objects: TensorParityStats ParityReport format_parity_report(...) Determinism contract: run_parity_case(...) calls seed_all(seed, deterministic=...) before input generation Metrics emitted per compared tensor: max_abs_err , mean_abs_err max_rel_err , mean_rel_err mismatch_count , total_count , mismatch_ratio pass/fail against configured tolerance + mismatch-ratio limit Exported from apex_x/runtime/__init__.py for direct runtime use Added tests: tests/test_parity_framework_core.py tests/test_parity_framework_tolerances.py tests are CPU-safe and use small shapes for CI speed Added documentation: docs/runtime/PARITY.md linked in docs/index.md and mkdocs.yml Usage instructions: create a ParityCase with input_factory , reference_fn , and candidate_fn run run_parity_case(case, seed=..., deterministic=True) serialize/report with report.to_dict() or format_parity_report(report) Validation status: python -m ruff check apex_x/runtime/parity.py apex_x/runtime/__init__.py tests/test_parity_framework_core.py tests/test_parity_framework_tolerances.py python -m mypy --cache-dir=/dev/null apex_x/runtime/parity.py apex_x/runtime/__init__.py python -m pytest -q tests/test_parity_framework_core.py tests/test_parity_framework_tolerances.py .venv/bin/mkdocs build --strict Triton TilePack gather kernel implemented with fallback dispatch: Added new kernel module: apex_x/kernels/triton/tilepack.py Added package exports: apex_x/kernels/__init__.py apex_x/kernels/triton/__init__.py Implemented Triton kernel contract: Input: F[B,C,H,W] contiguous NCHW Input indices: idx[B,K] integer ( int32 kernel path; int64 accepted and cast) Output: P[B,K,C,t,t] contiguous Kernel path support: fp16 , bf16 on CUDA no Python tile loops in kernel gather path Added vectorized reference fallback: tilepack_reference(...) uses tensor gather (no per-tile Python loops) Added dispatch behavior: tilepack_dispatch(...) falls back when Triton/CUDA unavailable falls back when requires_grad and inference_only=True reason: Triton path is inference-oriented without custom backward registration Added parity tests: tests/test_triton_tilepack_parity_dispatch.py tests/test_triton_tilepack_parity_gpu.py GPU parity auto-skips when Triton/CUDA unavailable Added benchmark: apex_x/bench/triton_tilepack_bench.py Added docs: docs/runtime/TRITON_TILEPACK.md docs/runtime/TRITON.md updated with TilePack status docs navigation updated in docs/index.md and mkdocs.yml Run commands: tests: python -m pytest -q tests/test_triton_tilepack_parity_dispatch.py tests/test_triton_tilepack_parity_gpu.py bench (module): python -m apex_x.bench.triton_tilepack_bench --iters 50 --warmup 10 --batch 1 --channels 128 --height 128 --width 128 --tile-size 8 --kmax 32 --dtype fp16 lint/type: python -m ruff check apex_x/kernels/triton/tilepack.py apex_x/bench/triton_tilepack_bench.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tilepack.py apex_x/bench/triton_tilepack_bench.py Validation status: python -m ruff check ... passed python -m mypy --cache-dir=/dev/null ... passed python -m pytest -q tests/test_triton_tilepack_parity_dispatch.py tests/test_triton_tilepack_parity_gpu.py passed (GPU tests skipped on CPU-only env) .venv/bin/mkdocs build --strict passed Triton TileUnpack scatter kernel extended to overlap + priority semantics: Added kernel module: apex_x/kernels/triton/tileunpack.py Added Triton kernel dispatch/availability API: get_triton_tileunpack_availability() tileunpack_reference(...) tileunpack_triton(...) tileunpack_dispatch(...) Added exports: apex_x/kernels/triton/__init__.py Implemented semantics: Inputs: F_base[B,C,H,W] , P_out[B,K,C,t,t] , and idx/meta Output: F_merged[B,C,H,W] deterministic overlap overwrite with priorities: per-tile levels[B,K] (higher level wins) or pre-sorted K-order ( assume_priority_sorted=True ) as implicit priority default mode: overlap_mode=\\\"override\\\" (priority overwrite) optional mode: overlap_mode=\\\"blend\\\" (currently reference fallback) Updated tests: tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py includes synthetic overlap fixtures and parity against reference behavior Updated microbenchmark: apex_x/bench/triton_tileunpack_bench.py supports overlap stress via --overlap-shift supports level-aware runs via default levels ( --no-levels to disable) Added docs: docs/runtime/TRITON_TILEUNPACK.md updated docs/runtime/TRITON.md updated docs nav ( docs/index.md , mkdocs.yml ) Run commands: parity tests: python -m pytest -q tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py microbench: python -m apex_x.bench.triton_tileunpack_bench --batch 1 --channels 128 --height 128 --width 128 --tile-size 8 --kmax 32 --overlap-shift 4 --warmup 10 --iters 50 --dtype fp16 lint/type: python -m ruff check apex_x/kernels/triton/tileunpack.py apex_x/bench/triton_tileunpack_bench.py tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tileunpack.py apex_x/bench/triton_tileunpack_bench.py Validation status: python -m ruff check ... passed python -m mypy --cache-dir=/dev/null ... passed python -m pytest -q tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py passed (GPU tests skipped on CPU-only env) python -m apex_x.bench.triton_tileunpack_bench --iters 3 --warmup 1 --batch 1 --channels 8 --height 32 --width 32 --tile-size 4 --kmax 4 --overlap-shift 2 --dtype fp16 executed successfully (reference backend on CPU) .venv/bin/mkdocs build --strict passed Triton FusionGate alpha/fusion kernels implemented with fallback dispatch: Added kernel module: apex_x/kernels/triton/fusiongate.py Added exports: apex_x/kernels/triton/__init__.py Implemented kernels and dispatch: alpha kernel: inputs: boundary/uncertainty proxies ( [B,1,H,W] or [B,H,W] ) output: alpha[B,1,H,W] formula: alpha = sigmoid(softplus(w_b) * boundary + softplus(w_u) * uncertainty + bias) optional fusion kernel: fused = base + alpha * (detail - base) supports optional in-place output in dispatch API fallback semantics: falls back to reference when Triton/CUDA unavailable falls back when autograd is requested and inference_only=True Added tests: tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py coverage: parity vs apex_x.model.FusionGate.compute_alpha (simplified alpha path) alpha range checks ( [0,1] ) optional fusion parity GPU parity auto-skip without CUDA+Triton Added microbenchmark: apex_x/bench/triton_fusiongate_bench.py measures: alpha reference vs dispatch alpha+fusion reference vs dispatch Added docs: docs/runtime/TRITON_FUSION.md updated: docs/runtime/TRITON.md docs/index.md mkdocs.yml Run commands: tests: python -m pytest -q tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py benchmark: python -m apex_x.bench.triton_fusiongate_bench --batch 1 --channels 128 --height 128 --width 128 --warmup 10 --iters 50 --dtype fp16 lint/type: python -m ruff check apex_x/kernels/triton/fusiongate.py apex_x/bench/triton_fusiongate_bench.py tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/fusiongate.py apex_x/bench/triton_fusiongate_bench.py Validation status: python -m ruff check ... passed python -m mypy --cache-dir=/dev/null ... passed python -m pytest -q tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py passed (GPU tests skipped on CPU-only env) python -m apex_x.bench.triton_fusiongate_bench --iters 3 --warmup 1 --batch 1 --channels 8 --height 32 --width 32 --dtype fp16 executed successfully (reference backend on CPU) .venv/bin/mkdocs build --strict passed","title":"What Exists Right Now (2026-02-07)"},{"location":"CONTEXT/#invariants-to-preserve","text":"Deterministic inference tile selection under fixed config Fixed Kmax -buffer shape contract for runtime compatibility No Python-side dynamic control flow in future export graph path CPU baseline must remain runnable at all times","title":"Invariants to Preserve"},{"location":"CONTEXT/#open-risks","text":"Tile-SSM is currently a placeholder scan, not final kernel-equivalent behavior Detection/segmentation heads are minimal baseline stubs Runtime plugins are currently specification-only, not implemented","title":"Open Risks"},{"location":"CONTEXT/#immediate-next-steps","text":"Expand baseline heads to full DET + INST-SEG proto path per spec. Add explicit continuous-budget training loop example with dual mu update. Add deterministic quadtree L1/L2 split implementation and tests. Add export smoke tests (ONNX contract checks with fixed Kmax ). Add perf threshold config file and CI perf guard for CPU baseline. Add CLI entrypoints (Typer + Rich) for run , test , and perf commands. Add initial concrete losses/ , train/ , infer/ , and export/ implementations beyond placeholders. Add a typed Typer command for config validate --config path.yaml --set key=value using new loader/override utilities. Add logging configuration knobs into RuntimeConfig (log level/format) and route through configure_logging() . Add apex-x config validate and apex-x config dump subcommands for explicit config workflows. Add docs deployment workflow (e.g., GitHub Pages) after docs structure stabilizes. Add real TRT/ORT RuntimeAdapterProtocol implementations behind feature flags. Add smoke example invocation to README and optionally CI as a dedicated quick check. Add ADR template/checklist for future decisions to keep decision records uniform. Integrate L0 mapping helpers directly inside pack/unpack metadata paths (store tile (ty,tx) alongside pixel origins) for easier runtime plugin parity checks. Add non-square grid Hilbert fixture coverage (e.g., 3x5 , 5x3 ) to lock padded-power-of-two traversal behavior. Add explicit fixture snapshots for scan modes ( l2r/r2l/u2d/d2u ) on representative non-square grids and enforce them in CI. Connect split-budget selection ( B2/B3 ) in inference path to quadtree depth-2 mappings/metadata and add end-to-end selection tests. Integrate TileSelection / TileSelectionTrace emission into model inference outputs and add CLI flag to dump selection traces for ablations. Add optional CLI command to generate overlay images from stored TileSelectionTrace JSON for quick qualitative routing inspection. Wire StaticCostModel into routing/inference selection path so budgeting uses per-level C_c/C_h + pack/unpack/split overhead directly instead of scalar placeholders. Integrate sample_oracle_set(...) into training loops so oracle subset S is produced from random + uncertainty-biased policies directly from PV u_hat . Add budget-selection debug artifact that logs per-tile score/rank and final stable tie-break order for exact replay in ablation runs. Integrate PV aggregation output x_i into router training/inference path so utility heads consume pooled mean/max/var vectors instead of placeholder signals. Wire RouterTinyMLP into model/routing execution path as the default trainable router backend (with config-selectable fallback to IdentityRouter ). Add config switch for router backend ( identity / tiny_mlp / kan_like ) and wire RouterKANLike into inference/training stubs. Integrate ste_gate_from_utilities(...) into training stubs so router utilities produce p_i / g_i directly in continuous-budget examples. Wire BudgetDualController into training stubs so mu , E[C] , and budget term are tracked/updated per step with debug logs. Use GreedySelectionResult.kmax_buffer + valid_count directly in model inference outputs to mirror runtime plugin shape contracts. Integrate deterministic_two_stage_selection(...) into model inference path so B1/B2 and L1 routing are exercised end-to-end in CPU baseline outputs. Use hysteresis_rollout(...) in temporal/video inference stubs and log count_mask_toggles(...) as an anti-flicker metric. Extend routing diagnostics to include L1/L2 selection once two-stage routing is wired into the model forward path. Add optional artifact export for diagnostics snapshots (JSON + histogram plots) from CLI train/predict commands for ablation workflows. Wire toggle states into YAML examples/README config snippets so users can reproduce dense/no-SSM/no-nesting baselines quickly. Integrate torch tile pack/unpack path into runtime adapter abstractions and add a CPU fallback selection path for adapter-level smoke tests. Integrate FusionGate into the model forward path (or runtime adapter path) to replace direct heavy overwrite with proxy-conditioned fusion in CPU baseline experiments. Integrate CheapBlock into PV/FF cheap path stubs and add micro-benchmarks for block latency under CPU profiles. Integrate TileRefineBlock after Tile-SSM in the model forward path so packed-tile local refinement is exercised end-to-end in baseline outputs. Wire PVBackbone into model execution path as the canonical PV stream source ( P3/P4/P5 ) and align routing signals to these outputs. Integrate PVModule into ApexXModel.forward so routing and diagnostics consume PV coarse proxies instead of handcrafted tile-signal placeholders. Replace/augment NumPy tile_ssm_scan usage in ApexXModel.forward with StableStateSpaceScan in torch execution paths and add parity checks for inference outputs. Add ApexXModel config switch for scan direction mode ( forward vs bidirectional ) and wire merge-gated bidirectional scan into packed-tile path. Wire decode_and_nms(...) into model/inference outputs so DET head predictions use the new deterministic decode/NMS path in end-to-end CPU runs. Wire PrototypeInstanceSegHead into end-to-end model/infer path (using DET-selected instances) and expose assembled masks in CLI predict outputs. Integrate instance_segmentation_losses(...) into training stubs with mask/box matching targets and log BCE/Dice/boundary components in trainer diagnostics. Wire FFTileRefinementHook active-tile indices from routing outputs in model/infer path so refinement uses real FF-selected tiles end-to-end. Wire generate_panoptic_output(...) into inference/CLI outputs so panoptic maps and segments_info are emitted from DET + INST-SEG + SEM-SEG predictions end-to-end. Wire evaluate_panoptic_quality(...) into dataset evaluation loops so apex-x eval can consume real predicted/GT panoptic artifacts and report dataset-level PQ metrics. Integrate TrackEmbeddingHead into model/infer outputs and add config-controlled tracking head enable/disable behavior. Add a basic association loop wrapper in apex_x/infer that keeps TrackState across frames and emits stable track IDs in CLI predict . Add optional motion gating term into Hungarian cost/gate path (for video mode) and verify flicker reduction with temporal fixtures. Integrate apply_pcgradpp(...) into concrete training step codepath so DET/SEG grouped losses are projected on shared trunk params during optimization and surfaced in trainer diagnostics. Integrate distillation_losses(...) into the concrete training path with config-driven weights/temperature/feature-layer selection and expose per-component values in trainer diagnostics. Integrate compute_oracle_delta_targets(...) + utility_oracle_loss(...) into router training loops so sampled set S drives utility regression/ranking with detached oracle targets. Wire TeacherModel into train/eval loops so EMA updates, distill outputs, and student-teacher loss plumbing are exercised end-to-end with config toggles. Expand ApexXTrainer stage loop from smoke-level synthetic batches to dataset-backed dataloaders with checkpointing/resume support. Add stage-aware CLI logging/artifacts ( stage_metrics.json , mu_history.json ) for ablation reproducibility. Add CI smoke command for staged training CLI ( apex-x train --steps-per-stage 1 ) to guard regressions in train wiring. Wire TransformPipeline and MosaicV2 into an actual dataset/dataloader path controlled by DataConfig knobs ( flip_prob , mosaic_prob , scale range). Add serialization/debug helpers to visualize transformed boxes/masks and mosaic split/crop decisions for reproducible augmentation ablations. Add dataset-wide evaluation loops that consume real model predictions and emit the new eval report (JSON/MD) directly from inference artifacts, beyond tiny fixture mode. Extend ablation runner to ingest real dataset eval outputs (instead of tiny fixture metrics) and add per-toggle significance summaries across seeds. Expand QAT coverage beyond Conv/Linear wrappers to selected normalization-sensitive blocks with explicit parity gates versus FP16 baseline. Add runtime-backed FP8 kernel probe path (beyond capability check) and enforce parity/perf gates before enabling FP8-by-default on compatible GPUs. Implement real Triton fused gather+gate+scatter kernel and wire it under gather_gate_scatter(...) dispatch when CUDA+Triton are available; add parity + perf thresholds against reference path. Add dataset/profile-specific perf baselines (e.g., quality/balanced/edge configs) and split tolerances by CPU model class for stricter regression gates.","title":"Immediate Next Steps"},{"location":"CONTEXT/#latest-update-2026-02-08-triton-fused-stage-1-pipeline","text":"Added a new practical fused Triton fast path module: apex_x/kernels/triton/fused_pack_op_unpack.py Implements gather -> pointwise affine + ReGLU-like gate -> scatter in one Triton kernel launch sequence. Added dispatch + fallback API: get_triton_fused_stage1_availability() fused_pack_op_unpack_reference(...) fused_pack_op_unpack_triton(...) fused_pack_op_unpack_dispatch(...) Determinism rule for Stage-1 path: requires unique tile indices per batch row to avoid overlap write races. Added exports: apex_x/kernels/triton/__init__.py now exports fused Stage-1 APIs. Added parity tests: tests/test_triton_fused_stage1_dispatch.py tests/test_triton_fused_stage1_gpu.py Added microbenchmark: apex_x/bench/triton_fused_stage1_bench.py compares: explicit reference composition ( pack -> op -> unpack ) separate dispatch composition ( TilePack dispatch -> op -> TileUnpack dispatch ) fused Stage-1 dispatch Added docs: docs/runtime/TRITON_FUSED_STAGE1.md updated: docs/runtime/TRITON.md docs/index.md mkdocs.yml","title":"Latest Update (2026-02-08): Triton Fused Stage-1 Pipeline"},{"location":"CONTEXT/#run-commands","text":"Tests: python -m pytest -q tests/test_triton_fused_stage1_dispatch.py tests/test_triton_fused_stage1_gpu.py Microbenchmark: python -m apex_x.bench.triton_fused_stage1_bench --batch 1 --channels 128 --height 128 --width 128 --tile-size 8 --kmax 32 --warmup 10 --iters 50 --dtype fp16 Lint/type quick checks: python -m ruff check apex_x/kernels/triton/fused_pack_op_unpack.py tests/test_triton_fused_stage1_dispatch.py tests/test_triton_fused_stage1_gpu.py apex_x/bench/triton_fused_stage1_bench.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/fused_pack_op_unpack.py apex_x/bench/triton_fused_stage1_bench.py","title":"Run Commands"},{"location":"CONTEXT/#remaining-work","text":"Wire Stage-1 fused kernel into legacy runtime entrypoint: apex_x/runtime/triton_fused.py::gather_gate_scatter(...) Extend fused kernel beyond Stage-1 local transform: add overlap-priority semantics in-kernel where needed integrate Tile-SSM-related fused blocks (next stages) Add GPU CI perf threshold gates for speedup_separate_over_fused .","title":"Remaining Work"},{"location":"CONTEXT/#latest-update-2026-02-08-triton-tilessm-scan-baseline","text":"Added Triton TileSSM scan module: apex_x/kernels/triton/tilessm_scan.py Forward-only recurrence scan over tokens tokens[B,K,C] with stable sanitization/clamping. Outputs: y[B,K,C] final_state[B,C] Added availability + dispatch API: get_triton_tilessm_availability() tilessm_scan_reference(...) tilessm_scan_triton(...) tilessm_scan_dispatch(...) Dispatch keeps training-safe behavior: inference_only=True falls back to reference when autograd is active. Exported TileSSM API: apex_x/kernels/triton/__init__.py Integrated inference path into model heavy FF scan: updated apex_x/model/ff_heavy_path.py new use_triton_inference_scan toggle eval mode uses tilessm_scan_dispatch(...) train mode keeps torch scan path ( StableStateSpaceScan / StableBidirectionalStateSpaceScan ) updated apex_x/model/ff_module.py routes RuntimeConfig.enable_runtime_plugins to FFHeavyPath(..., use_triton_inference_scan=...) Added tests: tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py Added throughput benchmark: apex_x/bench/triton_tilessm_bench.py Added docs: docs/runtime/TRITON_SSM.md updated: docs/runtime/TRITON.md docs/index.md mkdocs.yml","title":"Latest Update (2026-02-08): Triton TileSSM Scan Baseline"},{"location":"CONTEXT/#run-commands_1","text":"Tests: python -m pytest -q tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py tests/test_ff_heavy_path.py Benchmark: python -m apex_x.bench.triton_tilessm_bench --batch 2 --steps 256 --channels 128 --warmup 10 --iters 50 --dtype fp16 Lint/type checks: python -m ruff check apex_x/kernels/triton/tilessm_scan.py apex_x/model/ff_heavy_path.py apex_x/model/ff_module.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tilessm_scan.py apex_x/model/ff_heavy_path.py apex_x/model/ff_module.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py Docs: .venv/bin/mkdocs build --strict","title":"Run Commands"},{"location":"CONTEXT/#validation-status","text":"ruff : passed on changed TileSSM files. mypy : passed on changed TileSSM files. pytest : passed for new parity/integration tests (GPU tests auto-skipped on CPU-only environment). benchmark smoke run: completed on CPU fallback path. docs build: passed with strict mode.","title":"Validation Status"},{"location":"CONTEXT/#remaining-work_1","text":"Add multi-direction scan execution mode in Triton TileSSM path (current kernel is forward-only baseline). Add a fused TileSSM + tile-local refine path after this baseline. Add GPU CI lane for TileSSM parity/perf thresholds when CUDA runners are available.","title":"Remaining Work"},{"location":"CONTEXT/#latest-update-2026-02-08-triton-tilessm-multi-direction","text":"Extended apex_x/kernels/triton/tilessm_scan.py to support directional scanning: direction : forward , backward , bidirectional merge_mode for bidirectional: sum , avg , gated optional torch-computed merge_gate for gated merge ( [C] or [B,1,C] ) Added clean directional API: scan(tokens, direction=...) -> y routes through dispatch with fallback behavior Kept training/inference separation: training/backward still uses torch scan path inference dispatch can use Triton path ( inference_only=True ) Updated FF inference integration: apex_x/model/ff_heavy_path.py Triton inference path now uses directional dispatch ( forward and backward ) and applies learned torch gate for merge in bidirectional mode. Updated exports: apex_x/kernels/triton/__init__.py now exports: ScanDirection BidirectionalMergeMode scan Updated benchmark for multi-direction overhead: apex_x/bench/triton_tilessm_bench.py now reports forward/backward/bidirectional timings and overhead ratios vs forward. Updated tests: tests/test_triton_tilessm_parity_dispatch.py added backward parity vs torch manual recurrence added bidirectional parity for sum/avg/gated added clean API scan(...) test tests/test_triton_tilessm_parity_gpu.py added bidirectional avg parity test (GPU) Updated docs: docs/runtime/TRITON_SSM.md docs/runtime/TRITON.md","title":"Latest Update (2026-02-08): Triton TileSSM Multi-Direction"},{"location":"CONTEXT/#run-commands_2","text":"Tests: python -m pytest -q tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py tests/test_ff_heavy_path.py Benchmark: python -m apex_x.bench.triton_tilessm_bench --batch 2 --steps 256 --channels 128 --warmup 10 --iters 50 --dtype fp16 Lint/type: python -m ruff check apex_x/kernels/triton/tilessm_scan.py apex_x/kernels/triton/__init__.py apex_x/model/ff_heavy_path.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tilessm_scan.py apex_x/kernels/triton/__init__.py apex_x/model/ff_heavy_path.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py Docs: .venv/bin/mkdocs build --strict","title":"Run Commands"},{"location":"CONTEXT/#validation-status_1","text":"ruff : passed mypy : passed pytest : passed (GPU tests skipped on CPU-only environment) benchmark smoke run: passed (reference fallback on CPU) docs build ( mkdocs --strict ): passed","title":"Validation Status"},{"location":"CONTEXT/#latest-update-2026-02-08-tensorrt-build-hardening-harness","text":"Read runtime specs from: docs/runtime/PLUGIN_SPEC.md (canonical) docs/runtime/TENSORRT.md Added alias page: docs/runtime/PLUGIN_SPECS.md -> points to canonical spec Hardened TensorRT CMake in runtime/tensorrt/CMakeLists.txt : shared plugin library support: apexx_trt_plugins (SHARED) added always-build static core: apexx_trt_plugin_core (STATIC, PIC) compile-guard behavior: shared plugin library builds only when TensorRT headers and CUDA compiler are found if TRT/CUDA unavailable, shared build is skipped cleanly and repo remains buildable plugin info target kept available: apexx_trt_plugin_info harness target added conditionally (only with shared build): apexx_trt_plugin_harness Added minimal plugin enqueue-like path for stubs: runtime/tensorrt/include/apexx_trt/plugin_stub.hpp DummyTensor , PluginEnqueueInputs , PluginEnqueueOutputs , PluginStub::enqueue(...) implemented enqueue methods in: runtime/tensorrt/src/tile_pack_plugin.cpp runtime/tensorrt/src/tile_ssm_scan_plugin.cpp runtime/tensorrt/src/tile_unpack_fusion_plugin.cpp runtime/tensorrt/src/decode_nms_plugin.cpp Added shared-library C ABI entrypoints in: runtime/tensorrt/include/apexx_trt/common.hpp runtime/tensorrt/src/common.cpp symbols: apexx_trt_abi_version() apexx_trt_build_summary_cstr() apexx_trt_invoke_minimal(...) Added minimal runtime harness executable source: runtime/tensorrt/tests/plugin_harness_main.cpp harness behavior: loads plugin shared library via dlopen / LoadLibrary resolves C ABI symbols creates dummy tensors invokes minimal plugin call path for TilePack/TileSSMScan/TileUnpackFusion/DecodeNMS Added build doc: docs/runtime/TENSORRT_BUILD.md Updated docs navigation and runtime note cross-links: mkdocs.yml docs/index.md docs/runtime/TENSORRT.md","title":"Latest Update (2026-02-08): TensorRT Build Hardening + Harness"},{"location":"CONTEXT/#exact-build-commands","text":"Auto-detect build: cd runtime/tensorrt cmake -S . -B build cmake --build build -j ./build/apexx_trt_plugin_info Explicit TRT/CUDA paths: cd runtime/tensorrt cmake -S . -B build -DTENSORRT_INCLUDE_DIR=\\\"${TENSORRT_ROOT}/include\\\" -DCMAKE_CUDA_COMPILER=\\\"${CUDA_HOME}/bin/nvcc\\\" cmake --build build -j Force skip shared plugin build (portable/CI machines): cd runtime/tensorrt cmake -S . -B build -DAPEXX_ENABLE_TENSORRT=OFF -DAPEXX_ENABLE_CUDA=OFF -DAPEXX_BUILD_PLUGIN_TEST_HARNESS=OFF cmake --build build -j Harness run (when shared plugin target is built): ./build/apexx_trt_plugin_harness ./build/libapexx_trt_plugins.so or: export APEXX_TRT_PLUGIN_LIB=./build/libapexx_trt_plugins.so ./build/apexx_trt_plugin_harness","title":"Exact Build Commands"},{"location":"CONTEXT/#environment-variables","text":"TENSORRT_ROOT : TensorRT install root (optional) CUDA_HOME : CUDA root (optional) CMAKE_PREFIX_PATH : dependency discovery override (optional) APEXX_TRT_PLUGIN_LIB : path to shared plugin library for harness runtime loading","title":"Environment Variables"},{"location":"CONTEXT/#validation-status_2","text":"mkdocs build --strict : passed pytest tests/test_import_smoke.py : passed Local CMake configure/build execution could not be run in this environment because cmake binary is not installed ( command not found ).","title":"Validation Status"},{"location":"CONTEXT/#update-protocol-every-significant-change","text":"Update this file with: what changed why it changed what to do next If architecture changed, also update docs/DECISIONS.md If requirements changed, update PRD/spec first, then code","title":"Update Protocol (Every Significant Change)"},{"location":"DECISIONS/","text":"Architectural Decisions ADR-0001: Authoritative Documentation Split Date: 2026-02-07 Decision: Keep requirements in docs/PRD.md and implementation contract in docs/ENGINEERING_SPEC.md . Rationale: Separates product scope from engineering detail while preserving traceability. Consequence: Any architecture change must update both files. ADR-0002: CPU-Only Reference Baseline First Date: 2026-02-07 Decision: Start with a pure Python + NumPy CPU baseline before runtime kernels. Rationale: Fast iteration, deterministic debugging, and testable contracts independent of GPU/runtime stack. Consequence: Runtime parity work follows once behavior is stabilized. ADR-0003: Deterministic Greedy Budgeting for Inference Date: 2026-02-07 Decision: Use utility-per-cost greedy selection with fixed Kmax buffers. Rationale: Deterministic, simple, and export/runtime friendly. Consequence: Potential global-optimality gap vs exact knapsack is accepted for speed and determinism. ADR-0004: Ordering Modes Required in Contract Date: 2026-02-07 Decision: Support both Hilbert and multi-direction ordering modes. Rationale: Preserves geometry for sequence mixing and supports runtime experimentation. Consequence: Ordering determinism tests are mandatory. ADR-0005: Naming Conventions Baseline Date: 2026-02-07 Decision: Python modules/files use snake_case . Classes use PascalCase . Protocol interfaces use *Protocol suffix (for example RouterProtocol ). Config sections use stable top-level keys: model , routing , train , data , runtime . Rationale: Consistent naming lowers integration friction and improves discoverability across modules. Consequence: New public interfaces and new files must follow these naming rules. ADR-0006: Tensor Shape Conventions Date: 2026-02-07 Decision: Image/features default to channel-first NCHW ( [B,C,H,W] ). Detection boxes use [B,N,4] with cx,cy,w,h . Packed tile tensors use [B,K,C,t,t] . Tile index tensors use [B,K] and rely on fixed Kmax contracts. Rationale: Stable tensor contracts are required for deterministic runtime behavior and export parity. Consequence: Any shape-contract change must update docs/PRD.md , docs/ENGINEERING_SPEC.md , and tests. ADR-0007: Determinism Rules Date: 2026-02-07 Decision: Inference tile selection uses deterministic greedy utility-per-cost with fixed Kmax . Tile ordering remains deterministic (Hilbert or multi-direction scan). Reproducibility utilities ( seed_all , deterministic toggles) are part of baseline workflow. Export/runtime path must avoid Python-side inference control flow. Rationale: Determinism is a hard product constraint for regression tracking and deployment confidence. Consequence: Any nondeterministic path requires explicit rationale and targeted determinism tests.","title":"Decisions"},{"location":"DECISIONS/#architectural-decisions","text":"","title":"Architectural Decisions"},{"location":"DECISIONS/#adr-0001-authoritative-documentation-split","text":"Date: 2026-02-07 Decision: Keep requirements in docs/PRD.md and implementation contract in docs/ENGINEERING_SPEC.md . Rationale: Separates product scope from engineering detail while preserving traceability. Consequence: Any architecture change must update both files.","title":"ADR-0001: Authoritative Documentation Split"},{"location":"DECISIONS/#adr-0002-cpu-only-reference-baseline-first","text":"Date: 2026-02-07 Decision: Start with a pure Python + NumPy CPU baseline before runtime kernels. Rationale: Fast iteration, deterministic debugging, and testable contracts independent of GPU/runtime stack. Consequence: Runtime parity work follows once behavior is stabilized.","title":"ADR-0002: CPU-Only Reference Baseline First"},{"location":"DECISIONS/#adr-0003-deterministic-greedy-budgeting-for-inference","text":"Date: 2026-02-07 Decision: Use utility-per-cost greedy selection with fixed Kmax buffers. Rationale: Deterministic, simple, and export/runtime friendly. Consequence: Potential global-optimality gap vs exact knapsack is accepted for speed and determinism.","title":"ADR-0003: Deterministic Greedy Budgeting for Inference"},{"location":"DECISIONS/#adr-0004-ordering-modes-required-in-contract","text":"Date: 2026-02-07 Decision: Support both Hilbert and multi-direction ordering modes. Rationale: Preserves geometry for sequence mixing and supports runtime experimentation. Consequence: Ordering determinism tests are mandatory.","title":"ADR-0004: Ordering Modes Required in Contract"},{"location":"DECISIONS/#adr-0005-naming-conventions-baseline","text":"Date: 2026-02-07 Decision: Python modules/files use snake_case . Classes use PascalCase . Protocol interfaces use *Protocol suffix (for example RouterProtocol ). Config sections use stable top-level keys: model , routing , train , data , runtime . Rationale: Consistent naming lowers integration friction and improves discoverability across modules. Consequence: New public interfaces and new files must follow these naming rules.","title":"ADR-0005: Naming Conventions Baseline"},{"location":"DECISIONS/#adr-0006-tensor-shape-conventions","text":"Date: 2026-02-07 Decision: Image/features default to channel-first NCHW ( [B,C,H,W] ). Detection boxes use [B,N,4] with cx,cy,w,h . Packed tile tensors use [B,K,C,t,t] . Tile index tensors use [B,K] and rely on fixed Kmax contracts. Rationale: Stable tensor contracts are required for deterministic runtime behavior and export parity. Consequence: Any shape-contract change must update docs/PRD.md , docs/ENGINEERING_SPEC.md , and tests.","title":"ADR-0006: Tensor Shape Conventions"},{"location":"DECISIONS/#adr-0007-determinism-rules","text":"Date: 2026-02-07 Decision: Inference tile selection uses deterministic greedy utility-per-cost with fixed Kmax . Tile ordering remains deterministic (Hilbert or multi-direction scan). Reproducibility utilities ( seed_all , deterministic toggles) are part of baseline workflow. Export/runtime path must avoid Python-side inference control flow. Rationale: Determinism is a hard product constraint for regression tracking and deployment confidence. Consequence: Any nondeterministic path requires explicit rationale and targeted determinism tests.","title":"ADR-0007: Determinism Rules"},{"location":"ENGINEERING_SPEC/","text":"Apex-X v4 Engineering Specification 1. Authority and Scope This document is the engineering contract for Apex-X v4 implementation. docs/PRD.md defines product requirements; this file defines implementation details, equations, interfaces, and test criteria. 2. System Overview Apex-X v4 is a dynamic compute graph with: - dense low-resolution PV stream - sparse high-resolution FF stream - tile utility router - budget controllers for training and inference - tile pack/ssm/unpack path for heavy compute 3. Tensor and Shape Contracts 3.1 Inputs Image: x in R^{B x 3 x H x W} Optional temporal state: previous tile mask z(t-1) previous utilities U(t-1) previous SSM state s(t-1) 3.2 Core Feature Maps PV feature map ( stride 16 or 32 ): F_pv FF primary feature map ( stride 8 ): F_ff8 FF refine feature map ( stride 4 , optional): F_ff4 3.3 Tile Pack/Unpack Pack: - Inputs: - F: [B,C,Hf,Wf] - idx: [B,K] - fixed tile size t - Outputs: - P: [B,K,C,t,t] - meta = {origins, ordered_idx, grid_shape, tile_size} Unpack: - Inputs: - F_base: [B,C,Hf,Wf] - P_out: [B,K,C,t,t] - meta - optional gate and priority masks - Output: - F_merged: [B,C,Hf,Wf] Overlap semantics: - default: higher nesting level overrides lower ( L2 > L1 > L0 ) - optional weighted blend if explicitly configured 4. Router Specification 4.1 Router Inputs per Tile For tile i , aggregate PV statistics over mapped PV region: - objectness o_hat - uncertainty u_hat - boundary proxy b_hat - variance/energy v_hat - optional motion m_hat Feature vector: [ x_i \\in \\mathbb{R}^d,\\; d\\approx 12\\ldots24 ] 4.2 Router Outputs utility U_i split utility S_i optional temporal keep score T_i 4.3 Utility Oracle (DeltaLoss) For oracle subset S : [ \\Delta_i = L_{distill}(y^{(i=0)}, y_T) - L_{distill}(y^{(i=1)}, y_T) ] Train router utility head against Delta_i with L1 or ranking loss. Clamp outliers: [ \\Delta_i \\leftarrow clamp(\\Delta_i, -\\tau, \\tau) ] Pseudo-code: S = random_subset(tiles, rate=0.1..0.2) U high_uncertainty_tiles for i in S: loss_off = L_distill(run(tile_i=off), y_teacher) loss_on = L_distill(run(tile_i=on), y_teacher) Delta_i = clamp(loss_off - loss_on, -tau, tau) L_util = regression_or_ranking(U_i, Delta_i) 5. Continuous Budgeting (Training) 5.1 Gating [ p_i = \\sigma(U_i),\\quad g_i = STE(p_i) ] Forward uses g_i as hard gate; backward uses straight-through gradient. 5.2 Expected Cost [ \\mathbb{E}[C] = \\sum_i \\left(p_i C_h + (1-p_i) C_c\\right) ] where C_h and C_c are heavy and cheap path costs. 5.3 Dual Optimization Objective: [ L = L_{main} + \\mu(\\mathbb{E}[C]-B),\\quad \\mu\\ge0 ] Projected dual ascent: [ \\mu \\leftarrow max(0, \\mu + \\eta_\\mu (\\mathbb{E}[C]-B)) ] Pseudo-code: U = Router(x) p = sigmoid(U) g = STE(p) y = Forward(g) L_main = L_det + lambda_seg*L_seg + ... E = sum(p_i*C_h + (1-p_i)*C_c) L = L_main + mu*(E - B) opt_theta.step(grad(L, theta)) mu = max(0, mu + eta_mu*(E - B)) 6. Deterministic Inference Budgeting 6.1 Greedy Utility-per-Cost Given U_i , DeltaC_i , budget B , and fixed Kmax : [ score_i = \\frac{U_i}{\\Delta C_i} ] Sort descending; accept while both constraints hold: - spent + DeltaC_i <= B - |active| < Kmax 6.2 Kmax Buffer Contract runtime buffers are allocated to Kmax actual selected tile count is K <= Kmax unused entries are ignored via valid-count metadata Pseudo-code: scores = [U_i / DeltaC_i for i in tiles] order = argsort(scores, descending=True) active, spent = [], 0 for i in order: if len(active) >= Kmax: break if spent + DeltaC_i <= B: active.append(i) spent += DeltaC_i write_to_kmax_buffer(active) 7. Quadtree Nesting L0/L1/L2 7.1 Tile Hierarchy L0 : coarse tiles ( t0 ) L1 : t1=t0/2 L2 : t2=t1/2 (optional) 7.2 Budget Split B1 : heavy compute for selected L0 B2 : split budget for finer tiles optional B3 for L2 7.3 Split Utility [ score_i^{split} = \\frac{S_i}{O_{split,i}} ] Select split candidates under B2 . Pseudo-code: L0 = select_tiles(U_L0, cost_L0, B1, Kmax_L0) split_candidates = [] for i in L0: split_candidates.append((i, S_i / O_split_i)) L1 = select_tiles(split_candidates, split_cost, B2, Kmax_L1) if nesting_depth >= 2: L2 = recursive_split(L1, B3, Kmax_L2) 8. Ordering and Sequence Geometry 8.1 Required Modes Hilbert ordering for geometry-preserving locality Multi-direction scan aggregation: L->R R->L U->D D->U 8.2 Determinism Rule Ordering function must be pure and deterministic for the same input tile set and grid shape. 9. Tile-SSM and Local Refine 9.1 Tokenization Mode A (tile token): [ v_i = Pool(P_i),\\quad v_i\\in\\mathbb{R}^C ] Sequence v_1..v_K processed by Tile-SSM. Mode B (sub-patch token): - split each tile into 2x2 or 4x4 patches - use extended token sequence for fine detail 9.2 Mamba-like Placeholder Contract Tile-SSM module must provide: - streaming scan over ordered token sequence - optional persistent recurrent state - bounded temporary memory 9.3 Fusion Gate Heavy output and base feature merge: [ F_{merged} = F_{base} + g_{fuse} \\odot (F_{heavy} - F_{base}) ] where g_fuse in [0,1] may be scalar, channel-wise, or spatial. 10. Losses and Multi-task Gradient Handling 10.1 Task Losses DET: focal/quality focal + IoU-family box loss (+ optional DFL) INST-SEG: BCE + Dice + boundary loss Optional task heads define their own loss terms 10.2 PCGrad++ for Shared Trunk For gradient pair (g_a, g_b) on shared params, if cos(g_a, g_b) < 0 : [ g_a \\leftarrow g_a - \\frac{g_a\\cdot g_b}{||g_b||^2}g_b ] Do not project head-specific gradients. 11. Temporal Hysteresis and State Reuse 11.1 State z(t-1) : previous active tiles U(t-1) : previous utility s(t-1) : previous SSM state 11.2 Update Rule Hysteresis: [ z_i(t)=\\mathbf{1}[U_i(t)>\\theta_{on} \\lor (U_i(t)>\\theta_{off} \\land z_i(t-1)=1)] ] theta_on > theta_off to prevent flicker. Stability loss: [ L_{stab}=\\sum_i |p_i(t)-p_i(t-1)| ] Optional spatial TV term may be added. 12. Runtime Plugin Specifications Detailed runtime notes also live in docs/runtime/PLUGIN_SPEC.md . 12.1 TilePack Plugin Inputs: F, idx Outputs: P, meta Requirements: - FP16 and FP8 data support - deterministic ordering - bounded workspace 12.2 TileSSMScan Plugin Inputs: packed tokens/tiles + optional prior state Outputs: mixed outputs + next state Requirements: - streaming scan - multi-direction mode - no unbounded temporary allocations 12.3 TileUnpackFusion Plugin Inputs: F_base, P_out, meta, optional gate Outputs: merged feature map Requirements: - residual + gate semantics - overlap priority support 12.4 Optional Plugins MaskedConv fused DET decode + NMS 13. QAT and Precision Policy 13.1 Profiles Quality: FP16-heavy path, larger tile budget Balanced: FP16/FP8 mixed Edge: INT8 with router in FP16 13.2 QAT Rules keep router logits and normalization in higher precision during QAT calibrate heavy branches per-level validate parity vs FP16 baseline before enabling INT8 by default 13.3 Acceptance Threshold mAP/mIoU degradation must stay within agreed profile thresholds 14. Performance Regression Testing 14.1 Required Metrics latency p50/p95 memory peak selected tile count distribution budget adherence ( spent <= B ) 14.2 Regression Policy A change fails perf gate if: - p95 latency worsens over threshold - memory peak exceeds threshold - budget overshoot rate is non-zero in deterministic mode 14.3 Baseline Tooling scripts/perf_regression.py provides CPU reference timing runtime-specific harnesses must follow same report schema 15. Export Contracts (TRT/ORT) dynamic tile count only via fixed Kmax buffers + valid count fixed tile size and fixed max nesting depth in model profile inference graph must not depend on Python control flow 16. CPU Baseline Requirements Reference implementation must: - run on CPU-only environment - implement routing, deterministic budgeting, pack/unpack, and placeholder Tile-SSM - include tests for core contracts 17. Validation Matrix 17.1 Correctness pack/unpack identity overlap priority correctness deterministic ordering deterministic selection 17.2 Training Stability non-collapsing router probabilities under budget distillation gap trend improves over epochs 17.3 Export Parity bounded quality drop between PyTorch and TRT/ORT profiles 18. File Ownership Map apex_x/ : reference implementation tests/ : correctness checks docs/runtime/ : runtime plugin details scripts/ : repeatable perf checks 19. Change Management Any changes to equations, contracts, ordering, or precision policy require synchronized updates across: - docs/PRD.md - docs/ENGINEERING_SPEC.md - docs/DECISIONS.md - docs/CONTEXT.md","title":"Engineering Spec"},{"location":"ENGINEERING_SPEC/#apex-x-v4-engineering-specification","text":"","title":"Apex-X v4 Engineering Specification"},{"location":"ENGINEERING_SPEC/#1-authority-and-scope","text":"This document is the engineering contract for Apex-X v4 implementation. docs/PRD.md defines product requirements; this file defines implementation details, equations, interfaces, and test criteria.","title":"1. Authority and Scope"},{"location":"ENGINEERING_SPEC/#2-system-overview","text":"Apex-X v4 is a dynamic compute graph with: - dense low-resolution PV stream - sparse high-resolution FF stream - tile utility router - budget controllers for training and inference - tile pack/ssm/unpack path for heavy compute","title":"2. System Overview"},{"location":"ENGINEERING_SPEC/#3-tensor-and-shape-contracts","text":"","title":"3. Tensor and Shape Contracts"},{"location":"ENGINEERING_SPEC/#31-inputs","text":"Image: x in R^{B x 3 x H x W} Optional temporal state: previous tile mask z(t-1) previous utilities U(t-1) previous SSM state s(t-1)","title":"3.1 Inputs"},{"location":"ENGINEERING_SPEC/#32-core-feature-maps","text":"PV feature map ( stride 16 or 32 ): F_pv FF primary feature map ( stride 8 ): F_ff8 FF refine feature map ( stride 4 , optional): F_ff4","title":"3.2 Core Feature Maps"},{"location":"ENGINEERING_SPEC/#33-tile-packunpack","text":"Pack: - Inputs: - F: [B,C,Hf,Wf] - idx: [B,K] - fixed tile size t - Outputs: - P: [B,K,C,t,t] - meta = {origins, ordered_idx, grid_shape, tile_size} Unpack: - Inputs: - F_base: [B,C,Hf,Wf] - P_out: [B,K,C,t,t] - meta - optional gate and priority masks - Output: - F_merged: [B,C,Hf,Wf] Overlap semantics: - default: higher nesting level overrides lower ( L2 > L1 > L0 ) - optional weighted blend if explicitly configured","title":"3.3 Tile Pack/Unpack"},{"location":"ENGINEERING_SPEC/#4-router-specification","text":"","title":"4. Router Specification"},{"location":"ENGINEERING_SPEC/#41-router-inputs-per-tile","text":"For tile i , aggregate PV statistics over mapped PV region: - objectness o_hat - uncertainty u_hat - boundary proxy b_hat - variance/energy v_hat - optional motion m_hat Feature vector: [ x_i \\in \\mathbb{R}^d,\\; d\\approx 12\\ldots24 ]","title":"4.1 Router Inputs per Tile"},{"location":"ENGINEERING_SPEC/#42-router-outputs","text":"utility U_i split utility S_i optional temporal keep score T_i","title":"4.2 Router Outputs"},{"location":"ENGINEERING_SPEC/#43-utility-oracle-deltaloss","text":"For oracle subset S : [ \\Delta_i = L_{distill}(y^{(i=0)}, y_T) - L_{distill}(y^{(i=1)}, y_T) ] Train router utility head against Delta_i with L1 or ranking loss. Clamp outliers: [ \\Delta_i \\leftarrow clamp(\\Delta_i, -\\tau, \\tau) ] Pseudo-code: S = random_subset(tiles, rate=0.1..0.2) U high_uncertainty_tiles for i in S: loss_off = L_distill(run(tile_i=off), y_teacher) loss_on = L_distill(run(tile_i=on), y_teacher) Delta_i = clamp(loss_off - loss_on, -tau, tau) L_util = regression_or_ranking(U_i, Delta_i)","title":"4.3 Utility Oracle (DeltaLoss)"},{"location":"ENGINEERING_SPEC/#5-continuous-budgeting-training","text":"","title":"5. Continuous Budgeting (Training)"},{"location":"ENGINEERING_SPEC/#51-gating","text":"[ p_i = \\sigma(U_i),\\quad g_i = STE(p_i) ] Forward uses g_i as hard gate; backward uses straight-through gradient.","title":"5.1 Gating"},{"location":"ENGINEERING_SPEC/#52-expected-cost","text":"[ \\mathbb{E}[C] = \\sum_i \\left(p_i C_h + (1-p_i) C_c\\right) ] where C_h and C_c are heavy and cheap path costs.","title":"5.2 Expected Cost"},{"location":"ENGINEERING_SPEC/#53-dual-optimization","text":"Objective: [ L = L_{main} + \\mu(\\mathbb{E}[C]-B),\\quad \\mu\\ge0 ] Projected dual ascent: [ \\mu \\leftarrow max(0, \\mu + \\eta_\\mu (\\mathbb{E}[C]-B)) ] Pseudo-code: U = Router(x) p = sigmoid(U) g = STE(p) y = Forward(g) L_main = L_det + lambda_seg*L_seg + ... E = sum(p_i*C_h + (1-p_i)*C_c) L = L_main + mu*(E - B) opt_theta.step(grad(L, theta)) mu = max(0, mu + eta_mu*(E - B))","title":"5.3 Dual Optimization"},{"location":"ENGINEERING_SPEC/#6-deterministic-inference-budgeting","text":"","title":"6. Deterministic Inference Budgeting"},{"location":"ENGINEERING_SPEC/#61-greedy-utility-per-cost","text":"Given U_i , DeltaC_i , budget B , and fixed Kmax : [ score_i = \\frac{U_i}{\\Delta C_i} ] Sort descending; accept while both constraints hold: - spent + DeltaC_i <= B - |active| < Kmax","title":"6.1 Greedy Utility-per-Cost"},{"location":"ENGINEERING_SPEC/#62-kmax-buffer-contract","text":"runtime buffers are allocated to Kmax actual selected tile count is K <= Kmax unused entries are ignored via valid-count metadata Pseudo-code: scores = [U_i / DeltaC_i for i in tiles] order = argsort(scores, descending=True) active, spent = [], 0 for i in order: if len(active) >= Kmax: break if spent + DeltaC_i <= B: active.append(i) spent += DeltaC_i write_to_kmax_buffer(active)","title":"6.2 Kmax Buffer Contract"},{"location":"ENGINEERING_SPEC/#7-quadtree-nesting-l0l1l2","text":"","title":"7. Quadtree Nesting L0/L1/L2"},{"location":"ENGINEERING_SPEC/#71-tile-hierarchy","text":"L0 : coarse tiles ( t0 ) L1 : t1=t0/2 L2 : t2=t1/2 (optional)","title":"7.1 Tile Hierarchy"},{"location":"ENGINEERING_SPEC/#72-budget-split","text":"B1 : heavy compute for selected L0 B2 : split budget for finer tiles optional B3 for L2","title":"7.2 Budget Split"},{"location":"ENGINEERING_SPEC/#73-split-utility","text":"[ score_i^{split} = \\frac{S_i}{O_{split,i}} ] Select split candidates under B2 . Pseudo-code: L0 = select_tiles(U_L0, cost_L0, B1, Kmax_L0) split_candidates = [] for i in L0: split_candidates.append((i, S_i / O_split_i)) L1 = select_tiles(split_candidates, split_cost, B2, Kmax_L1) if nesting_depth >= 2: L2 = recursive_split(L1, B3, Kmax_L2)","title":"7.3 Split Utility"},{"location":"ENGINEERING_SPEC/#8-ordering-and-sequence-geometry","text":"","title":"8. Ordering and Sequence Geometry"},{"location":"ENGINEERING_SPEC/#81-required-modes","text":"Hilbert ordering for geometry-preserving locality Multi-direction scan aggregation: L->R R->L U->D D->U","title":"8.1 Required Modes"},{"location":"ENGINEERING_SPEC/#82-determinism-rule","text":"Ordering function must be pure and deterministic for the same input tile set and grid shape.","title":"8.2 Determinism Rule"},{"location":"ENGINEERING_SPEC/#9-tile-ssm-and-local-refine","text":"","title":"9. Tile-SSM and Local Refine"},{"location":"ENGINEERING_SPEC/#91-tokenization","text":"Mode A (tile token): [ v_i = Pool(P_i),\\quad v_i\\in\\mathbb{R}^C ] Sequence v_1..v_K processed by Tile-SSM. Mode B (sub-patch token): - split each tile into 2x2 or 4x4 patches - use extended token sequence for fine detail","title":"9.1 Tokenization"},{"location":"ENGINEERING_SPEC/#92-mamba-like-placeholder-contract","text":"Tile-SSM module must provide: - streaming scan over ordered token sequence - optional persistent recurrent state - bounded temporary memory","title":"9.2 Mamba-like Placeholder Contract"},{"location":"ENGINEERING_SPEC/#93-fusion-gate","text":"Heavy output and base feature merge: [ F_{merged} = F_{base} + g_{fuse} \\odot (F_{heavy} - F_{base}) ] where g_fuse in [0,1] may be scalar, channel-wise, or spatial.","title":"9.3 Fusion Gate"},{"location":"ENGINEERING_SPEC/#10-losses-and-multi-task-gradient-handling","text":"","title":"10. Losses and Multi-task Gradient Handling"},{"location":"ENGINEERING_SPEC/#101-task-losses","text":"DET: focal/quality focal + IoU-family box loss (+ optional DFL) INST-SEG: BCE + Dice + boundary loss Optional task heads define their own loss terms","title":"10.1 Task Losses"},{"location":"ENGINEERING_SPEC/#102-pcgrad-for-shared-trunk","text":"For gradient pair (g_a, g_b) on shared params, if cos(g_a, g_b) < 0 : [ g_a \\leftarrow g_a - \\frac{g_a\\cdot g_b}{||g_b||^2}g_b ] Do not project head-specific gradients.","title":"10.2 PCGrad++ for Shared Trunk"},{"location":"ENGINEERING_SPEC/#11-temporal-hysteresis-and-state-reuse","text":"","title":"11. Temporal Hysteresis and State Reuse"},{"location":"ENGINEERING_SPEC/#111-state","text":"z(t-1) : previous active tiles U(t-1) : previous utility s(t-1) : previous SSM state","title":"11.1 State"},{"location":"ENGINEERING_SPEC/#112-update-rule","text":"Hysteresis: [ z_i(t)=\\mathbf{1}[U_i(t)>\\theta_{on} \\lor (U_i(t)>\\theta_{off} \\land z_i(t-1)=1)] ] theta_on > theta_off to prevent flicker. Stability loss: [ L_{stab}=\\sum_i |p_i(t)-p_i(t-1)| ] Optional spatial TV term may be added.","title":"11.2 Update Rule"},{"location":"ENGINEERING_SPEC/#12-runtime-plugin-specifications","text":"Detailed runtime notes also live in docs/runtime/PLUGIN_SPEC.md .","title":"12. Runtime Plugin Specifications"},{"location":"ENGINEERING_SPEC/#121-tilepack-plugin","text":"Inputs: F, idx Outputs: P, meta Requirements: - FP16 and FP8 data support - deterministic ordering - bounded workspace","title":"12.1 TilePack Plugin"},{"location":"ENGINEERING_SPEC/#122-tilessmscan-plugin","text":"Inputs: packed tokens/tiles + optional prior state Outputs: mixed outputs + next state Requirements: - streaming scan - multi-direction mode - no unbounded temporary allocations","title":"12.2 TileSSMScan Plugin"},{"location":"ENGINEERING_SPEC/#123-tileunpackfusion-plugin","text":"Inputs: F_base, P_out, meta, optional gate Outputs: merged feature map Requirements: - residual + gate semantics - overlap priority support","title":"12.3 TileUnpackFusion Plugin"},{"location":"ENGINEERING_SPEC/#124-optional-plugins","text":"MaskedConv fused DET decode + NMS","title":"12.4 Optional Plugins"},{"location":"ENGINEERING_SPEC/#13-qat-and-precision-policy","text":"","title":"13. QAT and Precision Policy"},{"location":"ENGINEERING_SPEC/#131-profiles","text":"Quality: FP16-heavy path, larger tile budget Balanced: FP16/FP8 mixed Edge: INT8 with router in FP16","title":"13.1 Profiles"},{"location":"ENGINEERING_SPEC/#132-qat-rules","text":"keep router logits and normalization in higher precision during QAT calibrate heavy branches per-level validate parity vs FP16 baseline before enabling INT8 by default","title":"13.2 QAT Rules"},{"location":"ENGINEERING_SPEC/#133-acceptance-threshold","text":"mAP/mIoU degradation must stay within agreed profile thresholds","title":"13.3 Acceptance Threshold"},{"location":"ENGINEERING_SPEC/#14-performance-regression-testing","text":"","title":"14. Performance Regression Testing"},{"location":"ENGINEERING_SPEC/#141-required-metrics","text":"latency p50/p95 memory peak selected tile count distribution budget adherence ( spent <= B )","title":"14.1 Required Metrics"},{"location":"ENGINEERING_SPEC/#142-regression-policy","text":"A change fails perf gate if: - p95 latency worsens over threshold - memory peak exceeds threshold - budget overshoot rate is non-zero in deterministic mode","title":"14.2 Regression Policy"},{"location":"ENGINEERING_SPEC/#143-baseline-tooling","text":"scripts/perf_regression.py provides CPU reference timing runtime-specific harnesses must follow same report schema","title":"14.3 Baseline Tooling"},{"location":"ENGINEERING_SPEC/#15-export-contracts-trtort","text":"dynamic tile count only via fixed Kmax buffers + valid count fixed tile size and fixed max nesting depth in model profile inference graph must not depend on Python control flow","title":"15. Export Contracts (TRT/ORT)"},{"location":"ENGINEERING_SPEC/#16-cpu-baseline-requirements","text":"Reference implementation must: - run on CPU-only environment - implement routing, deterministic budgeting, pack/unpack, and placeholder Tile-SSM - include tests for core contracts","title":"16. CPU Baseline Requirements"},{"location":"ENGINEERING_SPEC/#17-validation-matrix","text":"","title":"17. Validation Matrix"},{"location":"ENGINEERING_SPEC/#171-correctness","text":"pack/unpack identity overlap priority correctness deterministic ordering deterministic selection","title":"17.1 Correctness"},{"location":"ENGINEERING_SPEC/#172-training-stability","text":"non-collapsing router probabilities under budget distillation gap trend improves over epochs","title":"17.2 Training Stability"},{"location":"ENGINEERING_SPEC/#173-export-parity","text":"bounded quality drop between PyTorch and TRT/ORT profiles","title":"17.3 Export Parity"},{"location":"ENGINEERING_SPEC/#18-file-ownership-map","text":"apex_x/ : reference implementation tests/ : correctness checks docs/runtime/ : runtime plugin details scripts/ : repeatable perf checks","title":"18. File Ownership Map"},{"location":"ENGINEERING_SPEC/#19-change-management","text":"Any changes to equations, contracts, ordering, or precision policy require synchronized updates across: - docs/PRD.md - docs/ENGINEERING_SPEC.md - docs/DECISIONS.md - docs/CONTEXT.md","title":"19. Change Management"},{"location":"FP8/","text":"Apex-X FP8 Precision Policy Scope This document defines how Apex-X enables FP8 for heavy compute ops and how it falls back safely. Authoritative references: - docs/PRD.md (FR-12) - docs/ENGINEERING_SPEC.md (Section 13) Policy Summary Heavy ops are FP8-eligible only when support is detected. Router and KAN-like paths stay in FP16. If FP8 is requested but not supported, fallback is FP16. Request Rules FP8 is requested when either is true: - runtime.precision_profile == \"balanced\" - train.qat_fp8 == true Support Detection Implemented in apex_x/runtime/precision.py : - device must be CUDA - torch build must expose FP8 dtype ( torch.float8_e4m3fn ) - CUDA capability gate is conservative ( sm90+ ) If any check fails, policy falls back to FP16. Effective Dtypes Policy object ( PrecisionPolicy ) exposes: - heavy_ops_dtype - router_dtype (always FP16) - kan_dtype (always FP16) - fp8_requested - fp8_enabled - fallback_reason Trainer Integration ApexXTrainer resolves precision policy at init and reports it in: - train_summary[\"precision\"] For heavy-op execution context: - FP16 path uses autocast where safe. - FP8 path is marked as ready and left for specialized kernel/plugin integration. Fallback Behavior (Expected on CPU) On CPU runs, balanced profile requests FP8 but falls back to FP16: - fp8_requested = true - fp8_enabled = false - fallback_reason = \"fp8_requires_cuda\" Validation Covered by tests/test_precision_policy.py : - CPU fallback smoke - mocked supported CUDA FP8 path - trainer summary precision diagnostics","title":"FP8 Policy"},{"location":"FP8/#apex-x-fp8-precision-policy","text":"","title":"Apex-X FP8 Precision Policy"},{"location":"FP8/#scope","text":"This document defines how Apex-X enables FP8 for heavy compute ops and how it falls back safely. Authoritative references: - docs/PRD.md (FR-12) - docs/ENGINEERING_SPEC.md (Section 13)","title":"Scope"},{"location":"FP8/#policy-summary","text":"Heavy ops are FP8-eligible only when support is detected. Router and KAN-like paths stay in FP16. If FP8 is requested but not supported, fallback is FP16.","title":"Policy Summary"},{"location":"FP8/#request-rules","text":"FP8 is requested when either is true: - runtime.precision_profile == \"balanced\" - train.qat_fp8 == true","title":"Request Rules"},{"location":"FP8/#support-detection","text":"Implemented in apex_x/runtime/precision.py : - device must be CUDA - torch build must expose FP8 dtype ( torch.float8_e4m3fn ) - CUDA capability gate is conservative ( sm90+ ) If any check fails, policy falls back to FP16.","title":"Support Detection"},{"location":"FP8/#effective-dtypes","text":"Policy object ( PrecisionPolicy ) exposes: - heavy_ops_dtype - router_dtype (always FP16) - kan_dtype (always FP16) - fp8_requested - fp8_enabled - fallback_reason","title":"Effective Dtypes"},{"location":"FP8/#trainer-integration","text":"ApexXTrainer resolves precision policy at init and reports it in: - train_summary[\"precision\"] For heavy-op execution context: - FP16 path uses autocast where safe. - FP8 path is marked as ready and left for specialized kernel/plugin integration.","title":"Trainer Integration"},{"location":"FP8/#fallback-behavior-expected-on-cpu","text":"On CPU runs, balanced profile requests FP8 but falls back to FP16: - fp8_requested = true - fp8_enabled = false - fallback_reason = \"fp8_requires_cuda\"","title":"Fallback Behavior (Expected on CPU)"},{"location":"FP8/#validation","text":"Covered by tests/test_precision_policy.py : - CPU fallback smoke - mocked supported CUDA FP8 path - trainer summary precision diagnostics","title":"Validation"},{"location":"PERF/","text":"Apex-X Performance Regression Suite Scope This document defines the CPU performance regression suite used in CI. Benchmarks The suite lives in: - apex_x/bench/perf.py - scripts/perf_regression.py It includes: 1. Fixed-size infer() benchmark (CPU baseline) - model: ApexXModel - input: [1,3,128,128] - metrics: infer_p50_ms , infer_p95_ms 2. Microbenchmarks (CPU) - TilePackTorch - TileUnpackTorch - FusionGate - metrics: tile_pack_p50_ms , tile_unpack_p50_ms , fusion_gate_p50_ms Baseline and Tolerances Committed baseline file: - scripts/perf_baseline_cpu.json Each metric has: - value_ms - max_regression_ratio - max_regression_abs_ms Regression check rule: - fail when current_ms > value_ms * (1 + ratio) + abs_ms Local Usage Run suite only: python scripts/perf_regression.py --output artifacts/perf_current.json Run and compare with baseline: python scripts/perf_regression.py \\ --compare \\ --baseline scripts/perf_baseline_cpu.json \\ --output artifacts/perf_current.json \\ --summary artifacts/perf_compare.json Regenerate baseline template from current machine: python scripts/perf_regression.py \\ --emit-baseline-template \\ --baseline scripts/perf_baseline_cpu.json CI Workflow job perf-regression runs on ubuntu-latest (CPU-only): - runs scripts/perf_regression.py --compare - writes: - artifacts/perf_current_ci.json - artifacts/perf_compare_ci.json - fails CI when status is fail Notes This suite is intentionally CPU-only and stable-friendly. Tolerances are set to avoid flakiness from CI VM noise while still catching major regressions.","title":"Performance Regression"},{"location":"PERF/#apex-x-performance-regression-suite","text":"","title":"Apex-X Performance Regression Suite"},{"location":"PERF/#scope","text":"This document defines the CPU performance regression suite used in CI.","title":"Scope"},{"location":"PERF/#benchmarks","text":"The suite lives in: - apex_x/bench/perf.py - scripts/perf_regression.py It includes: 1. Fixed-size infer() benchmark (CPU baseline) - model: ApexXModel - input: [1,3,128,128] - metrics: infer_p50_ms , infer_p95_ms 2. Microbenchmarks (CPU) - TilePackTorch - TileUnpackTorch - FusionGate - metrics: tile_pack_p50_ms , tile_unpack_p50_ms , fusion_gate_p50_ms","title":"Benchmarks"},{"location":"PERF/#baseline-and-tolerances","text":"Committed baseline file: - scripts/perf_baseline_cpu.json Each metric has: - value_ms - max_regression_ratio - max_regression_abs_ms Regression check rule: - fail when current_ms > value_ms * (1 + ratio) + abs_ms","title":"Baseline and Tolerances"},{"location":"PERF/#local-usage","text":"Run suite only: python scripts/perf_regression.py --output artifacts/perf_current.json Run and compare with baseline: python scripts/perf_regression.py \\ --compare \\ --baseline scripts/perf_baseline_cpu.json \\ --output artifacts/perf_current.json \\ --summary artifacts/perf_compare.json Regenerate baseline template from current machine: python scripts/perf_regression.py \\ --emit-baseline-template \\ --baseline scripts/perf_baseline_cpu.json","title":"Local Usage"},{"location":"PERF/#ci","text":"Workflow job perf-regression runs on ubuntu-latest (CPU-only): - runs scripts/perf_regression.py --compare - writes: - artifacts/perf_current_ci.json - artifacts/perf_compare_ci.json - fails CI when status is fail","title":"CI"},{"location":"PERF/#notes","text":"This suite is intentionally CPU-only and stable-friendly. Tolerances are set to avoid flakiness from CI VM noise while still catching major regressions.","title":"Notes"},{"location":"PRD/","text":"Apex-X v4 PRD 1. Document Status Project: Apex-X v4 Purpose: Authoritative product requirements for the Apex-X open-source repository This document is normative for scope, behavior, and acceptance criteria Implementation details are specified in docs/ENGINEERING_SPEC.md 2. Product Goal Apex-X v4 is a dual-stream, utility-routed vision architecture that preserves quality while enforcing explicit compute budgets. Primary outcomes: - high-quality detection and segmentation under fixed latency/compute budgets - deterministic inference behavior suitable for production runtime plugins - clean export path to TensorRT/ORT with dynamic selection handled via Kmax buffers and plugins 3. Scope 3.1 In Scope Tasks: DET, INST-SEG, SEM-SEG, optional POSE/TRACK/PANO Dual stream architecture: PV (always-on) + FF (sparse high-res tiles) Utility-based routing with oracle supervision Continuous-budget training and deterministic-budget inference Quadtree nesting ( L0/L1/L2 ) with split policy Tile graph pack/unpack contracts and deterministic ordering Temporal hysteresis for anti-flicker Runtime plugin contracts for TensorRT/ORT QAT policy for INT8/FP8 deployment paths 3.2 Out of Scope (for baseline release) Final production GPU kernels Multi-node distributed training recipes End-to-end tracking benchmarks 4. Terms PV : Peripheral Vision stream (low-res, dense) FF : Foveal Focus stream (high-res, sparse tiles) L0/L1/L2 : tile nesting levels ( L0 coarse, L1/L2 finer) Kmax : fixed upper bound on active tile buffer length B, B1, B2 : total and per-level budgets U_i : utility score for tile i S_i : split utility for tile i O_split : split overhead cost term 5. Functional Requirements FR-1 Dual Stream System shall compute PV densely and FF sparsely. FR-2 Utility Router System shall score tiles with utility U_i and optional split utility S_i . FR-3 Oracle Supervision System shall support oracle delta utility labels: [ \\Delta_i = L_{distill}(y^{(i=0)}, y_T) - L_{distill}(y^{(i=1)}, y_T) ] where i=0 means heavy path disabled on tile i , i=1 means enabled. FR-4 Continuous Budgeting (Training) System shall optimize routing probabilities via: [ p_i = \\sigma(U_i),\\quad g_i = STE(p_i) ] Expected cost: [ \\mathbb{E}[C] = \\sum_i \\left(p_i C_h + (1-p_i) C_c\\right) ] Constrained objective: [ L = L_{main} + \\mu(\\mathbb{E}[C] - B),\\quad \\mu \\ge 0 ] Dual update concept: - if E[C] > B , increase mu - if E[C] < B , decrease mu FR-5 Deterministic Inference Budgeting System shall select FF tiles deterministically by utility-per-cost: [ score_i = \\frac{U_i}{\\Delta C_i} ] sorted descending with selection until budget or Kmax is reached. FR-6 Quadtree Nesting System shall support two-level and optional three-level nesting: - L0 under budget B1 - L1/L2 split under budget B2 Split score: [ score^{split} i = \\frac{S_i}{O {split}} ] FR-7 Tile Tensor Contracts System shall support deterministic tile gather/scatter: - Pack: [B,C,Hf,Wf] + idx[B,K] -> P[B,K,C,t,t] + meta - Unpack: F_base + P_out + meta -> F_merged FR-8 Deterministic Tile Ordering System shall support: - Hilbert ordering - Multi-direction scan ordering ( LR , RL , UD , DU ) FR-9 Tile-SSM and Fusion Gate System shall include a tile-sequence mixer (Mamba-like placeholder acceptable in baseline) and fusion gate behavior. FR-10 Temporal Stability System shall support hysteresis: [ z_i(t)=\\mathbf{1}[U_i(t)>\\theta_{on}\\;\\lor\\;(U_i(t)>\\theta_{off}\\land z_i(t-1)=1)] ] with theta_on > theta_off . FR-11 Runtime Plugin Contracts System shall define plugin interfaces for TensorRT/ORT: - TilePack - TileSSMScan - TileUnpackFusion - optional MaskedConv - fused DecodeNMS FR-12 QAT/Precision Policy System shall define deployment profiles: - Quality: FP16-heavy - Balanced: FP16/FP8 mixed - Edge: INT8 (router FP16) INT4 allowed only with guaranteed kernels and quality gate. FR-13 Perf Regression Requirements System shall include repeatable latency and memory regressions with thresholds and fail criteria. 6. Non-Functional Requirements Determinism: same input + same config -> same active tile set Exportability: avoid Python-side runtime control flow CPU baseline runnable from clean environment Testability: correctness, stability, performance checks 7. Pseudo-Code Requirements 7.1 Utility Oracle Labeling for each minibatch: sample oracle tile subset S (random + high teacher uncertainty) for i in S: y0 = forward_with_tile(i, enabled=0) y1 = forward_with_tile(i, enabled=1) Delta_i = L_distill(y0, y_teacher) - L_distill(y1, y_teacher) train router utility head to predict Delta_i (L1 or ranking loss) 7.2 Continuous Budget Training U = router(features) p = sigmoid(U) g = STE(p) y = model_forward_with_gate(g) L_main = task_losses(y, target) E_cost = sum_i(p_i * C_h + (1-p_i) * C_c) L = L_main + mu * (E_cost - B) backprop(L) mu = max(0, mu + eta_mu * (E_cost - B)) 7.3 Deterministic Inference Budgeting U = router(features) for each tile i: score_i = U_i / DeltaC_i order = argsort(score, descending=True) active = [] spent = 0 for i in order: if len(active) == Kmax: break if spent + DeltaC_i <= B: active.append(i) spent += DeltaC_i return active 7.4 Quadtree L0/L1/L2 Policy L0 = select_by_budget(U_L0, cost_L0, B1, Kmax_L0) candidates = [] for i in L0: split_score_i = S_i / O_split_i candidates.append((i, split_score_i)) L1 = top_by_budget(candidates, B2, Kmax_L1) if nesting_depth == 2: L2 = repeat_split_for_L1_under_B3 7.5 TilePack/TileUnpack and Ordering idx_ordered = order_idx(idx, mode=hilbert|multi_direction) P, meta = TilePack(F, idx_ordered, t) P_mix = TileSSM_and_local_refine(P) F_merged = TileUnpackFusion(F_base, P_mix, meta, gate) 8. Architecture Requirements PV stream: always-dense low-res features FF stream: sparse high-res packed tiles FPN split: low-res dense path + high-res sparse path Tile-SSM + local refinement in FF heavy blocks DET/SEG heads consume merged features 9. Loss Requirements DET: focal/quality focal + IoU-family box loss (optional DFL) INST-SEG: BCE + Dice + boundary loss Multi-task conflict handling: PCGrad++ on shared trunk only PCGrad projection when cosine similarity < 0: [ g_a \\leftarrow g_a - \\frac{g_a \\cdot g_b}{\\lVert g_b \\rVert^2} g_b ] 10. Acceptance Criteria Release is accepted only if all are true: - deterministic tile selection and ordering pass tests - pack/unpack semantics pass identity and overlap tests - training budget control converges around target budget - CPU baseline runs via examples/run_cpu_baseline.py - CI passes lint and tests - runtime plugin contracts and precision profiles are documented 11. Dependencies and Interfaces reference implementation: Python + NumPy (CPU-only baseline) runtime targets: TensorRT and ONNX Runtime via plugin contracts 12. Deliverables in This Repository Source: apex_x/ Tests: tests/ Authoritative docs: docs/PRD.md , docs/ENGINEERING_SPEC.md Runtime docs: docs/runtime/PLUGIN_SPEC.md Project memory: docs/CONTEXT.md 13. Change Control Any architectural change to routing, budgets, tile contracts, ordering, plugin behavior, or precision policy must update: - docs/PRD.md - docs/ENGINEERING_SPEC.md - docs/DECISIONS.md - docs/CONTEXT.md","title":"PRD"},{"location":"PRD/#apex-x-v4-prd","text":"","title":"Apex-X v4 PRD"},{"location":"PRD/#1-document-status","text":"Project: Apex-X v4 Purpose: Authoritative product requirements for the Apex-X open-source repository This document is normative for scope, behavior, and acceptance criteria Implementation details are specified in docs/ENGINEERING_SPEC.md","title":"1. Document Status"},{"location":"PRD/#2-product-goal","text":"Apex-X v4 is a dual-stream, utility-routed vision architecture that preserves quality while enforcing explicit compute budgets. Primary outcomes: - high-quality detection and segmentation under fixed latency/compute budgets - deterministic inference behavior suitable for production runtime plugins - clean export path to TensorRT/ORT with dynamic selection handled via Kmax buffers and plugins","title":"2. Product Goal"},{"location":"PRD/#3-scope","text":"","title":"3. Scope"},{"location":"PRD/#31-in-scope","text":"Tasks: DET, INST-SEG, SEM-SEG, optional POSE/TRACK/PANO Dual stream architecture: PV (always-on) + FF (sparse high-res tiles) Utility-based routing with oracle supervision Continuous-budget training and deterministic-budget inference Quadtree nesting ( L0/L1/L2 ) with split policy Tile graph pack/unpack contracts and deterministic ordering Temporal hysteresis for anti-flicker Runtime plugin contracts for TensorRT/ORT QAT policy for INT8/FP8 deployment paths","title":"3.1 In Scope"},{"location":"PRD/#32-out-of-scope-for-baseline-release","text":"Final production GPU kernels Multi-node distributed training recipes End-to-end tracking benchmarks","title":"3.2 Out of Scope (for baseline release)"},{"location":"PRD/#4-terms","text":"PV : Peripheral Vision stream (low-res, dense) FF : Foveal Focus stream (high-res, sparse tiles) L0/L1/L2 : tile nesting levels ( L0 coarse, L1/L2 finer) Kmax : fixed upper bound on active tile buffer length B, B1, B2 : total and per-level budgets U_i : utility score for tile i S_i : split utility for tile i O_split : split overhead cost term","title":"4. Terms"},{"location":"PRD/#5-functional-requirements","text":"","title":"5. Functional Requirements"},{"location":"PRD/#fr-1-dual-stream","text":"System shall compute PV densely and FF sparsely.","title":"FR-1 Dual Stream"},{"location":"PRD/#fr-2-utility-router","text":"System shall score tiles with utility U_i and optional split utility S_i .","title":"FR-2 Utility Router"},{"location":"PRD/#fr-3-oracle-supervision","text":"System shall support oracle delta utility labels: [ \\Delta_i = L_{distill}(y^{(i=0)}, y_T) - L_{distill}(y^{(i=1)}, y_T) ] where i=0 means heavy path disabled on tile i , i=1 means enabled.","title":"FR-3 Oracle Supervision"},{"location":"PRD/#fr-4-continuous-budgeting-training","text":"System shall optimize routing probabilities via: [ p_i = \\sigma(U_i),\\quad g_i = STE(p_i) ] Expected cost: [ \\mathbb{E}[C] = \\sum_i \\left(p_i C_h + (1-p_i) C_c\\right) ] Constrained objective: [ L = L_{main} + \\mu(\\mathbb{E}[C] - B),\\quad \\mu \\ge 0 ] Dual update concept: - if E[C] > B , increase mu - if E[C] < B , decrease mu","title":"FR-4 Continuous Budgeting (Training)"},{"location":"PRD/#fr-5-deterministic-inference-budgeting","text":"System shall select FF tiles deterministically by utility-per-cost: [ score_i = \\frac{U_i}{\\Delta C_i} ] sorted descending with selection until budget or Kmax is reached.","title":"FR-5 Deterministic Inference Budgeting"},{"location":"PRD/#fr-6-quadtree-nesting","text":"System shall support two-level and optional three-level nesting: - L0 under budget B1 - L1/L2 split under budget B2 Split score: [ score^{split} i = \\frac{S_i}{O {split}} ]","title":"FR-6 Quadtree Nesting"},{"location":"PRD/#fr-7-tile-tensor-contracts","text":"System shall support deterministic tile gather/scatter: - Pack: [B,C,Hf,Wf] + idx[B,K] -> P[B,K,C,t,t] + meta - Unpack: F_base + P_out + meta -> F_merged","title":"FR-7 Tile Tensor Contracts"},{"location":"PRD/#fr-8-deterministic-tile-ordering","text":"System shall support: - Hilbert ordering - Multi-direction scan ordering ( LR , RL , UD , DU )","title":"FR-8 Deterministic Tile Ordering"},{"location":"PRD/#fr-9-tile-ssm-and-fusion-gate","text":"System shall include a tile-sequence mixer (Mamba-like placeholder acceptable in baseline) and fusion gate behavior.","title":"FR-9 Tile-SSM and Fusion Gate"},{"location":"PRD/#fr-10-temporal-stability","text":"System shall support hysteresis: [ z_i(t)=\\mathbf{1}[U_i(t)>\\theta_{on}\\;\\lor\\;(U_i(t)>\\theta_{off}\\land z_i(t-1)=1)] ] with theta_on > theta_off .","title":"FR-10 Temporal Stability"},{"location":"PRD/#fr-11-runtime-plugin-contracts","text":"System shall define plugin interfaces for TensorRT/ORT: - TilePack - TileSSMScan - TileUnpackFusion - optional MaskedConv - fused DecodeNMS","title":"FR-11 Runtime Plugin Contracts"},{"location":"PRD/#fr-12-qatprecision-policy","text":"System shall define deployment profiles: - Quality: FP16-heavy - Balanced: FP16/FP8 mixed - Edge: INT8 (router FP16) INT4 allowed only with guaranteed kernels and quality gate.","title":"FR-12 QAT/Precision Policy"},{"location":"PRD/#fr-13-perf-regression-requirements","text":"System shall include repeatable latency and memory regressions with thresholds and fail criteria.","title":"FR-13 Perf Regression Requirements"},{"location":"PRD/#6-non-functional-requirements","text":"Determinism: same input + same config -> same active tile set Exportability: avoid Python-side runtime control flow CPU baseline runnable from clean environment Testability: correctness, stability, performance checks","title":"6. Non-Functional Requirements"},{"location":"PRD/#7-pseudo-code-requirements","text":"","title":"7. Pseudo-Code Requirements"},{"location":"PRD/#71-utility-oracle-labeling","text":"for each minibatch: sample oracle tile subset S (random + high teacher uncertainty) for i in S: y0 = forward_with_tile(i, enabled=0) y1 = forward_with_tile(i, enabled=1) Delta_i = L_distill(y0, y_teacher) - L_distill(y1, y_teacher) train router utility head to predict Delta_i (L1 or ranking loss)","title":"7.1 Utility Oracle Labeling"},{"location":"PRD/#72-continuous-budget-training","text":"U = router(features) p = sigmoid(U) g = STE(p) y = model_forward_with_gate(g) L_main = task_losses(y, target) E_cost = sum_i(p_i * C_h + (1-p_i) * C_c) L = L_main + mu * (E_cost - B) backprop(L) mu = max(0, mu + eta_mu * (E_cost - B))","title":"7.2 Continuous Budget Training"},{"location":"PRD/#73-deterministic-inference-budgeting","text":"U = router(features) for each tile i: score_i = U_i / DeltaC_i order = argsort(score, descending=True) active = [] spent = 0 for i in order: if len(active) == Kmax: break if spent + DeltaC_i <= B: active.append(i) spent += DeltaC_i return active","title":"7.3 Deterministic Inference Budgeting"},{"location":"PRD/#74-quadtree-l0l1l2-policy","text":"L0 = select_by_budget(U_L0, cost_L0, B1, Kmax_L0) candidates = [] for i in L0: split_score_i = S_i / O_split_i candidates.append((i, split_score_i)) L1 = top_by_budget(candidates, B2, Kmax_L1) if nesting_depth == 2: L2 = repeat_split_for_L1_under_B3","title":"7.4 Quadtree L0/L1/L2 Policy"},{"location":"PRD/#75-tilepacktileunpack-and-ordering","text":"idx_ordered = order_idx(idx, mode=hilbert|multi_direction) P, meta = TilePack(F, idx_ordered, t) P_mix = TileSSM_and_local_refine(P) F_merged = TileUnpackFusion(F_base, P_mix, meta, gate)","title":"7.5 TilePack/TileUnpack and Ordering"},{"location":"PRD/#8-architecture-requirements","text":"PV stream: always-dense low-res features FF stream: sparse high-res packed tiles FPN split: low-res dense path + high-res sparse path Tile-SSM + local refinement in FF heavy blocks DET/SEG heads consume merged features","title":"8. Architecture Requirements"},{"location":"PRD/#9-loss-requirements","text":"DET: focal/quality focal + IoU-family box loss (optional DFL) INST-SEG: BCE + Dice + boundary loss Multi-task conflict handling: PCGrad++ on shared trunk only PCGrad projection when cosine similarity < 0: [ g_a \\leftarrow g_a - \\frac{g_a \\cdot g_b}{\\lVert g_b \\rVert^2} g_b ]","title":"9. Loss Requirements"},{"location":"PRD/#10-acceptance-criteria","text":"Release is accepted only if all are true: - deterministic tile selection and ordering pass tests - pack/unpack semantics pass identity and overlap tests - training budget control converges around target budget - CPU baseline runs via examples/run_cpu_baseline.py - CI passes lint and tests - runtime plugin contracts and precision profiles are documented","title":"10. Acceptance Criteria"},{"location":"PRD/#11-dependencies-and-interfaces","text":"reference implementation: Python + NumPy (CPU-only baseline) runtime targets: TensorRT and ONNX Runtime via plugin contracts","title":"11. Dependencies and Interfaces"},{"location":"PRD/#12-deliverables-in-this-repository","text":"Source: apex_x/ Tests: tests/ Authoritative docs: docs/PRD.md , docs/ENGINEERING_SPEC.md Runtime docs: docs/runtime/PLUGIN_SPEC.md Project memory: docs/CONTEXT.md","title":"12. Deliverables in This Repository"},{"location":"PRD/#13-change-control","text":"Any architectural change to routing, budgets, tile contracts, ordering, plugin behavior, or precision policy must update: - docs/PRD.md - docs/ENGINEERING_SPEC.md - docs/DECISIONS.md - docs/CONTEXT.md","title":"13. Change Control"},{"location":"QAT/","text":"Apex-X INT8 QAT Policy Scope This document defines the CPU-reference INT8 quantization-aware training (QAT) path and PTQ fallback used in Apex-X. Authoritative references: - docs/PRD.md (FR-12) - docs/ENGINEERING_SPEC.md (Section 13) Design Goals Provide a deterministic INT8 simulation path during training. Keep router/gating logic in FP16-safe precision path. Support PTQ fallback for edge precision profile when QAT is not enabled. Keep CPU baseline runnable and testable. Implemented Components Fake Quant Modules Implemented in apex_x/train/qat.py : - ActivationObserver : running min/max activation statistics. - ActivationFakeQuant : per-tensor activation fake quant. - WeightPerChannelFakeQuant : symmetric per-channel weight fake quant. - FakeQuantConv2d : wraps nn.Conv2d . - FakeQuantLinear : wraps nn.Linear . Quantization ranges: - Activations: unsigned INT8 ( 0..255 ), affine. - Weights: signed INT8 ( -127..127 ), symmetric per output channel. QAT Preparation prepare_int8_qat(model, ...) : - Replaces quantizable Conv2d / Linear modules with fake-quant wrappers. - Enables observers + fake quantization for training-time simulation. - Skips modules whose names include router , gate , or gating . PTQ Calibration Fallback prepare_int8_ptq(model, calibration_inputs, forward_fn, ...) : - Applies same wrapper conversion. - Runs observer-only calibration passes ( fake_quant=off ). - Freezes observers and enables fake quant ( observer=off , fake_quant=on ). This is used as fallback for runtime profile edge when INT8 QAT is not explicitly enabled. Trainer Integration apex_x/train/trainer.py integrates quantization policy: - If train.qat_enable=true and train.qat_int8=true : - prepare INT8 QAT ( mode=qat_int8 ) - Else if runtime.precision_profile=edge : - run PTQ calibration fallback ( mode=ptq_int8 ) - Else: - quantization disabled ( mode=disabled ) Trainer train_summary[\"quantization\"] includes: - mode - wrapped_modules - calibration_batches - router_gating_fp16 Router/Gating Precision Rule Router and gating paths are excluded from INT8 wrapping by policy: - skip-name filters include router , gate , gating - staged gating math keeps FP16 path for router utility gating operations This enforces the spec requirement that routing-sensitive numerics remain in higher precision. Validation Coverage is provided in tests/test_qat.py : - wrapper conversion and skip policy checks - PTQ calibration state transitions - trainer-level QAT/PTQ toggles and finite outputs","title":"QAT Policy"},{"location":"QAT/#apex-x-int8-qat-policy","text":"","title":"Apex-X INT8 QAT Policy"},{"location":"QAT/#scope","text":"This document defines the CPU-reference INT8 quantization-aware training (QAT) path and PTQ fallback used in Apex-X. Authoritative references: - docs/PRD.md (FR-12) - docs/ENGINEERING_SPEC.md (Section 13)","title":"Scope"},{"location":"QAT/#design-goals","text":"Provide a deterministic INT8 simulation path during training. Keep router/gating logic in FP16-safe precision path. Support PTQ fallback for edge precision profile when QAT is not enabled. Keep CPU baseline runnable and testable.","title":"Design Goals"},{"location":"QAT/#implemented-components","text":"","title":"Implemented Components"},{"location":"QAT/#fake-quant-modules","text":"Implemented in apex_x/train/qat.py : - ActivationObserver : running min/max activation statistics. - ActivationFakeQuant : per-tensor activation fake quant. - WeightPerChannelFakeQuant : symmetric per-channel weight fake quant. - FakeQuantConv2d : wraps nn.Conv2d . - FakeQuantLinear : wraps nn.Linear . Quantization ranges: - Activations: unsigned INT8 ( 0..255 ), affine. - Weights: signed INT8 ( -127..127 ), symmetric per output channel.","title":"Fake Quant Modules"},{"location":"QAT/#qat-preparation","text":"prepare_int8_qat(model, ...) : - Replaces quantizable Conv2d / Linear modules with fake-quant wrappers. - Enables observers + fake quantization for training-time simulation. - Skips modules whose names include router , gate , or gating .","title":"QAT Preparation"},{"location":"QAT/#ptq-calibration-fallback","text":"prepare_int8_ptq(model, calibration_inputs, forward_fn, ...) : - Applies same wrapper conversion. - Runs observer-only calibration passes ( fake_quant=off ). - Freezes observers and enables fake quant ( observer=off , fake_quant=on ). This is used as fallback for runtime profile edge when INT8 QAT is not explicitly enabled.","title":"PTQ Calibration Fallback"},{"location":"QAT/#trainer-integration","text":"apex_x/train/trainer.py integrates quantization policy: - If train.qat_enable=true and train.qat_int8=true : - prepare INT8 QAT ( mode=qat_int8 ) - Else if runtime.precision_profile=edge : - run PTQ calibration fallback ( mode=ptq_int8 ) - Else: - quantization disabled ( mode=disabled ) Trainer train_summary[\"quantization\"] includes: - mode - wrapped_modules - calibration_batches - router_gating_fp16","title":"Trainer Integration"},{"location":"QAT/#routergating-precision-rule","text":"Router and gating paths are excluded from INT8 wrapping by policy: - skip-name filters include router , gate , gating - staged gating math keeps FP16 path for router utility gating operations This enforces the spec requirement that routing-sensitive numerics remain in higher precision.","title":"Router/Gating Precision Rule"},{"location":"QAT/#validation","text":"Coverage is provided in tests/test_qat.py : - wrapper conversion and skip policy checks - PTQ calibration state transitions - trainer-level QAT/PTQ toggles and finite outputs","title":"Validation"},{"location":"TODO/","text":"Apex-X TODO Near-Term Implement full DET head outputs on multi-scale features ( P3..P7 ) in CPU baseline API. Implement instance segmentation prototype assembly + tile boundary refine path. Add quadtree split selection implementation ( B1/B2 , S_i/O_split ) for L1/L2 . Add temporal state object and hysteresis unit tests with sequence-level assertions. Runtime and Export Add ONNX export contract checks for fixed Kmax and tile-size constants. Add plugin I/O compliance test vectors for TilePack , TileSSMScan , TileUnpackFusion . Add TensorRT/ORT parity harness skeleton. Implement full Triton fused kernels for: pack + pre-norm gather path tile scan / Tile-SSM streaming path unpack + gated fusion scatter path Implement full TensorRT plugin stack (non-placeholder): TilePack TileSSMScan TileUnpackFusion fused decode + NMS path Implement ONNX Runtime custom-op adapter path for sparse tile execution. Add profile-based runtime selection ( Quality/Balanced/Edge ) with strict regression gates. Add end-to-end TRT/ORT vs reference parity tests (accuracy + deterministic tile-set parity). Training System Add utility-oracle sampling utilities and distillation delta label pipeline. Add continuous-budget trainer loop with dual variable scheduling. Add PCGrad++ projection hook for shared trunk parameters. Quality Gates Add regression thresholds file for p50/p95 latency and memory. Add repeatability tests for deterministic ordering modes. Add stricter CI checks for docs-contract drift.","title":"TODO"},{"location":"TODO/#apex-x-todo","text":"","title":"Apex-X TODO"},{"location":"TODO/#near-term","text":"Implement full DET head outputs on multi-scale features ( P3..P7 ) in CPU baseline API. Implement instance segmentation prototype assembly + tile boundary refine path. Add quadtree split selection implementation ( B1/B2 , S_i/O_split ) for L1/L2 . Add temporal state object and hysteresis unit tests with sequence-level assertions.","title":"Near-Term"},{"location":"TODO/#runtime-and-export","text":"Add ONNX export contract checks for fixed Kmax and tile-size constants. Add plugin I/O compliance test vectors for TilePack , TileSSMScan , TileUnpackFusion . Add TensorRT/ORT parity harness skeleton. Implement full Triton fused kernels for: pack + pre-norm gather path tile scan / Tile-SSM streaming path unpack + gated fusion scatter path Implement full TensorRT plugin stack (non-placeholder): TilePack TileSSMScan TileUnpackFusion fused decode + NMS path Implement ONNX Runtime custom-op adapter path for sparse tile execution. Add profile-based runtime selection ( Quality/Balanced/Edge ) with strict regression gates. Add end-to-end TRT/ORT vs reference parity tests (accuracy + deterministic tile-set parity).","title":"Runtime and Export"},{"location":"TODO/#training-system","text":"Add utility-oracle sampling utilities and distillation delta label pipeline. Add continuous-budget trainer loop with dual variable scheduling. Add PCGrad++ projection hook for shared trunk parameters.","title":"Training System"},{"location":"TODO/#quality-gates","text":"Add regression thresholds file for p50/p95 latency and memory. Add repeatability tests for deterministic ordering modes. Add stricter CI checks for docs-contract drift.","title":"Quality Gates"},{"location":"runtime/CAPS/","text":"Runtime Capability Detection Scope apex_x/runtime/caps.py provides a unified capability probe for runtime decisions. Main API: - detect_runtime_caps(...) -> RuntimeCaps - RuntimeCaps.to_dict() What Is Detected CUDA availability ( torch.cuda.is_available() ) device count active device name compute capability ( major.minor ) Triton module presence ( triton ) version (package metadata or module __version__ ) TensorRT Python module availability ( tensorrt ) Python package version (if available) C++ headers presence ( NvInfer.h or NvInferRuntime.h ) checks optional explicit search paths checks environment hints ( TENSORRT_INCLUDE_DIR , TRT_INCLUDE_DIR , TENSORRT_ROOT , etc.) checks common include directories INT8 availability for TensorRT usage requires TensorRT Python module requires CUDA availability requires tensorrt.BuilderFlag.INT8 FP8 FP8 dtype presence in torch build ( float8_e4m3fn , float8_e5m2 ) CUDA + compute capability gate ( sm90+ required in this baseline policy) Usage from apex_x.runtime import detect_runtime_caps caps = detect_runtime_caps() print(caps.to_dict()) With explicit TensorRT header probe path: caps = detect_runtime_caps(header_search_paths=[\"/usr/local/TensorRT/include\"]) Fallback Contract Missing optional runtimes never raise by default. Capability object always returns with explicit reason fields. CPU-only environments return: cuda.available = False triton.available = False tensorrt.int8_available = False fp8.available = False Test Strategy Tests in tests/test_caps_runtime.py and tests/test_caps_tensorrt_fp8.py Designed to pass on CPU-only machines using mocks for: CUDA responses Triton/TensorRT module discovery FP8 dtype support checks","title":"Runtime Capability Detection"},{"location":"runtime/CAPS/#runtime-capability-detection","text":"","title":"Runtime Capability Detection"},{"location":"runtime/CAPS/#scope","text":"apex_x/runtime/caps.py provides a unified capability probe for runtime decisions. Main API: - detect_runtime_caps(...) -> RuntimeCaps - RuntimeCaps.to_dict()","title":"Scope"},{"location":"runtime/CAPS/#what-is-detected","text":"","title":"What Is Detected"},{"location":"runtime/CAPS/#cuda","text":"availability ( torch.cuda.is_available() ) device count active device name compute capability ( major.minor )","title":"CUDA"},{"location":"runtime/CAPS/#triton","text":"module presence ( triton ) version (package metadata or module __version__ )","title":"Triton"},{"location":"runtime/CAPS/#tensorrt","text":"Python module availability ( tensorrt ) Python package version (if available) C++ headers presence ( NvInfer.h or NvInferRuntime.h ) checks optional explicit search paths checks environment hints ( TENSORRT_INCLUDE_DIR , TRT_INCLUDE_DIR , TENSORRT_ROOT , etc.) checks common include directories INT8 availability for TensorRT usage requires TensorRT Python module requires CUDA availability requires tensorrt.BuilderFlag.INT8","title":"TensorRT"},{"location":"runtime/CAPS/#fp8","text":"FP8 dtype presence in torch build ( float8_e4m3fn , float8_e5m2 ) CUDA + compute capability gate ( sm90+ required in this baseline policy)","title":"FP8"},{"location":"runtime/CAPS/#usage","text":"from apex_x.runtime import detect_runtime_caps caps = detect_runtime_caps() print(caps.to_dict()) With explicit TensorRT header probe path: caps = detect_runtime_caps(header_search_paths=[\"/usr/local/TensorRT/include\"])","title":"Usage"},{"location":"runtime/CAPS/#fallback-contract","text":"Missing optional runtimes never raise by default. Capability object always returns with explicit reason fields. CPU-only environments return: cuda.available = False triton.available = False tensorrt.int8_available = False fp8.available = False","title":"Fallback Contract"},{"location":"runtime/CAPS/#test-strategy","text":"Tests in tests/test_caps_runtime.py and tests/test_caps_tensorrt_fp8.py Designed to pass on CPU-only machines using mocks for: CUDA responses Triton/TensorRT module discovery FP8 dtype support checks","title":"Test Strategy"},{"location":"runtime/PARITY/","text":"Runtime Parity Framework Scope apex_x/runtime/parity.py provides a backend-agnostic parity harness for comparing: - PyTorch reference op (CPU/GPU) - Triton op (GPU) - TensorRT plugin op (GPU, integration-ready) The framework is lightweight and supports fast, small-shape checks for CI. Core API ParityCase run_parity_case(...) evaluate_parity_outputs(...) format_parity_report(...) ToleranceConfig Determinism run_parity_case(...) calls seed_all(seed, deterministic=...) before generating inputs. Recommended test settings: - fixed seed - deterministic=True - small static shapes for fast CI runs Tolerances ToleranceConfig supports per-precision tolerances: - default (fp32/other) - fp16 - bf16 - int8 Mismatch is computed with: - abs_err > atol + rtol * abs(reference) The report includes: - max_abs_err - mean_abs_err - max_rel_err - mean_rel_err - mismatch_count - total_count - mismatch_ratio Minimal Usage import torch from apex_x.runtime import ParityCase, run_parity_case case = ParityCase( name=\"tilepack-parity\", input_factory=lambda: torch.randn(1, 16, 8, 8), reference_fn=lambda x: x + 1.0, candidate_fn=lambda x: x + 1.0, reference_backend=\"pytorch_ref\", candidate_backend=\"triton\", ) report = run_parity_case(case, seed=123, deterministic=True) print(report.to_dict()) CI Notes Current CI can run CPU-safe parity unit tests only. GPU parity jobs (PyTorch vs Triton and later TensorRT) can reuse this API with: backend-specific candidate_fn profile-specific ToleranceConfig optional mismatch_ratio_limit for relaxed thresholds","title":"Runtime Parity Framework"},{"location":"runtime/PARITY/#runtime-parity-framework","text":"","title":"Runtime Parity Framework"},{"location":"runtime/PARITY/#scope","text":"apex_x/runtime/parity.py provides a backend-agnostic parity harness for comparing: - PyTorch reference op (CPU/GPU) - Triton op (GPU) - TensorRT plugin op (GPU, integration-ready) The framework is lightweight and supports fast, small-shape checks for CI.","title":"Scope"},{"location":"runtime/PARITY/#core-api","text":"ParityCase run_parity_case(...) evaluate_parity_outputs(...) format_parity_report(...) ToleranceConfig","title":"Core API"},{"location":"runtime/PARITY/#determinism","text":"run_parity_case(...) calls seed_all(seed, deterministic=...) before generating inputs. Recommended test settings: - fixed seed - deterministic=True - small static shapes for fast CI runs","title":"Determinism"},{"location":"runtime/PARITY/#tolerances","text":"ToleranceConfig supports per-precision tolerances: - default (fp32/other) - fp16 - bf16 - int8 Mismatch is computed with: - abs_err > atol + rtol * abs(reference) The report includes: - max_abs_err - mean_abs_err - max_rel_err - mean_rel_err - mismatch_count - total_count - mismatch_ratio","title":"Tolerances"},{"location":"runtime/PARITY/#minimal-usage","text":"import torch from apex_x.runtime import ParityCase, run_parity_case case = ParityCase( name=\"tilepack-parity\", input_factory=lambda: torch.randn(1, 16, 8, 8), reference_fn=lambda x: x + 1.0, candidate_fn=lambda x: x + 1.0, reference_backend=\"pytorch_ref\", candidate_backend=\"triton\", ) report = run_parity_case(case, seed=123, deterministic=True) print(report.to_dict())","title":"Minimal Usage"},{"location":"runtime/PARITY/#ci-notes","text":"Current CI can run CPU-safe parity unit tests only. GPU parity jobs (PyTorch vs Triton and later TensorRT) can reuse this API with: backend-specific candidate_fn profile-specific ToleranceConfig optional mismatch_ratio_limit for relaxed thresholds","title":"CI Notes"},{"location":"runtime/PLUGIN_SPEC/","text":"Runtime Plugin Specification (TensorRT / ORT) 1. Purpose Defines runtime plugin contracts for Apex-X v4 sparse tile execution. 2. TilePack Inputs: - feature tensor F[B,C,Hf,Wf] - indices idx[B,K] Outputs: - packed tensor P[B,K,C,t,t] - metadata meta Requirements: - deterministic index ordering support - FP16/FP8 tensors - no hidden reordering outside declared order_idx mode 3. TileSSMScan Inputs: - token sequence or packed tiles - optional recurrent state Outputs: - mixed output - next recurrent state Requirements: - streaming scan semantics - multi-direction execution mode - bounded temporary memory 4. TileUnpackFusion Inputs: - base map - packed outputs - metadata - optional gate map Outputs: - merged feature map Requirements: - supports residual and gating merge - deterministic overlap priority ( L2 > L1 > L0 by default) 5. Decode + NMS Provide fused decode and batched NMS path for DET head. 6. Precision Profiles Quality: FP16-heavy Balanced: FP16/FP8 Edge: INT8 (router and sensitive norms remain FP16) 7. Validation Each plugin release must pass: - shape and determinism tests - numerical parity checks vs reference CPU implementation - latency/memory regression gates","title":"Runtime Plugin Spec"},{"location":"runtime/PLUGIN_SPEC/#runtime-plugin-specification-tensorrt-ort","text":"","title":"Runtime Plugin Specification (TensorRT / ORT)"},{"location":"runtime/PLUGIN_SPEC/#1-purpose","text":"Defines runtime plugin contracts for Apex-X v4 sparse tile execution.","title":"1. Purpose"},{"location":"runtime/PLUGIN_SPEC/#2-tilepack","text":"Inputs: - feature tensor F[B,C,Hf,Wf] - indices idx[B,K] Outputs: - packed tensor P[B,K,C,t,t] - metadata meta Requirements: - deterministic index ordering support - FP16/FP8 tensors - no hidden reordering outside declared order_idx mode","title":"2. TilePack"},{"location":"runtime/PLUGIN_SPEC/#3-tilessmscan","text":"Inputs: - token sequence or packed tiles - optional recurrent state Outputs: - mixed output - next recurrent state Requirements: - streaming scan semantics - multi-direction execution mode - bounded temporary memory","title":"3. TileSSMScan"},{"location":"runtime/PLUGIN_SPEC/#4-tileunpackfusion","text":"Inputs: - base map - packed outputs - metadata - optional gate map Outputs: - merged feature map Requirements: - supports residual and gating merge - deterministic overlap priority ( L2 > L1 > L0 by default)","title":"4. TileUnpackFusion"},{"location":"runtime/PLUGIN_SPEC/#5-decode-nms","text":"Provide fused decode and batched NMS path for DET head.","title":"5. Decode + NMS"},{"location":"runtime/PLUGIN_SPEC/#6-precision-profiles","text":"Quality: FP16-heavy Balanced: FP16/FP8 Edge: INT8 (router and sensitive norms remain FP16)","title":"6. Precision Profiles"},{"location":"runtime/PLUGIN_SPEC/#7-validation","text":"Each plugin release must pass: - shape and determinism tests - numerical parity checks vs reference CPU implementation - latency/memory regression gates","title":"7. Validation"},{"location":"runtime/PLUGIN_SPECS/","text":"Runtime Plugin Specs This page is an alias for the canonical plugin contract document: docs/runtime/PLUGIN_SPEC.md Use PLUGIN_SPEC.md as the authoritative source.","title":"Runtime Plugin Specs Alias"},{"location":"runtime/PLUGIN_SPECS/#runtime-plugin-specs","text":"This page is an alias for the canonical plugin contract document: docs/runtime/PLUGIN_SPEC.md Use PLUGIN_SPEC.md as the authoritative source.","title":"Runtime Plugin Specs"},{"location":"runtime/TENSORRT/","text":"TensorRT Runtime Scaffolding Scope This document describes the C++ TensorRT plugin scaffolding under runtime/tensorrt/ . Detailed build and harness instructions: - docs/runtime/TENSORRT_BUILD.md Current status: - build system and plugin contracts are in place - TilePack now has a real TensorRT plugin implementation path (guarded) - remaining plugins are stubs - code is guarded to build on machines without TensorRT/CUDA installed Directory Layout runtime/tensorrt/CMakeLists.txt runtime/tensorrt/include/apexx_trt/ common.hpp plugin_stub.hpp tile_pack_plugin.hpp tile_ssm_scan_plugin.hpp tile_unpack_fusion_plugin.hpp decode_nms_plugin.hpp (optional) runtime/tensorrt/src/ common.cpp tile_pack_plugin.cpp tile_ssm_scan_plugin.cpp tile_unpack_fusion_plugin.cpp decode_nms_plugin.cpp plugin_info_main.cpp Contract Mapping Contracts are inherited from: - docs/runtime/PLUGIN_SPEC.md - docs/ENGINEERING_SPEC.md Mapped plugin stubs: - TilePack - contract: F[B,C,Hf,Wf] + idx[B,K] -> P[B,K,C,t,t] + meta - real TensorRT plugin path available under runtime/tensorrt/plugins/tilepack.* - TileSSMScan - contract: packed token/tile sequence + optional state -> mixed outputs + next state - TileUnpackFusion - contract: base + packed outputs + meta + gate -> merged features with overlap priority - DecodeNMS (optional) - contract: fused decode + NMS for DET path Build Guard Behavior CMakeLists.txt probes platform features and sets: - APEXX_ENABLE_TENSORRT to 1 only if TensorRT headers are found - APEXX_ENABLE_CUDA to 1 only if a CUDA compiler is available If unavailable, it builds a stub-only static library with the same API surface. Build Steps Use: - docs/runtime/TENSORRT_BUILD.md Output shared library (when TRT+CUDA available): apexx_trt_plugins static core library (always): apexx_trt_plugin_core utility executable: apexx_trt_plugin_info prints build summary and plugin availability flags harness executable (when shared library built): apexx_trt_plugin_harness dynamically loads shared library and invokes minimal plugin call path Remaining Work map stubs to real TensorRT IPluginV2DynamicExt implementations add shape inference checks and serialization blobs add CUDA kernels for packed gather/scan/scatter fast path add parity tests vs CPU reference ops (pack/unpack/fusion/decode) runtime/tensorrt/plugins/ tilepack.h tilepack.cpp tilepack.cu","title":"TensorRT Runtime Notes"},{"location":"runtime/TENSORRT/#tensorrt-runtime-scaffolding","text":"","title":"TensorRT Runtime Scaffolding"},{"location":"runtime/TENSORRT/#scope","text":"This document describes the C++ TensorRT plugin scaffolding under runtime/tensorrt/ . Detailed build and harness instructions: - docs/runtime/TENSORRT_BUILD.md Current status: - build system and plugin contracts are in place - TilePack now has a real TensorRT plugin implementation path (guarded) - remaining plugins are stubs - code is guarded to build on machines without TensorRT/CUDA installed","title":"Scope"},{"location":"runtime/TENSORRT/#directory-layout","text":"runtime/tensorrt/CMakeLists.txt runtime/tensorrt/include/apexx_trt/ common.hpp plugin_stub.hpp tile_pack_plugin.hpp tile_ssm_scan_plugin.hpp tile_unpack_fusion_plugin.hpp decode_nms_plugin.hpp (optional) runtime/tensorrt/src/ common.cpp tile_pack_plugin.cpp tile_ssm_scan_plugin.cpp tile_unpack_fusion_plugin.cpp decode_nms_plugin.cpp plugin_info_main.cpp","title":"Directory Layout"},{"location":"runtime/TENSORRT/#contract-mapping","text":"Contracts are inherited from: - docs/runtime/PLUGIN_SPEC.md - docs/ENGINEERING_SPEC.md Mapped plugin stubs: - TilePack - contract: F[B,C,Hf,Wf] + idx[B,K] -> P[B,K,C,t,t] + meta - real TensorRT plugin path available under runtime/tensorrt/plugins/tilepack.* - TileSSMScan - contract: packed token/tile sequence + optional state -> mixed outputs + next state - TileUnpackFusion - contract: base + packed outputs + meta + gate -> merged features with overlap priority - DecodeNMS (optional) - contract: fused decode + NMS for DET path","title":"Contract Mapping"},{"location":"runtime/TENSORRT/#build-guard-behavior","text":"CMakeLists.txt probes platform features and sets: - APEXX_ENABLE_TENSORRT to 1 only if TensorRT headers are found - APEXX_ENABLE_CUDA to 1 only if a CUDA compiler is available If unavailable, it builds a stub-only static library with the same API surface.","title":"Build Guard Behavior"},{"location":"runtime/TENSORRT/#build-steps","text":"Use: - docs/runtime/TENSORRT_BUILD.md","title":"Build Steps"},{"location":"runtime/TENSORRT/#output","text":"shared library (when TRT+CUDA available): apexx_trt_plugins static core library (always): apexx_trt_plugin_core utility executable: apexx_trt_plugin_info prints build summary and plugin availability flags harness executable (when shared library built): apexx_trt_plugin_harness dynamically loads shared library and invokes minimal plugin call path","title":"Output"},{"location":"runtime/TENSORRT/#remaining-work","text":"map stubs to real TensorRT IPluginV2DynamicExt implementations add shape inference checks and serialization blobs add CUDA kernels for packed gather/scan/scatter fast path add parity tests vs CPU reference ops (pack/unpack/fusion/decode) runtime/tensorrt/plugins/ tilepack.h tilepack.cpp tilepack.cu","title":"Remaining Work"},{"location":"runtime/TENSORRT_BUILD/","text":"TensorRT Plugin Build and Harness Scope This document describes how to build Apex-X TensorRT plugin artifacts and run the minimal runtime harness. Directory: - runtime/tensorrt/ CMake Targets apexx_trt_plugin_core (always built) static stub/core target used for metadata utilities apexx_trt_plugins (shared library, conditional) built only when both TensorRT headers and CUDA compiler are found linked only when TensorRT/CUDA runtime libraries are discoverable ( nvinfer , cudart ) apexx_trt_plugin_info (always built) prints plugin build summary apexx_trt_plugin_harness (conditional) built only when shared plugin library is built loads the shared library dynamically and invokes minimal plugin call path apexx_trt_tilepack_test (conditional) built only when real TilePack TensorRT plugin is enabled runs C++ parity check for TilePack plugin enqueue output vs host reference Environment Variables TENSORRT_ROOT optional root directory containing TensorRT headers (e.g. ${TENSORRT_ROOT}/include/NvInfer.h ) CUDA_HOME optional CUDA root path CMAKE_PREFIX_PATH optional path list for dependency discovery APEXX_TRT_PLUGIN_LIB runtime path for harness to load plugin shared library if not set, harness uses platform default library name APEXX_ENABLE_REAL_TILEPACK_PLUGIN CMake option to enable real TensorRT TilePack plugin build default: ON Build: Default (Auto-Detect) cd runtime/tensorrt cmake -S . -B build cmake --build build -j Run summary: ./build/apexx_trt_plugin_info If TensorRT/CUDA are unavailable: - shared plugin target is skipped - harness target is skipped - core/info targets still build Build: Explicit TensorRT + CUDA Paths cd runtime/tensorrt cmake -S . -B build \\ -DAPEXX_ENABLE_REAL_TILEPACK_PLUGIN=ON \\ -DTENSORRT_INCLUDE_DIR=\"${TENSORRT_ROOT}/include\" \\ -DCMAKE_CUDA_COMPILER=\"${CUDA_HOME}/bin/nvcc\" cmake --build build -j If CMake cannot locate runtime libraries automatically, provide search paths via: - CMAKE_PREFIX_PATH - LD_LIBRARY_PATH / DYLD_LIBRARY_PATH / PATH (platform dependent) Build: Force Skip Shared Plugins (Portable CI) cd runtime/tensorrt cmake -S . -B build \\ -DAPEXX_ENABLE_TENSORRT=OFF \\ -DAPEXX_ENABLE_CUDA=OFF \\ -DAPEXX_BUILD_PLUGIN_TEST_HARNESS=OFF cmake --build build -j Build: Disable Real TilePack Plugin but Keep Shared Stub Library cd runtime/tensorrt cmake -S . -B build \\ -DAPEXX_ENABLE_REAL_TILEPACK_PLUGIN=OFF cmake --build build -j Harness Usage If apexx_trt_plugins and harness are built: cd runtime/tensorrt ./build/apexx_trt_plugin_harness ./build/libapexx_trt_plugins.so Or with env var: export APEXX_TRT_PLUGIN_LIB=./build/libapexx_trt_plugins.so ./build/apexx_trt_plugin_harness macOS example library name: - libapexx_trt_plugins.dylib Windows example library name: - apexx_trt_plugins.dll Minimal Harness Contract Harness resolves C ABI symbols from the shared library: - apexx_trt_abi_version - apexx_trt_build_summary_cstr - apexx_trt_invoke_minimal Harness then: - creates dummy float buffers - calls minimal plugin path for: - TilePack - TileSSMScan - TileUnpackFusion - DecodeNMS This validates loadability and basic enqueue-like plugin call flow. TilePack C++ Test Harness When apexx_trt_tilepack_test is built: cd runtime/tensorrt ./build/apexx_trt_tilepack_test This test: - creates TilePack plugin through creator - serializes/deserializes plugin instance - executes enqueue() with CUDA buffers - compares output against a host reference implementation Python Parity Test (Optional) If TensorRT Python package is available: export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so python -m pytest -q tests/test_tensorrt_tilepack_parity.py The test builds a TensorRT engine containing TilePack and compares output to PyTorch reference.","title":"TensorRT Build Guide"},{"location":"runtime/TENSORRT_BUILD/#tensorrt-plugin-build-and-harness","text":"","title":"TensorRT Plugin Build and Harness"},{"location":"runtime/TENSORRT_BUILD/#scope","text":"This document describes how to build Apex-X TensorRT plugin artifacts and run the minimal runtime harness. Directory: - runtime/tensorrt/","title":"Scope"},{"location":"runtime/TENSORRT_BUILD/#cmake-targets","text":"apexx_trt_plugin_core (always built) static stub/core target used for metadata utilities apexx_trt_plugins (shared library, conditional) built only when both TensorRT headers and CUDA compiler are found linked only when TensorRT/CUDA runtime libraries are discoverable ( nvinfer , cudart ) apexx_trt_plugin_info (always built) prints plugin build summary apexx_trt_plugin_harness (conditional) built only when shared plugin library is built loads the shared library dynamically and invokes minimal plugin call path apexx_trt_tilepack_test (conditional) built only when real TilePack TensorRT plugin is enabled runs C++ parity check for TilePack plugin enqueue output vs host reference","title":"CMake Targets"},{"location":"runtime/TENSORRT_BUILD/#environment-variables","text":"TENSORRT_ROOT optional root directory containing TensorRT headers (e.g. ${TENSORRT_ROOT}/include/NvInfer.h ) CUDA_HOME optional CUDA root path CMAKE_PREFIX_PATH optional path list for dependency discovery APEXX_TRT_PLUGIN_LIB runtime path for harness to load plugin shared library if not set, harness uses platform default library name APEXX_ENABLE_REAL_TILEPACK_PLUGIN CMake option to enable real TensorRT TilePack plugin build default: ON","title":"Environment Variables"},{"location":"runtime/TENSORRT_BUILD/#build-default-auto-detect","text":"cd runtime/tensorrt cmake -S . -B build cmake --build build -j Run summary: ./build/apexx_trt_plugin_info If TensorRT/CUDA are unavailable: - shared plugin target is skipped - harness target is skipped - core/info targets still build","title":"Build: Default (Auto-Detect)"},{"location":"runtime/TENSORRT_BUILD/#build-explicit-tensorrt-cuda-paths","text":"cd runtime/tensorrt cmake -S . -B build \\ -DAPEXX_ENABLE_REAL_TILEPACK_PLUGIN=ON \\ -DTENSORRT_INCLUDE_DIR=\"${TENSORRT_ROOT}/include\" \\ -DCMAKE_CUDA_COMPILER=\"${CUDA_HOME}/bin/nvcc\" cmake --build build -j If CMake cannot locate runtime libraries automatically, provide search paths via: - CMAKE_PREFIX_PATH - LD_LIBRARY_PATH / DYLD_LIBRARY_PATH / PATH (platform dependent)","title":"Build: Explicit TensorRT + CUDA Paths"},{"location":"runtime/TENSORRT_BUILD/#build-force-skip-shared-plugins-portable-ci","text":"cd runtime/tensorrt cmake -S . -B build \\ -DAPEXX_ENABLE_TENSORRT=OFF \\ -DAPEXX_ENABLE_CUDA=OFF \\ -DAPEXX_BUILD_PLUGIN_TEST_HARNESS=OFF cmake --build build -j","title":"Build: Force Skip Shared Plugins (Portable CI)"},{"location":"runtime/TENSORRT_BUILD/#build-disable-real-tilepack-plugin-but-keep-shared-stub-library","text":"cd runtime/tensorrt cmake -S . -B build \\ -DAPEXX_ENABLE_REAL_TILEPACK_PLUGIN=OFF cmake --build build -j","title":"Build: Disable Real TilePack Plugin but Keep Shared Stub Library"},{"location":"runtime/TENSORRT_BUILD/#harness-usage","text":"If apexx_trt_plugins and harness are built: cd runtime/tensorrt ./build/apexx_trt_plugin_harness ./build/libapexx_trt_plugins.so Or with env var: export APEXX_TRT_PLUGIN_LIB=./build/libapexx_trt_plugins.so ./build/apexx_trt_plugin_harness macOS example library name: - libapexx_trt_plugins.dylib Windows example library name: - apexx_trt_plugins.dll","title":"Harness Usage"},{"location":"runtime/TENSORRT_BUILD/#minimal-harness-contract","text":"Harness resolves C ABI symbols from the shared library: - apexx_trt_abi_version - apexx_trt_build_summary_cstr - apexx_trt_invoke_minimal Harness then: - creates dummy float buffers - calls minimal plugin path for: - TilePack - TileSSMScan - TileUnpackFusion - DecodeNMS This validates loadability and basic enqueue-like plugin call flow.","title":"Minimal Harness Contract"},{"location":"runtime/TENSORRT_BUILD/#tilepack-c-test-harness","text":"When apexx_trt_tilepack_test is built: cd runtime/tensorrt ./build/apexx_trt_tilepack_test This test: - creates TilePack plugin through creator - serializes/deserializes plugin instance - executes enqueue() with CUDA buffers - compares output against a host reference implementation","title":"TilePack C++ Test Harness"},{"location":"runtime/TENSORRT_BUILD/#python-parity-test-optional","text":"If TensorRT Python package is available: export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so python -m pytest -q tests/test_tensorrt_tilepack_parity.py The test builds a TensorRT engine containing TilePack and compares output to PyTorch reference.","title":"Python Parity Test (Optional)"},{"location":"runtime/TRITON/","text":"Triton Fused Tile Ops Scope This document describes Triton runtime paths for Apex-X tile operations. TilePack-specific kernel notes are documented in: - docs/runtime/TRITON_TILEPACK.md TileUnpack-specific kernel notes are documented in: - docs/runtime/TRITON_TILEUNPACK.md FusionGate-specific kernel notes are documented in: - docs/runtime/TRITON_FUSION.md TileSSM scan notes are documented in: - docs/runtime/TRITON_SSM.md Stage-1 fused pack/op/unpack notes are documented in: - docs/runtime/TRITON_FUSED_STAGE1.md Current repository behavior is environment-driven: - if CUDA + Triton are available: Triton backend can be selected - if unavailable: reference PyTorch path is used APIs Legacy runtime fused API (still reference-first) is in: - apex_x/runtime/triton_fused.py Stage-1 fused kernel API is in: - apex_x/kernels/triton/fused_pack_op_unpack.py Legacy runtime fused entrypoints: get_triton_availability() gather_gate_scatter_reference(...) gather_gate_scatter(...) gather_gate_scatter(...) dispatches: - Triton backend when available and requested - reference backend on fallback ( allow_fallback=True ) Result object includes: - backend ( reference or triton ) - fallback_reason - merged , priority_map , alpha_map , meta Semantics Reference path preserves the same contracts as tile ops and fusion gate modules: - deterministic tile ordering - overlap priority semantics - fusion equation: - fused = base + alpha * (heavy - base) - priority map update semantics identical to TileUnpackTorch Availability Contract get_triton_availability() checks: - Triton import availability - CUDA availability - CUDA device count If any check fails, dispatch falls back with explicit reason: - triton_not_installed - cuda_unavailable - cuda_device_not_found Kernel Status Current status in this repository: - dedicated Triton TilePack gather kernel is implemented with fallback dispatch: - apex_x/kernels/triton/tilepack.py - dedicated Triton TileUnpack scatter kernel is implemented with deterministic overlap priority: - apex_x/kernels/triton/tileunpack.py - dedicated Triton FusionGate alpha/fusion kernels are implemented with fallback dispatch: - apex_x/kernels/triton/fusiongate.py - baseline Triton TileSSM scan kernel is implemented with inference-first dispatch: - apex_x/kernels/triton/tilessm_scan.py - supports forward , backward , and bidirectional directional APIs - bidirectional merge modes: sum , avg , gated (gate computed in torch) - Stage-1 fused fast path ( gather -> affine+ReGLU -> scatter ) is implemented: - apex_x/kernels/triton/fused_pack_op_unpack.py - legacy runtime gather_gate_scatter(...) Triton entrypoint remains a stub: - _triton_fused_kernel_stub(...) raises NotImplementedError - fallback keeps the reference path stable This keeps compatibility for the old runtime API while enabling a practical fused kernel path. Correctness Tests tests/test_triton_fused.py reference dispatch parity vs explicit reference pipeline fallback behavior when Triton is unavailable forced Triton path without fallback raises stub error tests/test_triton_fused_stage1_dispatch.py CPU fallback parity vs separate pack/op/unpack composition deterministic duplicate-index guard tests/test_triton_fused_stage1_gpu.py GPU parity and dispatch checks (auto-skip when CUDA/Triton unavailable) tests/test_triton_tilessm_parity_dispatch.py parity vs torch stable scan on CPU/reference path tests/test_triton_tilessm_parity_gpu.py GPU parity and dispatch checks for TileSSM scan (auto-skip without CUDA/Triton) Microbenchmark scripts/triton_fused_bench.py reports reference vs dispatched path timing prints backend and fallback reason on CPU/no-Triton setups, benchmark exercises fallback path When Triton kernel is implemented, the same script should report runtime speedup. For Stage-1 fused microbench: - python -m apex_x.bench.triton_fused_stage1_bench","title":"Triton Runtime Notes"},{"location":"runtime/TRITON/#triton-fused-tile-ops","text":"","title":"Triton Fused Tile Ops"},{"location":"runtime/TRITON/#scope","text":"This document describes Triton runtime paths for Apex-X tile operations. TilePack-specific kernel notes are documented in: - docs/runtime/TRITON_TILEPACK.md TileUnpack-specific kernel notes are documented in: - docs/runtime/TRITON_TILEUNPACK.md FusionGate-specific kernel notes are documented in: - docs/runtime/TRITON_FUSION.md TileSSM scan notes are documented in: - docs/runtime/TRITON_SSM.md Stage-1 fused pack/op/unpack notes are documented in: - docs/runtime/TRITON_FUSED_STAGE1.md Current repository behavior is environment-driven: - if CUDA + Triton are available: Triton backend can be selected - if unavailable: reference PyTorch path is used","title":"Scope"},{"location":"runtime/TRITON/#apis","text":"Legacy runtime fused API (still reference-first) is in: - apex_x/runtime/triton_fused.py Stage-1 fused kernel API is in: - apex_x/kernels/triton/fused_pack_op_unpack.py Legacy runtime fused entrypoints: get_triton_availability() gather_gate_scatter_reference(...) gather_gate_scatter(...) gather_gate_scatter(...) dispatches: - Triton backend when available and requested - reference backend on fallback ( allow_fallback=True ) Result object includes: - backend ( reference or triton ) - fallback_reason - merged , priority_map , alpha_map , meta","title":"APIs"},{"location":"runtime/TRITON/#semantics","text":"Reference path preserves the same contracts as tile ops and fusion gate modules: - deterministic tile ordering - overlap priority semantics - fusion equation: - fused = base + alpha * (heavy - base) - priority map update semantics identical to TileUnpackTorch","title":"Semantics"},{"location":"runtime/TRITON/#availability-contract","text":"get_triton_availability() checks: - Triton import availability - CUDA availability - CUDA device count If any check fails, dispatch falls back with explicit reason: - triton_not_installed - cuda_unavailable - cuda_device_not_found","title":"Availability Contract"},{"location":"runtime/TRITON/#kernel-status","text":"Current status in this repository: - dedicated Triton TilePack gather kernel is implemented with fallback dispatch: - apex_x/kernels/triton/tilepack.py - dedicated Triton TileUnpack scatter kernel is implemented with deterministic overlap priority: - apex_x/kernels/triton/tileunpack.py - dedicated Triton FusionGate alpha/fusion kernels are implemented with fallback dispatch: - apex_x/kernels/triton/fusiongate.py - baseline Triton TileSSM scan kernel is implemented with inference-first dispatch: - apex_x/kernels/triton/tilessm_scan.py - supports forward , backward , and bidirectional directional APIs - bidirectional merge modes: sum , avg , gated (gate computed in torch) - Stage-1 fused fast path ( gather -> affine+ReGLU -> scatter ) is implemented: - apex_x/kernels/triton/fused_pack_op_unpack.py - legacy runtime gather_gate_scatter(...) Triton entrypoint remains a stub: - _triton_fused_kernel_stub(...) raises NotImplementedError - fallback keeps the reference path stable This keeps compatibility for the old runtime API while enabling a practical fused kernel path.","title":"Kernel Status"},{"location":"runtime/TRITON/#correctness-tests","text":"tests/test_triton_fused.py reference dispatch parity vs explicit reference pipeline fallback behavior when Triton is unavailable forced Triton path without fallback raises stub error tests/test_triton_fused_stage1_dispatch.py CPU fallback parity vs separate pack/op/unpack composition deterministic duplicate-index guard tests/test_triton_fused_stage1_gpu.py GPU parity and dispatch checks (auto-skip when CUDA/Triton unavailable) tests/test_triton_tilessm_parity_dispatch.py parity vs torch stable scan on CPU/reference path tests/test_triton_tilessm_parity_gpu.py GPU parity and dispatch checks for TileSSM scan (auto-skip without CUDA/Triton)","title":"Correctness Tests"},{"location":"runtime/TRITON/#microbenchmark","text":"scripts/triton_fused_bench.py reports reference vs dispatched path timing prints backend and fallback reason on CPU/no-Triton setups, benchmark exercises fallback path When Triton kernel is implemented, the same script should report runtime speedup. For Stage-1 fused microbench: - python -m apex_x.bench.triton_fused_stage1_bench","title":"Microbenchmark"},{"location":"runtime/TRITON_FUSED_STAGE1/","text":"Triton Fused Stage-1 Tile Pipeline Scope This document specifies the first practical fused Triton fast path for Apex-X: - gather selected tiles from dense map - apply lightweight per-tile transform ( pointwise affine + ReGLU-like gate ) - scatter transformed tiles back to dense map Implementation: - apex_x/kernels/triton/fused_pack_op_unpack.py Tensor Contract Input feature map: F [B, C, H, W] , contiguous NCHW Input indices: idx [B, K] , integer tile ids ( int32 kernel path; int64 accepted and cast) Tile size: t Output merged map: F_out [B, C, H, W] , contiguous Tile id domain: - 0 <= idx < (H / t) * (W / t) - H % t == 0 and W % t == 0 Stage-1 deterministic overwrite assumption: - indices are required to be unique per batch row - this avoids write races and gives deterministic semantics Transform Definition For each selected tile pixel value x : value = value_scale * x + value_bias gate = gate_scale * x + gate_bias y = value * ReLU(gate) This is a minimal ReGLU-like placeholder to validate fused infrastructure. API get_triton_fused_stage1_availability() apply_pointwise_affine_reglu(...) separate_pack_op_unpack_reference(...) fused_pack_op_unpack_reference(...) fused_pack_op_unpack_triton(...) fused_pack_op_unpack_dispatch(...) Dispatch behavior: - prefers Triton on CUDA when available - falls back to reference on unsupported environments - falls back to reference when requires_grad and inference_only=True Parity and Correctness Tests: - tests/test_triton_fused_stage1_dispatch.py - tests/test_triton_fused_stage1_gpu.py Coverage: - CPU fallback parity against separate reference composition ( pack -> op -> unpack ) - deterministic duplicate-index guard - GPU parity (fp16) when Triton/CUDA is available - autograd-safe fallback behavior Benchmark Microbenchmark: - apex_x/bench/triton_fused_stage1_bench.py Command: python -m apex_x.bench.triton_fused_stage1_bench \\ --batch 1 \\ --channels 128 \\ --height 128 \\ --width 128 \\ --tile-size 8 \\ --kmax 32 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Reported metrics: - reference_p50/p95 : explicit reference composition path - separate_dispatch_p50/p95 : split dispatch path ( TilePack -> op -> TileUnpack ) - fused_dispatch_p50/p95 : fused Stage-1 dispatch path - speedup_separate_over_fused : primary speedup metric for Stage-1 Limitations (Stage-1) transform is intentionally lightweight and local only no Tile-SSM fusion yet no overlap-priority blending semantics inside this kernel path (unique indices required) no custom backward kernel (inference-oriented Triton path)","title":"Triton Fused Stage-1"},{"location":"runtime/TRITON_FUSED_STAGE1/#triton-fused-stage-1-tile-pipeline","text":"","title":"Triton Fused Stage-1 Tile Pipeline"},{"location":"runtime/TRITON_FUSED_STAGE1/#scope","text":"This document specifies the first practical fused Triton fast path for Apex-X: - gather selected tiles from dense map - apply lightweight per-tile transform ( pointwise affine + ReGLU-like gate ) - scatter transformed tiles back to dense map Implementation: - apex_x/kernels/triton/fused_pack_op_unpack.py","title":"Scope"},{"location":"runtime/TRITON_FUSED_STAGE1/#tensor-contract","text":"Input feature map: F [B, C, H, W] , contiguous NCHW Input indices: idx [B, K] , integer tile ids ( int32 kernel path; int64 accepted and cast) Tile size: t Output merged map: F_out [B, C, H, W] , contiguous Tile id domain: - 0 <= idx < (H / t) * (W / t) - H % t == 0 and W % t == 0 Stage-1 deterministic overwrite assumption: - indices are required to be unique per batch row - this avoids write races and gives deterministic semantics","title":"Tensor Contract"},{"location":"runtime/TRITON_FUSED_STAGE1/#transform-definition","text":"For each selected tile pixel value x : value = value_scale * x + value_bias gate = gate_scale * x + gate_bias y = value * ReLU(gate) This is a minimal ReGLU-like placeholder to validate fused infrastructure.","title":"Transform Definition"},{"location":"runtime/TRITON_FUSED_STAGE1/#api","text":"get_triton_fused_stage1_availability() apply_pointwise_affine_reglu(...) separate_pack_op_unpack_reference(...) fused_pack_op_unpack_reference(...) fused_pack_op_unpack_triton(...) fused_pack_op_unpack_dispatch(...) Dispatch behavior: - prefers Triton on CUDA when available - falls back to reference on unsupported environments - falls back to reference when requires_grad and inference_only=True","title":"API"},{"location":"runtime/TRITON_FUSED_STAGE1/#parity-and-correctness","text":"Tests: - tests/test_triton_fused_stage1_dispatch.py - tests/test_triton_fused_stage1_gpu.py Coverage: - CPU fallback parity against separate reference composition ( pack -> op -> unpack ) - deterministic duplicate-index guard - GPU parity (fp16) when Triton/CUDA is available - autograd-safe fallback behavior","title":"Parity and Correctness"},{"location":"runtime/TRITON_FUSED_STAGE1/#benchmark","text":"Microbenchmark: - apex_x/bench/triton_fused_stage1_bench.py Command: python -m apex_x.bench.triton_fused_stage1_bench \\ --batch 1 \\ --channels 128 \\ --height 128 \\ --width 128 \\ --tile-size 8 \\ --kmax 32 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Reported metrics: - reference_p50/p95 : explicit reference composition path - separate_dispatch_p50/p95 : split dispatch path ( TilePack -> op -> TileUnpack ) - fused_dispatch_p50/p95 : fused Stage-1 dispatch path - speedup_separate_over_fused : primary speedup metric for Stage-1","title":"Benchmark"},{"location":"runtime/TRITON_FUSED_STAGE1/#limitations-stage-1","text":"transform is intentionally lightweight and local only no Tile-SSM fusion yet no overlap-priority blending semantics inside this kernel path (unique indices required) no custom backward kernel (inference-oriented Triton path)","title":"Limitations (Stage-1)"},{"location":"runtime/TRITON_FUSION/","text":"Triton FusionGate Kernel Scope This document describes the Triton FusionGate kernels implemented in: - apex_x/kernels/triton/fusiongate.py Implemented paths: - alpha kernel: computes alpha[B,1,H,W] - optional fused kernel: computes F = F_base + alpha * (F_detail - F_base) Tensor Contract Inputs: boundary_proxy [B,1,H,W] (or [B,H,W] ) uncertainty_proxy [B,1,H,W] (or [B,H,W] ) Alpha output: alpha [B,1,H,W] Optional fusion inputs: base_features [B,C,H,W] detail_features [B,C,H,W] Optional fusion output: fused [B,C,H,W] Formula Weight parameterization follows model FusionGate : w_b = softplus(boundary_log_weight) w_u = softplus(uncertainty_log_weight) Alpha: alpha = sigmoid(w_b * boundary + w_u * uncertainty + bias) Fusion: fused = base + alpha * (detail - base) Dtype Support Triton path: fp16 , bf16 on CUDA Reference path: fp32 , fp16 , bf16 Dispatch and Fallback Use: - fusiongate_dispatch(...) Behavior: - prefers Triton when available - falls back to reference path when Triton/CUDA unavailable - falls back to reference path when autograd is requested and inference_only=True - optional in-place fusion is supported ( inplace_fusion=True ) API get_triton_fusiongate_availability() fusiongate_alpha_reference(...) fusiongate_fuse_reference(...) fusiongate_alpha_triton(...) fusiongate_fuse_triton(...) fusiongate_dispatch(...) Testing tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py Coverage: - parity vs apex_x.model.FusionGate alpha behavior - alpha range check [0,1] - optional fusion parity - GPU Triton parity (auto-skip without CUDA+Triton) Microbenchmark Implemented in: - apex_x/bench/triton_fusiongate_bench.py Run: python -m apex_x.bench.triton_fusiongate_bench \\ --batch 1 \\ --channels 128 \\ --height 128 \\ --width 128 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Report includes: - alpha path timing (reference vs dispatch) - alpha+fusion timing (reference vs dispatch) - backend/fallback info and speedup ratios","title":"Triton FusionGate"},{"location":"runtime/TRITON_FUSION/#triton-fusiongate-kernel","text":"","title":"Triton FusionGate Kernel"},{"location":"runtime/TRITON_FUSION/#scope","text":"This document describes the Triton FusionGate kernels implemented in: - apex_x/kernels/triton/fusiongate.py Implemented paths: - alpha kernel: computes alpha[B,1,H,W] - optional fused kernel: computes F = F_base + alpha * (F_detail - F_base)","title":"Scope"},{"location":"runtime/TRITON_FUSION/#tensor-contract","text":"Inputs: boundary_proxy [B,1,H,W] (or [B,H,W] ) uncertainty_proxy [B,1,H,W] (or [B,H,W] ) Alpha output: alpha [B,1,H,W] Optional fusion inputs: base_features [B,C,H,W] detail_features [B,C,H,W] Optional fusion output: fused [B,C,H,W]","title":"Tensor Contract"},{"location":"runtime/TRITON_FUSION/#formula","text":"Weight parameterization follows model FusionGate : w_b = softplus(boundary_log_weight) w_u = softplus(uncertainty_log_weight) Alpha: alpha = sigmoid(w_b * boundary + w_u * uncertainty + bias) Fusion: fused = base + alpha * (detail - base)","title":"Formula"},{"location":"runtime/TRITON_FUSION/#dtype-support","text":"Triton path: fp16 , bf16 on CUDA Reference path: fp32 , fp16 , bf16","title":"Dtype Support"},{"location":"runtime/TRITON_FUSION/#dispatch-and-fallback","text":"Use: - fusiongate_dispatch(...) Behavior: - prefers Triton when available - falls back to reference path when Triton/CUDA unavailable - falls back to reference path when autograd is requested and inference_only=True - optional in-place fusion is supported ( inplace_fusion=True )","title":"Dispatch and Fallback"},{"location":"runtime/TRITON_FUSION/#api","text":"get_triton_fusiongate_availability() fusiongate_alpha_reference(...) fusiongate_fuse_reference(...) fusiongate_alpha_triton(...) fusiongate_fuse_triton(...) fusiongate_dispatch(...)","title":"API"},{"location":"runtime/TRITON_FUSION/#testing","text":"tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py Coverage: - parity vs apex_x.model.FusionGate alpha behavior - alpha range check [0,1] - optional fusion parity - GPU Triton parity (auto-skip without CUDA+Triton)","title":"Testing"},{"location":"runtime/TRITON_FUSION/#microbenchmark","text":"Implemented in: - apex_x/bench/triton_fusiongate_bench.py Run: python -m apex_x.bench.triton_fusiongate_bench \\ --batch 1 \\ --channels 128 \\ --height 128 \\ --width 128 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Report includes: - alpha path timing (reference vs dispatch) - alpha+fusion timing (reference vs dispatch) - backend/fallback info and speedup ratios","title":"Microbenchmark"},{"location":"runtime/TRITON_SSM/","text":"Triton TileSSM Scan (Baseline) Scope This document defines the baseline Triton TileSSM scan implementation: - input tokens tokens [B,K,C] - linear recurrence scan over K for each (B,C) stream - supports directions: - forward - backward - bidirectional - output sequence y [B,K,C] and final state: - [B,C] for forward / backward - [B,2,C] for bidirectional ( forward_state , backward_state ) Implementation: - apex_x/kernels/triton/tilessm_scan.py Recurrence Per batch b , step k , channel c : driven = input_gain[c] * x[b,k,c] + state_bias[c] state = decay[c] * state + (1 - decay[c]) * driven y[b,k,c] = output_gain[c] * state Stability notes: - token sanitization: nan -> 0 , infinities clamped - token clamp range default: [-1e4, 1e4] - decay clamped into (1e-6, 1-1e-6) - accumulation computed in fp32 in both reference and Triton paths API get_triton_tilessm_availability() tilessm_scan_reference(...) tilessm_scan_triton(...) tilessm_scan_dispatch(...) scan(tokens, direction=...) -> y (clean API) Direction and merge options: - direction : forward | backward | bidirectional - merge_mode (for bidirectional): sum | avg | gated - merge_gate (optional, torch-computed): [C] or [B,1,C] , used when merge_mode=\"gated\" Dispatch semantics: - inference-first Triton path ( prefer_triton=True ) - reference fallback when Triton/CUDA unavailable - reference fallback when requires_grad and inference_only=True Training vs Inference Integration Training/backward path should continue using torch scan modules: StableStateSpaceScan StableBidirectionalStateSpaceScan Inference can opt into Triton dispatch via model wiring. Current integration: - apex_x/model/ff_heavy_path.py - new use_triton_inference_scan toggle - when enabled and module is in .eval() mode, scan uses tilessm_scan_dispatch(...) - in .train() mode, existing torch scan path is kept Limitations (Baseline) kernel is still forward recurrence only; backward and bidirectional are built from directional composition no custom backward kernel (training path remains torch/reference) compile specialization by K (sequence length) conservative cap in Triton path: K <= 4096 Tests tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py Coverage: - parity vs torch stable scan on small/medium shapes - parity for backward and bidirectional merge modes ( sum/avg/gated ) - deterministic CPU fallback behavior - autograd-safe fallback in inference-only mode - integration check: eval uses dispatch; train uses torch path Benchmark Microbenchmark: - apex_x/bench/triton_tilessm_bench.py Run: python -m apex_x.bench.triton_tilessm_bench \\ --batch 2 \\ --steps 256 \\ --channels 128 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Reported: - per-direction timings: - forward - backward - bidirectional (avg, gated) - multi-direction overhead ratios vs forward - forward throughput and speedup","title":"Triton TileSSM Scan"},{"location":"runtime/TRITON_SSM/#triton-tilessm-scan-baseline","text":"","title":"Triton TileSSM Scan (Baseline)"},{"location":"runtime/TRITON_SSM/#scope","text":"This document defines the baseline Triton TileSSM scan implementation: - input tokens tokens [B,K,C] - linear recurrence scan over K for each (B,C) stream - supports directions: - forward - backward - bidirectional - output sequence y [B,K,C] and final state: - [B,C] for forward / backward - [B,2,C] for bidirectional ( forward_state , backward_state ) Implementation: - apex_x/kernels/triton/tilessm_scan.py","title":"Scope"},{"location":"runtime/TRITON_SSM/#recurrence","text":"Per batch b , step k , channel c : driven = input_gain[c] * x[b,k,c] + state_bias[c] state = decay[c] * state + (1 - decay[c]) * driven y[b,k,c] = output_gain[c] * state Stability notes: - token sanitization: nan -> 0 , infinities clamped - token clamp range default: [-1e4, 1e4] - decay clamped into (1e-6, 1-1e-6) - accumulation computed in fp32 in both reference and Triton paths","title":"Recurrence"},{"location":"runtime/TRITON_SSM/#api","text":"get_triton_tilessm_availability() tilessm_scan_reference(...) tilessm_scan_triton(...) tilessm_scan_dispatch(...) scan(tokens, direction=...) -> y (clean API) Direction and merge options: - direction : forward | backward | bidirectional - merge_mode (for bidirectional): sum | avg | gated - merge_gate (optional, torch-computed): [C] or [B,1,C] , used when merge_mode=\"gated\" Dispatch semantics: - inference-first Triton path ( prefer_triton=True ) - reference fallback when Triton/CUDA unavailable - reference fallback when requires_grad and inference_only=True","title":"API"},{"location":"runtime/TRITON_SSM/#training-vs-inference-integration","text":"Training/backward path should continue using torch scan modules: StableStateSpaceScan StableBidirectionalStateSpaceScan Inference can opt into Triton dispatch via model wiring. Current integration: - apex_x/model/ff_heavy_path.py - new use_triton_inference_scan toggle - when enabled and module is in .eval() mode, scan uses tilessm_scan_dispatch(...) - in .train() mode, existing torch scan path is kept","title":"Training vs Inference Integration"},{"location":"runtime/TRITON_SSM/#limitations-baseline","text":"kernel is still forward recurrence only; backward and bidirectional are built from directional composition no custom backward kernel (training path remains torch/reference) compile specialization by K (sequence length) conservative cap in Triton path: K <= 4096","title":"Limitations (Baseline)"},{"location":"runtime/TRITON_SSM/#tests","text":"tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py Coverage: - parity vs torch stable scan on small/medium shapes - parity for backward and bidirectional merge modes ( sum/avg/gated ) - deterministic CPU fallback behavior - autograd-safe fallback in inference-only mode - integration check: eval uses dispatch; train uses torch path","title":"Tests"},{"location":"runtime/TRITON_SSM/#benchmark","text":"Microbenchmark: - apex_x/bench/triton_tilessm_bench.py Run: python -m apex_x.bench.triton_tilessm_bench \\ --batch 2 \\ --steps 256 \\ --channels 128 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Reported: - per-direction timings: - forward - backward - bidirectional (avg, gated) - multi-direction overhead ratios vs forward - forward throughput and speedup","title":"Benchmark"},{"location":"runtime/TRITON_TILEPACK/","text":"Triton TilePack Kernel Scope This document describes the Triton TilePack gather kernel implemented in: - apex_x/kernels/triton/tilepack.py The kernel gathers tiles from a dense feature map into packed layout without Python tile loops. Tensor Contract Input feature map: F [B, C, H, W] , contiguous NCHW Input tile ids: idx [B, K] , integer tile ids (expected int32 , int64 accepted and cast) Output packed tensor: P [B, K, C, t, t] , contiguous Grid assumptions: - H % t == 0 and W % t == 0 - tile id domain: [0, (H / t) * (W / t) - 1] Layout assumption: - idx order is consumed as-is by Triton path - no implicit ordering/reordering inside kernel Dtype Support Triton kernel path: fp16 , bf16 Reference path: fp32 , fp16 , bf16 Dispatch and Fallback Use: - tilepack_dispatch(...) Behavior: - prefers Triton when available ( CUDA + Triton ) - falls back to vectorized reference PyTorch gather when unavailable - falls back to reference when feature_map.requires_grad and inference_only=True - reason: current Triton path is inference-oriented and does not register custom backward API get_triton_tilepack_availability() tilepack_reference(...) tilepack_triton(...) tilepack_dispatch(...) Testing Parity tests: - tests/test_triton_tilepack_parity_dispatch.py - tests/test_triton_tilepack_parity_gpu.py Coverage: - CPU fallback correctness vs TilePackTorch - GPU parity vs TilePackTorch on multiple shapes (when Triton/CUDA available) - deterministic seed use - gradient safety via reference fallback path Microbenchmark Implemented in: - apex_x/bench/triton_tilepack_bench.py Run: python -m apex_x.bench.triton_tilepack_bench \\ --batch 1 \\ --channels 128 \\ --height 128 \\ --width 128 \\ --tile-size 8 \\ --kmax 32 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Report includes: - availability and backend selected - fallback reason (if any) - reference_p50/p95 and dispatch_p50/p95 - speedup ratio ( reference / dispatch )","title":"Triton TilePack"},{"location":"runtime/TRITON_TILEPACK/#triton-tilepack-kernel","text":"","title":"Triton TilePack Kernel"},{"location":"runtime/TRITON_TILEPACK/#scope","text":"This document describes the Triton TilePack gather kernel implemented in: - apex_x/kernels/triton/tilepack.py The kernel gathers tiles from a dense feature map into packed layout without Python tile loops.","title":"Scope"},{"location":"runtime/TRITON_TILEPACK/#tensor-contract","text":"Input feature map: F [B, C, H, W] , contiguous NCHW Input tile ids: idx [B, K] , integer tile ids (expected int32 , int64 accepted and cast) Output packed tensor: P [B, K, C, t, t] , contiguous Grid assumptions: - H % t == 0 and W % t == 0 - tile id domain: [0, (H / t) * (W / t) - 1] Layout assumption: - idx order is consumed as-is by Triton path - no implicit ordering/reordering inside kernel","title":"Tensor Contract"},{"location":"runtime/TRITON_TILEPACK/#dtype-support","text":"Triton kernel path: fp16 , bf16 Reference path: fp32 , fp16 , bf16","title":"Dtype Support"},{"location":"runtime/TRITON_TILEPACK/#dispatch-and-fallback","text":"Use: - tilepack_dispatch(...) Behavior: - prefers Triton when available ( CUDA + Triton ) - falls back to vectorized reference PyTorch gather when unavailable - falls back to reference when feature_map.requires_grad and inference_only=True - reason: current Triton path is inference-oriented and does not register custom backward","title":"Dispatch and Fallback"},{"location":"runtime/TRITON_TILEPACK/#api","text":"get_triton_tilepack_availability() tilepack_reference(...) tilepack_triton(...) tilepack_dispatch(...)","title":"API"},{"location":"runtime/TRITON_TILEPACK/#testing","text":"Parity tests: - tests/test_triton_tilepack_parity_dispatch.py - tests/test_triton_tilepack_parity_gpu.py Coverage: - CPU fallback correctness vs TilePackTorch - GPU parity vs TilePackTorch on multiple shapes (when Triton/CUDA available) - deterministic seed use - gradient safety via reference fallback path","title":"Testing"},{"location":"runtime/TRITON_TILEPACK/#microbenchmark","text":"Implemented in: - apex_x/bench/triton_tilepack_bench.py Run: python -m apex_x.bench.triton_tilepack_bench \\ --batch 1 \\ --channels 128 \\ --height 128 \\ --width 128 \\ --tile-size 8 \\ --kmax 32 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Report includes: - availability and backend selected - fallback reason (if any) - reference_p50/p95 and dispatch_p50/p95 - speedup ratio ( reference / dispatch )","title":"Microbenchmark"},{"location":"runtime/TRITON_TILEUNPACK/","text":"Triton TileUnpack Kernel Scope This document describes the Triton TileUnpack scatter kernel implemented in: - apex_x/kernels/triton/tileunpack.py Current scope: - L0 tile scatter with overlap handling - deterministic priority overwrite semantics - optional blend mode (currently routed through reference path) Tensor Contract Input base map: F_base [B, C, H, W] , contiguous NCHW Input packed map: P_out [B, K, C, t, t] , contiguous Input indices/meta: either idx [B,K] tile ids, or meta containing origins [B,K,2] Output merged map: F_merged [B, C, H, W] Assumptions: - H % t == 0 , W % t == 0 - overlaps are allowed Priority inputs: - levels [B,K] integer levels (higher wins), or - pre-sorted K-order ( assume_priority_sorted=True ) where later K wins when levels are absent Dtype Support Triton kernel path: fp16 , bf16 on CUDA Reference path: fp32 , fp16 , bf16 Dispatch and Fallback Use: - tileunpack_dispatch(...) Behavior: - prefers Triton when available ( CUDA + Triton ) - falls back to reference path when Triton is unavailable - falls back to reference when autograd is requested ( requires_grad and inference_only=True ) - defaults to overlap_mode=\"override\" (priority overwrite) - overlap_mode=\"blend\" is supported via reference fallback in this stage API get_triton_tileunpack_availability() tileunpack_reference(...) tileunpack_triton(...) tileunpack_dispatch(...) Testing Parity tests: - tests/test_triton_tileunpack_parity_dispatch.py - tests/test_triton_tileunpack_parity_gpu.py Coverage: - CPU fallback parity with TileUnpackTorch and reference overlap semantics - GPU parity with TileUnpackTorch on representative shapes (auto-skip without CUDA+Triton) - gradient-safe fallback behavior - synthetic overlap fixtures with explicit overwrite expectations Microbenchmark Implemented in: - apex_x/bench/triton_tileunpack_bench.py Run: python -m apex_x.bench.triton_tileunpack_bench \\ --batch 1 \\ --channels 128 \\ --height 128 \\ --width 128 \\ --tile-size 8 \\ --kmax 32 \\ --overlap-shift 4 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Report includes: - backend selected and fallback reason - reference_p50/p95 , dispatch_p50/p95 - speedup ratio ( reference / dispatch )","title":"Triton TileUnpack"},{"location":"runtime/TRITON_TILEUNPACK/#triton-tileunpack-kernel","text":"","title":"Triton TileUnpack Kernel"},{"location":"runtime/TRITON_TILEUNPACK/#scope","text":"This document describes the Triton TileUnpack scatter kernel implemented in: - apex_x/kernels/triton/tileunpack.py Current scope: - L0 tile scatter with overlap handling - deterministic priority overwrite semantics - optional blend mode (currently routed through reference path)","title":"Scope"},{"location":"runtime/TRITON_TILEUNPACK/#tensor-contract","text":"Input base map: F_base [B, C, H, W] , contiguous NCHW Input packed map: P_out [B, K, C, t, t] , contiguous Input indices/meta: either idx [B,K] tile ids, or meta containing origins [B,K,2] Output merged map: F_merged [B, C, H, W] Assumptions: - H % t == 0 , W % t == 0 - overlaps are allowed Priority inputs: - levels [B,K] integer levels (higher wins), or - pre-sorted K-order ( assume_priority_sorted=True ) where later K wins when levels are absent","title":"Tensor Contract"},{"location":"runtime/TRITON_TILEUNPACK/#dtype-support","text":"Triton kernel path: fp16 , bf16 on CUDA Reference path: fp32 , fp16 , bf16","title":"Dtype Support"},{"location":"runtime/TRITON_TILEUNPACK/#dispatch-and-fallback","text":"Use: - tileunpack_dispatch(...) Behavior: - prefers Triton when available ( CUDA + Triton ) - falls back to reference path when Triton is unavailable - falls back to reference when autograd is requested ( requires_grad and inference_only=True ) - defaults to overlap_mode=\"override\" (priority overwrite) - overlap_mode=\"blend\" is supported via reference fallback in this stage","title":"Dispatch and Fallback"},{"location":"runtime/TRITON_TILEUNPACK/#api","text":"get_triton_tileunpack_availability() tileunpack_reference(...) tileunpack_triton(...) tileunpack_dispatch(...)","title":"API"},{"location":"runtime/TRITON_TILEUNPACK/#testing","text":"Parity tests: - tests/test_triton_tileunpack_parity_dispatch.py - tests/test_triton_tileunpack_parity_gpu.py Coverage: - CPU fallback parity with TileUnpackTorch and reference overlap semantics - GPU parity with TileUnpackTorch on representative shapes (auto-skip without CUDA+Triton) - gradient-safe fallback behavior - synthetic overlap fixtures with explicit overwrite expectations","title":"Testing"},{"location":"runtime/TRITON_TILEUNPACK/#microbenchmark","text":"Implemented in: - apex_x/bench/triton_tileunpack_bench.py Run: python -m apex_x.bench.triton_tileunpack_bench \\ --batch 1 \\ --channels 128 \\ --height 128 \\ --width 128 \\ --tile-size 8 \\ --kmax 32 \\ --overlap-shift 4 \\ --warmup 10 \\ --iters 50 \\ --dtype fp16 Report includes: - backend selected and fallback reason - reference_p50/p95 , dispatch_p50/p95 - speedup ratio ( reference / dispatch )","title":"Microbenchmark"}]}