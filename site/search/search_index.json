{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Apex-X Documentation","text":"<p>Primary documentation entrypoints:</p> <ul> <li>Project overview and runnable quickstart: <code>README.md</code></li> <li>Training flow and checkpoint policy: <code>docs/TRAINING_GUIDE.md</code></li> <li>Runtime and deployment notes: <code>docs/runtime/</code></li> <li>Benchmark protocol/docs: <code>docs/benchmarks.md</code></li> </ul>"},{"location":"#core-docs","title":"Core docs","text":"<ol> <li><code>docs/ENGINEERING_SPEC.md</code></li> <li><code>docs/algorithms.md</code></li> <li><code>docs/DECISIONS.md</code></li> <li><code>docs/release/CHECKLIST.md</code></li> </ol>"},{"location":"#cli-reference-current","title":"CLI reference (current)","text":"<ul> <li><code>python -m apex_x.cli train ...</code></li> <li><code>python -m apex_x.cli eval ...</code></li> <li><code>python -m apex_x.cli predict ...</code></li> <li><code>python -m apex_x.cli bench ...</code></li> <li><code>python -m apex_x.cli ablate ...</code></li> <li><code>python -m apex_x.cli export ...</code></li> <li><code>python -m apex_x.cli dataset-preflight ...</code></li> </ul>"},{"location":"#validation-rule-for-docs","title":"Validation rule for docs","text":"<p>User-facing claims should be backed by reproducible artifacts: - config used - commit hash - checkpoint lineage - evaluation reports</p>"},{"location":"CI_GPU/","title":"GPU CI Perf Regression","text":""},{"location":"CI_GPU/#goal","title":"Goal","text":"<p>Run GPU performance regression checks for Apex-X while keeping default CI safe and cheap on public runners.</p> <p>This workflow is: - <code>.github/workflows/perf_gpu.yml</code></p> <p>CPU perf regression remains in: - <code>.github/workflows/ci.yml</code> (<code>perf-regression</code> job)</p>"},{"location":"CI_GPU/#trigger-model","title":"Trigger Model","text":"<ul> <li>Pull request (<code>pull_request</code>, mandatory for GPU-critical paths):</li> <li>triggers when a PR changes:<ul> <li><code>apex_x/kernels/**</code></li> <li><code>apex_x/runtime/**</code></li> <li><code>runtime/tensorrt/**</code></li> </ul> </li> <li>PRs from forks are blocked by policy in this workflow (no untrusted code on self-hosted GPU)</li> <li>PRs from trusted branches in this repository execute GPU regression on self-hosted runner</li> <li>Manual (<code>workflow_dispatch</code>):</li> <li>default is <code>run_mode=skip</code> (safe default)</li> <li>set <code>run_mode=self-hosted-gpu</code> to execute benchmark on self-hosted GPU</li> <li>Scheduled (<code>schedule</code>):</li> <li>runs only when repository variable <code>APEXX_ENABLE_GPU_NIGHTLY</code> is set to <code>true</code></li> <li>intended for trusted environments with self-hosted GPU runner capacity</li> </ul>"},{"location":"CI_GPU/#runner-requirements-self-hosted","title":"Runner Requirements (Self-Hosted)","text":"<p>Recommended labels: - <code>self-hosted</code> - <code>linux</code> - <code>x64</code> - <code>gpu</code></p> <p>Minimum environment: - NVIDIA driver + CUDA runtime compatible with installed PyTorch - Python 3.11+ toolchain - enough free GPU memory for fixed benchmark profile</p>"},{"location":"CI_GPU/#baseline-and-regression-policy","title":"Baseline and Regression Policy","text":"<p>Stored baseline: - <code>scripts/perf_baseline_gpu.json</code></p> <p>Regression runner: - <code>scripts/perf_regression_gpu.py</code> - <code>scripts/perf_regression_trt.py</code> (optional TRT shape-sweep wrapper when engine path is provided)</p> <p>Compare command used in workflow: <pre><code>python scripts/perf_regression_gpu.py \\\n  --compare \\\n  --baseline scripts/perf_baseline_gpu.json \\\n  --output artifacts/perf_gpu_current_ci.json \\\n  --summary artifacts/perf_gpu_compare_ci.json \\\n  --trend-output artifacts/perf_gpu_trend_ci.json\n</code></pre></p> <p>Fail behavior: - workflow fails when any tracked metric exceeds:   - <code>allowed_max = baseline_value * (1 + max_regression_ratio) + max_regression_abs_ms</code> - workflow also fails when benchmark status is not <code>ok</code> (for example CUDA unavailable)</p> <p>Trend artifact: - <code>artifacts/perf_gpu_trend_ci.json</code> contains normalized metric deltas for weekly/per-run tracking. - release evidence draft is auto-generated:   - <code>artifacts/release/release_attestation_gpu_ci.json</code>   - <code>artifacts/release/release_attestation_gpu_ci.md</code></p>"},{"location":"CI_GPU/#baseline-maintenance","title":"Baseline Maintenance","text":"<p>Regenerate template on the target GPU runner: <pre><code>python scripts/perf_regression_gpu.py \\\n  --emit-baseline-template \\\n  --baseline scripts/perf_baseline_gpu.json\n</code></pre></p> <p>Then tighten tolerances per runner stability: - microbench metrics: lower absolute thresholds - end-to-end metrics: slightly wider p95 threshold</p>"},{"location":"CI_GPU/#optional-tensorrt-inputs","title":"Optional TensorRT Inputs","text":"<p>Workflow dispatch supports: - <code>trt_engine_path</code>: optional <code>.engine</code> for TRT end-to-end bench - <code>trt_plugin_lib</code>: optional plugin shared library path</p> <p>If omitted, TRT sections are skipped and reported as such.</p> <p>Optional TRT wrapper command (same compare formula + normalized trend artifact): <pre><code>python scripts/perf_regression_trt.py \\\n  --compare \\\n  --baseline scripts/perf_baseline_trt.json \\\n  --output artifacts/perf_trt_current_ci.json \\\n  --summary artifacts/perf_trt_compare_ci.json \\\n  --trend-output artifacts/perf_trt_trend_ci.json \\\n  --trt-engine-path /abs/path/to/apex_x.engine\n</code></pre></p>"},{"location":"CI_GPU/#required-check-setup","title":"Required Check Setup","text":"<p>To enforce merge blocking for GPU-critical changes, set branch protection to require: - <code>GPU Perf Regression / gpu-perf-regression</code></p> <p>With this policy, GPU-critical PRs cannot merge without passing GPU regression.</p>"},{"location":"CI_GPU/#security-notes","title":"Security Notes","text":"<p>Self-hosted GPU runners execute repository code and are high-trust resources.</p> <p>Recommendations: - Keep fork PRs blocked from self-hosted GPU execution (workflow has explicit guard job). - Do not use <code>pull_request_target</code> for GPU perf execution. - Use dedicated, isolated self-hosted GPU runners for CI only. - Avoid exposing production secrets to GPU perf workflow. - Keep repository permissions minimal (<code>contents: read</code> is used by default). - Pin and review third-party actions before changing workflow dependencies.</p>"},{"location":"CONTEXT/","title":"Apex-X Project Context (Persistent Memory)","text":""},{"location":"CONTEXT/#authoritative-links-mandatory","title":"Authoritative Links (Mandatory)","text":"<ul> <li>PRD: <code>docs/PRD.md</code></li> <li>Engineering spec: <code>docs/ENGINEERING_SPEC.md</code></li> <li>Runtime plugin spec: <code>docs/runtime/PLUGIN_SPEC.md</code></li> <li>Decisions log: <code>docs/DECISIONS.md</code></li> <li>Active worklist: <code>docs/TODO.md</code></li> </ul>"},{"location":"CONTEXT/#project-identity","title":"Project Identity","text":"<ul> <li>Name: <code>apex-x</code></li> <li>Version: <code>0.1.0</code></li> <li>License: <code>Apache-2.0</code></li> <li>Current baseline: CPU-only reference implementation</li> </ul>"},{"location":"CONTEXT/#recent-updates-2026-02-11","title":"Recent Updates (2026-02-11)","text":"<ul> <li>Deployment CUDA host validation snapshot was refreshed:</li> <li><code>torch.cuda.is_available() == True</code>, <code>device_count == 1</code> (<code>NVIDIA GeForce RTX 2070 SUPER</code>, <code>sm75</code>)</li> <li><code>triton</code> import available (<code>3.5.1</code>)</li> <li><code>tensorrt</code> import available (<code>10.15.1.29</code>)</li> <li><code>onnxruntime</code> import available (<code>1.24.1</code>)</li> <li>environment artifact bundle:<ul> <li><code>artifacts/env_check_cuda.json</code></li> <li><code>artifacts/env_check_cuda.md</code></li> </ul> </li> <li>Triton runtime compatibility fixes for Triton <code>3.5.x</code> were applied:</li> <li>removed <code>typing.Any</code> annotations from <code>@triton.jit</code> kernel signatures</li> <li>promoted <code>BLOCK_*</code> launch parameters to <code>tl.constexpr</code> where used by <code>tl.arange(...)</code></li> <li>fixed <code>tileunpack_triton(...)</code> overlap-mode validator to accept both <code>override</code> and <code>blend</code></li> <li>GPU parity suite now passes on CUDA host:<ul> <li><code>python -m pytest -q tests/test_triton_*_gpu.py</code></li> </ul> </li> <li>Triton TileUnpack blend fast-path was optimized with a dedicated CUDA kernel:</li> <li><code>tileunpack_triton(...)</code> now uses <code>_tileunpack_blend_update_kernel</code> for     <code>overlap_mode=\"blend\"</code> instead of Python ordered patch loop.</li> <li>added GPU coverage for blend kernel telemetry:<ul> <li><code>tests/test_triton_tileunpack_overlap_gpu.py</code></li> </ul> </li> <li>new blend microbench artifacts:<ul> <li><code>artifacts/perf_triton_tileunpack_blend.json</code></li> <li><code>artifacts/perf_triton_tileunpack_blend.md</code></li> </ul> </li> <li>Triton TileSSM long-sequence (<code>K &gt; 4096</code>) closure evidence was captured on CUDA host:</li> <li>added GPU parity test for chunked scan dispatch path:<ul> <li><code>tests/test_triton_tilessm_parity_gpu.py::test_tilessm_triton_long_sequence_chunked_parity_fp16</code></li> </ul> </li> <li>long-<code>K</code> parity artifacts:<ul> <li><code>artifacts/parity_tilessm_long_k.json</code></li> <li><code>artifacts/parity_tilessm_long_k.md</code></li> <li><code>artifacts/test_tilessm_long_k.log</code></li> </ul> </li> <li>long-<code>K</code> perf artifacts:<ul> <li><code>artifacts/perf_triton_tilessm_long_k.json</code></li> <li><code>artifacts/perf_triton_tilessm_long_k.md</code></li> </ul> </li> <li>Triton fused Stage-1 closure evidence was captured on CUDA host:</li> <li>parity suite artifacts:<ul> <li><code>artifacts/parity_triton_fused_stage1.json</code></li> <li><code>artifacts/parity_triton_fused_stage1.md</code></li> <li><code>artifacts/test_triton_fused_stage1.log</code></li> </ul> </li> <li>perf artifacts:<ul> <li><code>artifacts/perf_triton_fused_stage1.json</code></li> <li><code>artifacts/perf_triton_fused_stage1.md</code></li> </ul> </li> <li>key metric snapshot:<ul> <li><code>speedup_separate_over_fused = 2.5973x</code> at   <code>B=1, C=128, H=W=128, t=8, K=32, dtype=float16</code></li> </ul> </li> <li>CUDA perf evidence for Triton autotune registry was captured:</li> <li><code>artifacts/perf_gpu.json</code></li> <li><code>artifacts/perf_gpu.md</code></li> <li>includes <code>triton_autotune.summary</code> + <code>triton_autotune.entries</code> telemetry</li> <li>TensorRT compare baseline wiring was finalized with committed template:</li> <li><code>scripts/perf_baseline_trt.json</code></li> <li>deployment runner still required for real TensorRT metrics/baseline tuning</li> <li>Runtime environment was further unblocked on this host:</li> <li><code>tensorrt==10.15.1.29</code> (<code>tensorrt-cu12</code>), <code>onnxruntime==1.24.1</code>, <code>cmake==4.2.1</code></li> <li>CUDA compiler toolchain installed in conda base (<code>cuda-nvcc 12.8.93</code>)</li> <li>updated environment snapshot:<ul> <li><code>artifacts/env_check_cuda.json</code></li> <li><code>artifacts/env_check_cuda.md</code></li> </ul> </li> <li>TensorRT plugin native toolchain was unblocked on this host:</li> <li>TensorRT headers synced from <code>TensorRT release/10.15</code> (<code>TRT 10.15.1.29</code>)     into <code>artifacts/toolchains/tensorrt_10_15_1_29/include</code></li> <li>shared plugin library build now succeeds:<ul> <li><code>runtime/tensorrt/build_cuda_10_15/libapexx_trt_plugins.so</code></li> </ul> </li> <li>native plugin executables passed on CUDA host:<ul> <li><code>artifacts/trt_native_tests.log</code></li> </ul> </li> <li>configure/build logs:<ul> <li><code>artifacts/trt_cmake_configure.log</code></li> <li><code>artifacts/trt_cmake_build.log</code></li> </ul> </li> <li>TensorRT Python parity harness now passes on this host with real plugin <code>.so</code>:</li> <li>parity log: <code>artifacts/trt_plugin_parity_pytest.log</code></li> <li>summary bundle: <code>artifacts/trt_plugin_validation_summary.{json,md}</code></li> <li>result: all plugin parity suites pass (<code>7 passed</code>, <code>0 skipped</code>).</li> <li>TensorRT plugin shape/serialization closure evidence was refreshed:</li> <li>CMake now registers native plugin tests for <code>ctest</code> in     <code>runtime/tensorrt/CMakeLists.txt</code></li> <li><code>ctest</code> now executes native plugin tests in <code>runtime/tensorrt/build_cuda_10_15</code>:<ul> <li><code>trt_tilepack_plugin_native</code></li> <li><code>trt_tileunpackfusion_plugin_native</code></li> <li><code>trt_tilessm_plugin_native</code></li> <li><code>trt_nms_decode_plugin_native</code></li> </ul> </li> <li>plugin parity suite shape coverage expanded with multiple shape cases:<ul> <li><code>tests/test_tensorrt_tilepack_parity.py</code></li> <li><code>tests/test_tensorrt_tilessm_parity.py</code></li> <li><code>tests/test_tensorrt_tileunpackfusion_parity.py</code></li> </ul> </li> <li>validation artifacts:<ul> <li><code>artifacts/trt_plugins_ctest_cuda_10_15.log</code></li> <li><code>artifacts/trt_plugins_native_exec.log</code></li> <li><code>artifacts/trt_plugins_pytest.log</code></li> <li><code>artifacts/trt_plugin_shape_serialization_validation.json</code></li> <li><code>artifacts/trt_plugin_shape_serialization_validation.md</code></li> </ul> </li> <li>TensorRT parity matrix coverage on CUDA host was expanded:</li> <li>TilePack parity now enforces backend matrix checks:<ul> <li>reference vs triton</li> <li>reference vs tensorrt</li> <li>triton vs tensorrt</li> </ul> </li> <li>TileSSM parity now enforces backend matrix checks:<ul> <li>reference vs triton</li> <li>reference vs tensorrt</li> <li>triton vs tensorrt</li> </ul> </li> <li>updated tests:<ul> <li><code>tests/test_tensorrt_tilepack_parity.py</code></li> <li><code>tests/test_tensorrt_tilessm_parity.py</code></li> </ul> </li> <li>Local TensorRT shape-sweep and compare/trend artifacts were captured using a real CUDA engine:</li> <li>engine artifact: <code>artifacts/models/trt_bench_dynamic.engine</code></li> <li>shape sweep: <code>artifacts/perf_trt_shape_sweep.json</code>, <code>artifacts/perf_trt_shape_sweep.md</code></li> <li>compare/trend: <code>artifacts/perf_trt_current.json</code>, <code>artifacts/perf_trt_compare.json</code>, <code>artifacts/perf_trt_trend.json</code></li> <li>local rerun refreshed on-host evidence at <code>2026-02-11T13:51:37Z</code> / <code>13:51:52Z</code> (still synthetic engine)</li> <li>deployment-engine blocker logs were refreshed:<ul> <li><code>artifacts/trt_shape_sweep_deployment_blocker.log</code></li> <li><code>artifacts/trt_regression_deployment_blocker.log</code></li> </ul> </li> <li>Service bridge TensorRT input-shape handling was hardened for dynamic engines:</li> <li><code>apex_x/runtime/service_bridge.py</code> now resolves engine input shapes and infers dynamic dimensions from flat input payloads</li> <li>coverage expanded in <code>tests/test_runtime_service_bridge.py</code></li> <li>real TensorRT bridge probe artifact:<ul> <li><code>artifacts/service_bridge_trt_real_engine.json</code></li> </ul> </li> <li>Go runtime bridge validation on host:</li> <li>ORT bridge with real ONNX model:<ul> <li>model: <code>artifacts/models/bridge_test_model.onnx</code></li> <li>logs: <code>artifacts/go_ort_bridge_test_real_model.log</code></li> <li>probe: <code>artifacts/service_bridge_ort_real_model.json</code></li> </ul> </li> <li>TRT bridge with real TensorRT engine:<ul> <li>logs: <code>artifacts/go_trt_bridge_test_real_engine.log</code></li> <li>probe: <code>artifacts/service_bridge_trt_real_engine.json</code></li> <li>shape-profile diagnostic: <code>artifacts/service_bridge_trt_real_engine.raw.log</code></li> </ul> </li> <li>local rerun refreshed artifacts with valid and invalid-shape probes (<code>2026-02-11T13:56Z</code>)</li> <li>deployment-engine blocker log refreshed:<ul> <li><code>artifacts/go_trt_bridge_deployment_blocker.log</code></li> </ul> </li> <li>GitHub CLI status on host:</li> <li><code>gh</code> installed (<code>2.86.0</code>)</li> <li>auth missing (<code>gh auth status</code> fails; requires <code>GH_TOKEN</code> or <code>gh auth login</code>)</li> <li>branch-protection API blocker evidence:<ul> <li><code>artifacts/github_branch_protection_blocker.log</code></li> </ul> </li> <li>weekly GPU workflow dispatch blocker evidence:<ul> <li><code>artifacts/github_weekly_gpu_blocker.log</code></li> </ul> </li> <li>FP8 request-path evidence was refreshed on deployment host:</li> <li>benchmark artifacts:<ul> <li><code>artifacts/perf_gpu_fp8.json</code></li> <li><code>artifacts/perf_gpu_fp8.md</code></li> </ul> </li> <li>compare artifacts:<ul> <li><code>artifacts/perf_gpu_fp8_current.json</code></li> <li><code>artifacts/perf_gpu_fp8_compare.json</code></li> </ul> </li> <li>blocker log:<ul> <li><code>artifacts/fp8_sm90_blocker.log</code></li> </ul> </li> <li> <p>effective precision remains FP16 with reason <code>compute_capability_below_sm90</code> on <code>sm75</code>.</p> </li> <li> <p>Runtime capability contract was hardened:</p> </li> <li>canonical reason-code catalog added in <code>apex_x/runtime/caps.py</code></li> <li>dynamic reason strings were removed in favor of stable reason codes</li> <li>runtime reason catalog exposed via <code>runtime_reason_catalog()</code></li> <li>Capability docs/spec alignment updates:</li> <li><code>docs/runtime/CAPS.md</code> now defines frozen backend capability matrix and reason-code contract</li> <li><code>docs/ENGINEERING_SPEC.md</code> includes explicit runtime capability and parity contract section</li> <li><code>docs/PRD.md</code> adds FR-14 for runtime capability transparency</li> <li>Parity tolerance framework was extended:</li> <li>FP8 tolerance added to <code>ToleranceConfig</code></li> <li>profile API added (<code>quality</code>, <code>balanced</code>, <code>edge</code>) with op/e2e tolerance bundles</li> <li>new tests added in <code>tests/test_runtime_parity.py</code></li> <li>Deterministic replay package was added:</li> <li>golden fixtures: <code>tests/fixtures/replay_golden_small.json</code>, <code>tests/fixtures/replay_golden_medium.json</code></li> <li>replay hash utilities in <code>apex_x/utils/repro.py</code> (<code>build_replay_manifest</code>, JSON/file SHA256 helpers)</li> <li>replay validation test in <code>tests/test_replay_golden.py</code></li> <li>CLI backend selection contract was added:</li> <li><code>--backend cpu|torch|triton|tensorrt</code></li> <li><code>--fallback-policy strict|permissive</code></li> <li>strict mode returns actionable errors; permissive mode falls back deterministically</li> <li>backend selection metadata is emitted by <code>predict</code> and <code>eval</code></li> <li>TODO progression:</li> <li>completed tasks <code>P0-01</code>, <code>P0-02</code>, <code>P0-03</code>, and <code>P1-01</code> were removed from <code>docs/TODO.md</code></li> <li>completed task <code>P2-04</code> was removed from active backlog and moved to completed history</li> <li>CI/perf gate hardening updates:</li> <li>GPU perf workflow now has PR path trigger for GPU-critical code paths:<ul> <li><code>apex_x/kernels/**</code></li> <li><code>apex_x/runtime/**</code></li> <li><code>runtime/tensorrt/**</code></li> </ul> </li> <li>fork PRs are fail-closed for GPU workflow execution (<code>blocked-untrusted-pr</code>)</li> <li>CPU and GPU perf regression scripts now emit normalized trend artifacts:<ul> <li>CPU: <code>--trend-output artifacts/perf_trend_cpu_ci.json</code></li> <li>GPU: <code>--trend-output artifacts/perf_gpu_trend_ci.json</code></li> </ul> </li> <li>weekly trend workflow added:<ul> <li><code>.github/workflows/perf_trend_weekly.yml</code></li> <li>CPU weekly trend always runs</li> <li>GPU weekly trend runs on self-hosted CUDA when <code>APEXX_ENABLE_GPU_WEEKLY=true</code></li> </ul> </li> <li>workflow policy contract test added:<ul> <li><code>tests/test_gpu_ci_workflow_contract.py</code></li> <li>guards GPU-critical PR triggers + trusted self-hosted execution rules</li> </ul> </li> <li>Release checklist evidence automation was added:</li> <li>new generator script: <code>scripts/release_attestation.py</code></li> <li>outputs JSON + Markdown attestation bundles with artifact SHA256/status fields</li> <li>CI workflows now auto-publish release evidence drafts:<ul> <li>CPU CI: <code>artifacts/release/release_attestation_ci.{json,md}</code></li> <li>GPU CI: <code>artifacts/release/release_attestation_gpu_ci.{json,md}</code></li> <li>weekly trend CPU/GPU jobs publish corresponding weekly attestation bundles</li> </ul> </li> <li>Backward-compatibility migration documentation was finalized:</li> <li>migration guide: <code>docs/release/MIGRATION.md</code></li> <li>project changelog baseline: <code>CHANGELOG.md</code></li> <li>X-03 was removed from active queue after adding explicit deprecation timeline and migration actions.</li> <li>Temporal hysteresis quality gates were expanded:</li> <li>new budget-aware update API: <code>hysteresis_update_with_budget(...)</code></li> <li><code>hysteresis_rollout(...)</code> now supports optional <code>max_active</code> frame cap</li> <li>new stability metrics:<ul> <li><code>tile_flip_rate(...)</code></li> <li><code>temporal_consistency(...)</code></li> <li><code>mean_active_ratio(...)</code></li> <li><code>summarize_temporal_stability(...)</code></li> </ul> </li> <li>model CPU baseline now applies budget-aware hysteresis using <code>kmax_l0</code></li> <li>new sequence-level tests in <code>tests/test_temporal_hysteresis_metrics.py</code></li> <li>Quadtree recursion budgeting was extended to depth-2 deterministic selection:</li> <li>new API: <code>deterministic_three_stage_selection(...)</code> in <code>apex_x/routing/inference_budget.py</code></li> <li>stage contracts:<ul> <li><code>L0</code> under <code>B1</code></li> <li><code>L0 -&gt; L1</code> split under <code>B2</code></li> <li><code>L1 -&gt; L2</code> split under <code>B3</code></li> </ul> </li> <li>deterministic parent tie-break at split stages: score desc, tile-id asc</li> <li><code>Kmax_L1</code>/<code>Kmax_L2</code> capacity limits enforced during child expansion</li> <li>new tests: <code>tests/test_three_stage_selection.py</code></li> <li>Deterministic inference-budget stress coverage was expanded:</li> <li>new stress suite: <code>tests/test_inference_budget_stress.py</code></li> <li>covers:<ul> <li>equal-utility tie scaling</li> <li>zero/near-zero delta-cost stability</li> <li>saturated <code>Kmax</code> clipping</li> <li>adversarial close-score repeatability</li> </ul> </li> <li>Continuous dual-budget convergence controls were hardened:</li> <li><code>BudgetDualController</code> now supports adaptive update schedule with:<ul> <li>step decay</li> <li>EMA-scaled learning-rate modulation</li> <li>deadband near target budget</li> <li>optional delta clipping</li> </ul> </li> <li>trainer/model wiring now passes dual schedule config fields from <code>TrainConfig</code></li> <li>stage-3 trainer metrics now report dual dynamics:<ul> <li><code>dual_effective_lr_last</code></li> <li><code>dual_error_ema_last</code></li> <li><code>dual_update_count</code></li> </ul> </li> <li>new convergence test suite: <code>tests/test_dual_budget_convergence.py</code></li> <li>PCGrad++ monitoring was completed for shared-trunk training:</li> <li><code>apply_pcgradpp(...)</code> diagnostics now include conflict metrics before/after projection</li> <li>trainer emits <code>train_summary[\"pcgrad\"]</code> payload with:<ul> <li>pair counts/rates (<code>before</code> vs <code>after</code>)</li> <li>shared/head parameter counts</li> <li>gradient norm snapshots</li> </ul> </li> <li>head-gradient non-projection and conflict-rate behavior are covered in:<ul> <li><code>tests/test_pcgradpp.py</code></li> <li><code>tests/test_trainer_stages.py</code></li> </ul> </li> <li>FP8 operational telemetry was extended:</li> <li><code>precision.py</code> fallback reasons now align to canonical runtime reason-codes</li> <li>GPU benchmark now supports explicit FP8 request mode (<code>--dtype fp8</code>)</li> <li>benchmark report now includes requested-vs-effective precision fields:<ul> <li><code>requested_dtype</code></li> <li><code>effective_dtype</code></li> <li><code>fp8_requested</code></li> <li><code>fp8_enabled</code></li> <li><code>fp8_fallback_reason</code></li> </ul> </li> <li>FP8 telemetry coverage added in:<ul> <li><code>tests/test_precision_policy.py</code></li> <li><code>tests/test_gpu_bench_fp8.py</code></li> </ul> </li> <li>Unified perf regression policy was extended to TensorRT shape-sweep:</li> <li>new script: <code>scripts/perf_regression_trt.py</code></li> <li>new baseline: <code>scripts/perf_baseline_trt.json</code></li> <li>trend artifacts:<ul> <li><code>artifacts/perf_trt_trend_ci.json</code></li> <li><code>artifacts/perf_trt_trend_weekly.json</code></li> </ul> </li> <li>GPU workflows now run optional TRT compare/trend when <code>TRT_ENGINE_PATH</code> is provided.</li> <li>Oracle supervision pipeline was hardened:</li> <li>oracle sampler now supports a third component for long-tail tile selection</li> <li>stage-2 trainer now logs/reports oracle label diagnostics:<ul> <li>sample composition (<code>random</code>, <code>uncertainty</code>, <code>long_tail</code>)</li> <li>delta distribution summary (<code>mean/std/min/max/abs_p95</code>)</li> <li>clipping diagnostics (<code>clipped_ratio</code>)</li> </ul> </li> <li>new stats helper added: <code>summarize_oracle_delta_targets(...)</code></li> <li>coverage expanded in:<ul> <li><code>tests/test_oracle_sampling.py</code></li> <li><code>tests/test_oracle_distill.py</code></li> <li><code>tests/test_trainer_stages.py</code></li> </ul> </li> <li>Export and predict runtime paths reached active-queue completion status:</li> <li><code>P1-02</code> closure validated by export contract tests:<ul> <li><code>tests/test_export.py</code></li> <li><code>tests/test_tensorrt_export_manifest.py</code></li> </ul> </li> <li><code>P1-03</code> closure validated by runner/CLI backend execution tests:<ul> <li><code>tests/test_infer_runner.py</code></li> <li><code>tests/test_cli.py</code></li> </ul> </li> <li>deployment-host evidence for real TensorRT engines remains tracked in device-blocked queue.</li> <li>Eval runtime path reached active-queue completion status:</li> <li><code>P1-04</code> closure validated by eval + dataset execution contract tests:<ul> <li><code>tests/test_eval_metrics.py</code></li> <li><code>tests/test_infer_runner.py</code></li> <li><code>tests/test_cli.py</code></li> </ul> </li> <li>TensorRT INT8 sensitive-layer precision enforcement was hardened:</li> <li>builder now emits per-layer precision evidence in <code>EngineBuildResult.layer_precision_status</code></li> <li>strict precision-constraint mode now fails build when matched layers cannot be constrained</li> <li>new coverage added in <code>tests/test_tensorrt_precision_policy.py</code></li> <li>TensorRT plugin build-time contract validation was hardened:</li> <li>builder now validates plugin creator contracts for:<ul> <li>presence</li> <li>version</li> <li>namespace</li> <li>plugin field-signature metadata</li> </ul> </li> <li>strict mode emits actionable mismatch errors for required plugins</li> <li><code>PluginContract</code> overrides are now supported in build config</li> <li>new non-CUDA unit coverage:<ul> <li><code>tests/test_tensorrt_plugin_contracts.py</code></li> </ul> </li> <li>TensorRT INT8 calibration cache governance was completed:</li> <li>cache-key contract now binds calibration cache reuse to:<ul> <li>model/export identity hash</li> <li>plugin version/namespace metadata</li> <li>precision profile</li> <li>calibration dataset version (explicit or auto-digest)</li> </ul> </li> <li>calibrator cache blob now enforces key-aware stale-cache invalidation.</li> <li>legacy raw cache blobs are accepted only when key governance is disabled.</li> <li>new non-CUDA unit coverage:<ul> <li><code>tests/test_tensorrt_int8_cache.py</code></li> </ul> </li> <li>TensorRT parity harness was expanded with backend matrix + sweep APIs:</li> <li><code>ParityMatrixCase</code> and <code>run_parity_matrix_case(...)</code> compare:<ul> <li>reference vs triton</li> <li>reference vs tensorrt</li> <li>triton vs tensorrt</li> </ul> </li> <li><code>run_parity_sweep(...)</code> provides shape/precision sweep aggregation with profile-aware tolerances.</li> <li>CPU-safe harness contract coverage added in:<ul> <li><code>tests/test_trt_parity_harness.py</code></li> </ul> </li> <li>Triton TileSSM long-sequence behavior was hardened:</li> <li>Triton forward scan now streams long sequences in chunks when <code>K &gt; 4096</code>.</li> <li>chunk execution carries recurrent state between launches to preserve recurrence semantics.</li> <li>CPU-safe chunking contract tests added in:<ul> <li><code>tests/test_triton_tilessm_parity_dispatch.py</code></li> </ul> </li> <li>Triton TileUnpack overlap blend dispatch gap was reduced:</li> <li><code>overlap_mode=\\\"blend\\\"</code> no longer has a forced reference-only dispatch branch.</li> <li>blend overlap path now executes through <code>tileunpack_triton(...)</code> entrypoint with     ordered composition parity semantics.</li> <li>overlap tests expanded:<ul> <li><code>tests/test_triton_tileunpack_overlap_dispatch.py</code></li> <li><code>tests/test_triton_tileunpack_overlap_gpu.py</code></li> </ul> </li> <li>Triton Stage-1 fused selector was integrated into FF heavy-path inference:</li> <li><code>FFHeavyPath</code> now routes to <code>fused_pack_op_unpack_dispatch(...)</code> only when strict     compatibility predicates hold (eval, identity refine, constant FiLM params, unique indices).</li> <li>non-compatible cases remain on deterministic decomposed <code>pack -&gt; FiLM -&gt; unpack</code> path.</li> <li>new coverage:<ul> <li><code>tests/test_ff_heavy_path_fused_stage1.py</code></li> </ul> </li> <li>Triton autotune registry + benchmark telemetry were added:</li> <li>new module: <code>apex_x/kernels/triton/autotune_registry.py</code></li> <li>registry records per-op/per-shape-bucket selected launch config and cache counters</li> <li>instrumented kernels:<ul> <li>TilePack</li> <li>TileUnpack priority/scatter</li> <li>FusionGate alpha/fuse</li> <li>fused stage-1 pack/op/unpack</li> </ul> </li> <li>GPU benchmark report now exports:<ul> <li><code>triton_autotune.summary</code></li> <li><code>triton_autotune.entries</code></li> </ul> </li> <li>CPU-safe contract coverage added in:<ul> <li><code>tests/test_triton_autotune_registry.py</code></li> </ul> </li> <li>Runtime docs consistency sweep completed:</li> <li>synchronized current implementation status in:<ul> <li><code>docs/runtime/TRITON.md</code></li> <li><code>docs/runtime/TENSORRT.md</code></li> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> </ul> </li> <li>Release readiness docs were strengthened:</li> <li>added <code>docs/release/CHECKLIST.md</code> with mandatory artifact attestation + rollback section</li> <li>linked checklist in <code>README.md</code>, docs index, and MkDocs navigation</li> <li>Go runtime telemetry schema was aligned with Python CLI/runtime reports:</li> <li><code>/predict</code> response now includes <code>runtime</code> payload with backend selection, fallback, precision, and latency breakdown</li> <li>latency keys aligned to <code>latency_ms.total</code>, <code>latency_ms.backend_execute</code>, <code>latency_ms.backend_preflight</code></li> <li>service test coverage updated for runtime telemetry schema</li> <li>explicit SLA-oriented error policies added:<ul> <li>queue saturation returns <code>429 Too Many Requests</code></li> <li>predict timeout returns <code>504 Gateway Timeout</code></li> </ul> </li> <li>Go runtime backend bridge execution path was added:</li> <li>ORT and TRT adapters now support optional Python bridge execution via:<ul> <li><code>APEXX_ORT_BRIDGE_CMD</code></li> <li><code>APEXX_TRT_BRIDGE_CMD</code></li> </ul> </li> <li>bridge protocol entrypoint implemented at <code>apex_x/runtime/service_bridge.py</code></li> <li>Go service backend error classification now maps:<ul> <li>backend unavailable -&gt; <code>503 Service Unavailable</code></li> <li>backend inference/protocol failure -&gt; <code>502 Bad Gateway</code></li> </ul> </li> <li>synthetic score fallback was removed from ORT/TRT adapters (fail-closed behavior)</li> <li>native host limitation snapshot:<ul> <li>Go <code>onnxruntime</code> native pkg-config package is unavailable on this host</li> <li>TensorRT Python module is unavailable on this host</li> </ul> </li> <li>Go runtime canary parity mode was added:</li> <li>optional shadow adapter execution with configurable sample rate</li> <li>mismatch telemetry counters (<code>samples</code>, <code>compares</code>, <code>mismatches</code>, <code>errors</code>, <code>mismatch_ratio</code>)</li> <li>canary behavior is asynchronous to avoid impacting primary response path</li> <li>configurable canary payload capture policy and storage controls were added:<ul> <li>policy: <code>off|mismatch|error|all</code></li> <li>JSONL sink: <code>APEXX_CANARY_CAPTURE_PATH</code></li> <li>file size guard: <code>APEXX_CANARY_CAPTURE_MAX_BYTES</code></li> </ul> </li> <li>SLA gate test was added for timeout/overflow rates and canary overhead:<ul> <li><code>TestCanaryLoadGateThresholds</code> in <code>runtime/go/internal/service/sla_gate_test.go</code></li> <li>CI wiring added in <code>.github/workflows/ci.yml</code> (<code>go-runtime</code> job)</li> </ul> </li> <li>Inference runner abstraction was added for CLI predict/eval:</li> <li>new module <code>apex_x/infer/runner.py</code> introduces:<ul> <li><code>run_model_inference(...)</code> with a common inference result schema</li> <li><code>RuntimeMetadata</code> and <code>InferenceRunResult</code> dataclasses</li> <li><code>extract_routing_diagnostics(...)</code> (replacing ad-hoc placeholder extraction)</li> <li>FFModule-based <code>torch</code> executor path for backend-specific execution</li> <li>triton execution branch with capability checks and deterministic fallback behavior</li> </ul> </li> <li><code>apex_x/cli.py</code> now uses the runner for both <code>predict</code> and <code>eval</code></li> <li><code>predict</code> now supports <code>--report-json</code> with runtime metadata + routing diagnostics payload</li> <li>TensorRT branch now performs preflight validation:<ul> <li>capability gate (<code>cuda</code> + TensorRT Python)</li> <li>env-driven artifact checks (<code>APEXX_EXPORT_MANIFEST_PATH</code>, <code>APEXX_TRT_ENGINE_PATH</code>)</li> <li>deterministic fallback/error reasons when runtime execution is unavailable</li> </ul> </li> <li>TensorRT runtime execution path is now implemented:<ul> <li>new <code>TensorRTEngineExecutor</code> in <code>apex_x/runtime/tensorrt/executor.py</code></li> <li>runner executes real TensorRT serialized engine inference when <code>APEXX_TRT_ENGINE_PATH</code> is set</li> <li>optional runtime env controls:</li> <li><code>APEXX_TRT_PLUGIN_LIB</code> for plugin shared libraries</li> <li><code>APEXX_TRT_INPUT_NAME</code> for explicit input tensor selection</li> <li><code>APEXX_TRT_PRIMARY_OUTPUT_NAME</code> for primary output mapping in CLI result schema</li> <li><code>APEXX_TRT_EXTRA_INPUTS_NPZ</code> for named auxiliary tensors in multi-input TRT engines</li> <li><code>APEXX_TRT_DET_BOXES_NAME</code>, <code>APEXX_TRT_DET_SCORES_NAME</code>,     <code>APEXX_TRT_DET_CLASS_IDS_NAME</code>, <code>APEXX_TRT_DET_VALID_NAME</code>     for explicit DET output binding into CLI det schema</li> </ul> </li> <li>TensorRT deployment shape-sweep harness was added:<ul> <li><code>apex_x/bench/trt_engine_sweep.py</code></li> <li>supports repeated shape cases for single/multi-input engines</li> <li>emits JSON + Markdown sweep summary for per-shape TRT runtime evidence</li> </ul> </li> <li>runtime metadata now explicitly includes:<ul> <li>requested backend</li> <li>selected backend</li> <li>actual execution backend</li> <li>precision profile</li> <li>selection/execution fallback reasons</li> <li>latency breakdown (<code>total</code>, <code>backend_execute</code>, <code>backend_preflight</code>)</li> <li>runtime capability snapshot</li> </ul> </li> <li>Eval dataset adapter path was added:</li> <li><code>apex_x/infer/runner.py</code> now includes:<ul> <li><code>load_eval_images_npz(...)</code></li> <li><code>load_eval_dataset_npz(...)</code></li> <li><code>evaluate_model_dataset(...)</code></li> <li><code>ModelDatasetEvalSummary</code></li> </ul> </li> <li><code>apex_x/cli.py eval</code> now accepts:<ul> <li><code>--dataset-npz</code> for <code>.npz/.npy</code> image arrays</li> <li><code>--max-samples</code> for deterministic subset evaluation</li> </ul> </li> <li>when dataset eval is enabled, reports include <code>model_eval</code> aggregates in JSON/Markdown</li> <li>optional dataset target contract:<ul> <li><code>.npz</code> key <code>det_score_target</code> (or compat alias <code>det_scores_target</code>)</li> <li><code>.npz</code> key <code>selected_tiles_target</code> (or compat alias <code>selected_tiles_targets</code>)</li> <li>model-eval report now includes target regression metrics:</li> <li><code>det_score_target</code>: <code>mae</code>, <code>rmse</code>, <code>bias</code>, <code>r2</code>, <code>pearson_corr</code></li> <li><code>selected_tiles_target</code>: <code>mae</code>, <code>rmse</code>, <code>bias</code>, <code>exact_match_rate</code></li> </ul> </li> <li>eval JSON reports now always include <code>runtime</code> metadata for backend/precision/fallback traceability</li> <li>Export/TRT handoff integration was added:</li> <li><code>apex_x/runtime/tensorrt/builder.py</code> now provides:<ul> <li><code>load_export_manifest(...)</code></li> <li><code>TensorRTEngineBuilder.build_from_export_manifest(...)</code></li> </ul> </li> <li>manifest loader validates ONNX path and optional SHA256 integrity before build handoff</li> </ul>"},{"location":"CONTEXT/#current-architecture-snapshot","title":"Current Architecture Snapshot","text":"<ul> <li>Dual-stream concept established in docs (PV dense + FF sparse)</li> <li>Utility-based router contracts defined</li> <li>Continuous and deterministic budgeting contracts defined</li> <li>Quadtree nesting policy defined (<code>L0/L1/L2</code>)</li> <li>TilePack/TileUnpack and ordering contracts defined</li> <li>Tile-SSM placeholder behavior defined</li> </ul>"},{"location":"CONTEXT/#what-exists-right-now-2026-02-07","title":"What Exists Right Now (2026-02-07)","text":"<ul> <li>Repository scaffold created:</li> <li><code>apex_x/</code>, <code>tests/</code>, <code>docs/</code>, <code>docs/runtime/</code>, <code>examples/</code>, <code>scripts/</code>, <code>runtime/</code>, <code>.github/workflows/</code></li> <li>Governance/Open-source files added:</li> <li><code>LICENSE</code>, <code>CODE_OF_CONDUCT.md</code>, <code>CONTRIBUTING.md</code>, <code>SECURITY.md</code></li> <li>Authoritative docs added/updated:</li> <li><code>docs/PRD.md</code> (full)</li> <li><code>docs/ENGINEERING_SPEC.md</code> (full)</li> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> <li>CPU baseline code added:</li> <li><code>apex_x/config/schema.py</code></li> <li><code>apex_x/routing/core.py</code></li> <li><code>apex_x/tiles/ops.py</code></li> <li><code>apex_x/utils/ssm.py</code></li> <li><code>apex_x/model/core.py</code></li> <li>Validation assets added:</li> <li><code>tests/test_router.py</code></li> <li><code>tests/test_tile_ops.py</code></li> <li><code>tests/test_model.py</code></li> <li><code>scripts/perf_regression.py</code></li> <li><code>.github/workflows/ci.yml</code></li> <li>Tooling and developer workflow baseline:</li> <li><code>pyproject.toml</code> now targets <code>python&gt;=3.11</code></li> <li>runtime deps: <code>torch</code>, <code>numpy</code>, <code>typer</code>, <code>rich</code>, <code>pydantic</code></li> <li>dev deps/tools: <code>pytest</code>, <code>ruff</code>, <code>black</code>, <code>mypy</code>, <code>pre-commit</code></li> <li>pre-commit hooks: <code>.pre-commit-config.yaml</code></li> <li>CI now runs lint + typecheck + tests on <code>ubuntu-latest</code> with CPU torch index</li> <li>Package skeleton and public API surfaces:</li> <li>Created package layout: <code>apex_x/config/</code>, <code>apex_x/model/</code>, <code>apex_x/tiles/</code>, <code>apex_x/routing/</code>, <code>apex_x/losses/</code>, <code>apex_x/train/</code>, <code>apex_x/infer/</code>, <code>apex_x/data/</code>, <code>apex_x/export/</code>, <code>apex_x/bench/</code>, <code>apex_x/runtime/</code>, <code>apex_x/utils/</code></li> <li>Root API now exports required surfaces in <code>apex_x/__init__.py</code>:<ul> <li><code>ApexXConfig</code>, <code>ApexXModel</code></li> <li><code>Router</code>, <code>BudgetController</code></li> <li><code>TilePack</code>, <code>TileUnpack</code></li> <li><code>Exporter</code></li> </ul> </li> <li>Added import smoke coverage in <code>tests/test_import_smoke.py</code></li> <li>Migrated baseline code into package modules and removed legacy flat modules to avoid namespace ambiguity</li> <li>Nested configuration system implemented:</li> <li>New nested config domains in <code>apex_x/config/schema.py</code>:<ul> <li><code>ModelConfig</code> (profiles, channels, strides, tile sizes, Kmax, nesting depth)</li> <li><code>RoutingConfig</code> (budgets B/B1/B2/B3, costs, hysteresis/split thresholds)</li> <li><code>TrainConfig</code> (curriculum, dual-<code>mu</code> parameters, distill weights, PCGrad++, QAT toggles)</li> <li><code>DataConfig</code> (COCO paths and augmentation knobs)</li> <li><code>RuntimeConfig</code> (precision profile, export/runtime toggles)</li> </ul> </li> <li>Top-level <code>ApexXConfig</code> now nests all sections and performs cross-section validation</li> <li>Added YAML + CLI-style override support in <code>apex_x/config/io.py</code>:<ul> <li><code>load_yaml_config(path, overrides=...)</code></li> <li><code>apply_overrides(cfg, [\\\"section.key=value\\\", ...])</code></li> </ul> </li> <li>Added config validation test coverage in <code>tests/test_config.py</code> + fixture <code>tests/fixtures/apex_x_config.yaml</code></li> <li>Updated model to consume nested config fields in <code>apex_x/model/core.py</code></li> <li>Added <code>PyYAML</code> + <code>types-PyYAML</code> to project dependencies for runtime + typing support</li> <li>Reproducibility and logging utilities implemented:</li> <li>Added <code>apex_x/utils/repro.py</code>:<ul> <li><code>seed_all()</code></li> <li><code>set_deterministic_mode()</code></li> <li><code>deterministic_mode()</code> context manager</li> <li><code>get_determinism_state()</code></li> <li><code>reproducibility_notes()</code> (CPU vs CUDA behavior notes)</li> </ul> </li> <li>Added <code>apex_x/utils/logging.py</code>:<ul> <li><code>configure_logging()</code></li> <li><code>get_logger()</code> shared <code>apex_x.*</code> logger namespace</li> <li><code>log_event()</code> structured key/value logging with <code>rich</code></li> </ul> </li> <li>Wired shared logger usage in:<ul> <li><code>apex_x/config/io.py</code></li> <li><code>apex_x/model/core.py</code></li> </ul> </li> <li>Added determinism tests in <code>tests/test_repro.py</code></li> <li>CLI surface implemented:</li> <li>Added Typer CLI entrypoint in <code>apex_x/cli.py</code> with commands:<ul> <li><code>apex-x train</code></li> <li><code>apex-x eval</code></li> <li><code>apex-x predict</code></li> <li><code>apex-x bench</code></li> <li><code>apex-x ablate</code></li> <li><code>apex-x export</code></li> </ul> </li> <li>All commands load config via YAML and support repeated <code>--set section.key=value</code> overrides</li> <li>Added console script entrypoint in <code>pyproject.toml</code>:<ul> <li><code>[project.scripts] apex-x = \\\"apex_x.cli:main\\\"</code></li> </ul> </li> <li>Added CLI parsing/behavior tests in <code>tests/test_cli.py</code></li> <li>Documentation scaffold implemented:</li> <li>Added MkDocs config in <code>mkdocs.yml</code></li> <li>Added docs home page in <code>docs/index.md</code> linking PRD/spec/runtime/context/decisions/TODO</li> <li>Added docs build instructions in <code>docs/index.md</code> and <code>README.md</code></li> <li>Added docs dependency group in <code>pyproject.toml</code>:<ul> <li><code>.[docs]</code> with <code>mkdocs</code></li> </ul> </li> <li>Added CI docs build job in <code>.github/workflows/ci.yml</code> running:<ul> <li><code>mkdocs build --strict</code></li> </ul> </li> <li>Protocol typing standardization implemented:</li> <li>Added explicit protocol names and aliases for consistency:<ul> <li><code>RouterProtocol</code></li> <li><code>BudgetControllerProtocol</code></li> <li><code>TilePackerProtocol</code></li> <li><code>RuntimeAdapterProtocol</code></li> </ul> </li> <li>Kept backward-compatible aliases in existing modules (<code>Router</code>, <code>BudgetController</code>, <code>TilePack</code>, etc.)</li> <li>Added runtime adapter interface + reference adapter:<ul> <li><code>apex_x/runtime/interfaces.py</code></li> <li><code>apex_x/runtime/adapters.py</code> (<code>NullRuntimeAdapter</code>)</li> </ul> </li> <li>Updated model typing to consume protocol-based interfaces in <code>apex_x/model/core.py</code></li> <li>Added minimal protocol-conformance tests:<ul> <li><code>tests/test_protocols.py</code></li> </ul> </li> <li>Updated import-smoke expectations in <code>tests/test_import_smoke.py</code></li> <li>CPU smoke example added:</li> <li>Added <code>examples/smoke_cpu.py</code> that:<ul> <li>loads YAML config</li> <li>instantiates <code>ApexXModel</code> stub</li> <li>runs one forward pass on random input</li> </ul> </li> <li>Added <code>examples/smoke_cpu.yaml</code> default config for fast CPU smoke runs</li> <li>Added <code>tests/test_smoke_cpu_example.py</code> as a quick smoke pytest</li> <li>Documentation governance updates:</li> <li>Added initial convention ADRs in <code>docs/DECISIONS.md</code> for:<ul> <li>naming conventions</li> <li>tensor shape contracts</li> <li>determinism rules</li> </ul> </li> <li>Expanded <code>docs/TODO.md</code> with known future implementation tracks:<ul> <li>full Triton fused kernels</li> <li>full TensorRT plugin stack</li> <li>ONNX Runtime custom-op sparse path and parity gates</li> </ul> </li> <li>Strengthened <code>CONTRIBUTING.md</code> policy to require:<ul> <li><code>docs/CONTEXT.md</code> update in every significant PR</li> <li><code>docs/DECISIONS.md</code> update when architectural/convention decisions change</li> </ul> </li> <li>L0 tiling mapping implemented and validated:</li> <li>Added <code>apex_x/tiles/mapping.py</code> with explicit L0 mapping API:<ul> <li><code>l0_grid_shape(feature_h, feature_w, tile_size)</code> with strict divisibility checks</li> <li><code>l0_tile_to_index(ty, tx, grid_h, grid_w)</code> and <code>l0_index_to_tile(index, grid_h, grid_w)</code> with bounds checks</li> <li>batched helpers:</li> <li><code>l0_indices_to_coords(indices[B,K], grid_h, grid_w) -&gt; coords[B,K,2]</code></li> <li><code>l0_coords_to_indices(coords[B,K,2], grid_h, grid_w) -&gt; indices[B,K]</code></li> </ul> </li> <li>Wired tile ops grid sizing to strict mapping validation:<ul> <li><code>apex_x/tiles/ops.py::tile_grid_shape</code> now uses <code>l0_grid_shape(...)</code></li> </ul> </li> <li>Exported mapping API through <code>apex_x/tiles/__init__.py</code></li> <li>Added focused tests in <code>tests/test_tile_mapping.py</code>:<ul> <li>index/coord bijection</li> <li>batched <code>[B,K]</code> roundtrip</li> <li>invalid size/divisibility</li> <li>out-of-bounds and shape/dtype validation</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>Hilbert ordering implemented for coordinates and indices:</li> <li>Added <code>apex_x/tiles/ordering.py</code> with explicit Hilbert APIs:<ul> <li><code>hilbert_distance(tx, ty, order_n)</code></li> <li><code>hilbert_order_coords(grid_h, grid_w)</code> for full-grid coordinate traversal</li> <li><code>hilbert_order_indices(indices, grid_h, grid_w)</code> for subset index ordering</li> <li><code>hilbert_full_indices(grid_h, grid_w)</code> for complete index traversal</li> </ul> </li> <li>Updated <code>apex_x/tiles/ops.py</code>:<ul> <li><code>order_idx(..., mode=\\\"hilbert\\\")</code> now uses <code>hilbert_order_indices(...)</code></li> </ul> </li> <li>Exported ordering APIs from <code>apex_x/tiles/__init__.py</code></li> <li>Added fixtures:<ul> <li><code>tests/fixtures/hilbert_2x2.json</code></li> <li><code>tests/fixtures/hilbert_4x4.json</code></li> <li><code>tests/fixtures/hilbert_8x8.json</code></li> </ul> </li> <li>Added fixture-driven tests in <code>tests/test_tile_hilbert.py</code>:<ul> <li>exact traversal match vs fixtures</li> <li>determinism across repeated calls</li> <li>full coverage of all coordinates/indices</li> <li>subset index ordering stability + parity with <code>order_idx(..., mode=\\\"hilbert\\\")</code></li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>Scan ordering variants and stable dispatcher implemented:</li> <li>Extended <code>apex_x/tiles/ordering.py</code> with scan modes and dispatcher utilities:<ul> <li>Scan variants: <code>l2r</code>, <code>r2l</code>, <code>u2d</code>, <code>d2u</code></li> <li>Stable dispatcher: <code>order_tile_indices(indices, grid_h, grid_w, mode=...)</code></li> <li>Mode normalization and aliases: <code>normalize_order_mode(...)</code></li> <li>supports <code>scan_lr/scan_rl/scan_ud/scan_du</code> + short aliases + canonical names</li> <li>Scan inverse mapping helper: <code>inverse_scan_mode(...)</code></li> <li>Explicit scan ordering APIs:</li> <li><code>scan_order_coords(grid_h, grid_w, mode)</code></li> <li><code>scan_order_indices(indices, grid_h, grid_w, mode)</code></li> </ul> </li> <li>Updated <code>apex_x/tiles/ops.py</code>:<ul> <li><code>order_idx(...)</code> now delegates to <code>order_tile_indices(...)</code> (single path for ordering semantics)</li> </ul> </li> <li>Exported new ordering APIs from <code>apex_x/tiles/__init__.py</code></li> <li>Added tests in <code>tests/test_tile_scan_ordering.py</code>:<ul> <li>deterministic ordering for all scan variants</li> <li>alias/normalization correctness</li> <li>stable ordering behavior on duplicate indices</li> <li>reversible mapping checks:</li> <li><code>L2R &lt;-&gt; R2L</code> by horizontal mirror</li> <li><code>U2D &lt;-&gt; D2U</code> by vertical mirror</li> <li>dispatcher parity with <code>order_idx(...)</code></li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>L0-&gt;L1 quadtree mapping and metadata implemented:</li> <li>Added <code>apex_x/tiles/quadtree.py</code> with deterministic L0/L1 mapping APIs:<ul> <li><code>l1_grid_shape_from_l0(l0_grid_h, l0_grid_w)</code></li> <li><code>l0_l1_grid_shapes_from_feature(feature_h, feature_w, tile_size_l0, tile_size_l1)</code></li> <li><code>l0_to_l1_children_coords(l0_ty, l0_tx, l0_grid_h, l0_grid_w)</code> (TL, TR, BL, BR order)</li> <li><code>l0_to_l1_children_indices(l0_index, l0_grid_h, l0_grid_w)</code></li> <li>reverse mapping:</li> <li><code>l1_to_l0_parent_coord(l1_ty, l1_tx, l0_grid_h, l0_grid_w)</code></li> <li><code>l1_to_l0_parent_index(l1_index, l0_grid_h, l0_grid_w)</code></li> <li>metadata builder:</li> <li><code>build_l0_l1_quadtree_meta(parent_indices, l0_grid_h, l0_grid_w) -&gt; L0L1QuadtreeMeta</code></li> </ul> </li> <li>Exported new quadtree APIs in <code>apex_x/tiles/__init__.py</code></li> <li>Added tests in <code>tests/test_tile_quadtree.py</code>:<ul> <li>boundary tile mapping correctness (bottom-right L0 tile to L1 children)</li> <li>reverse parent mapping across full L1 grids</li> <li>multiple config coverage via <code>(feature_h, feature_w, tile_size_l0, tile_size_l1)</code> parametrization</li> <li>metadata shape/content checks</li> <li>invalid ratio/divisibility/out-of-bounds validation checks</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>L2 nesting and overlap priority contract implemented:</li> <li>Extended <code>apex_x/tiles/quadtree.py</code> with L1-&gt;L2 and combined depth-2 utilities:<ul> <li>grid shapes:</li> <li><code>l2_grid_shape_from_l1(...)</code></li> <li><code>l1_l2_grid_shapes_from_feature(...)</code></li> <li><code>l0_l1_l2_grid_shapes_from_feature(...)</code></li> <li>mappings:</li> <li><code>l1_to_l2_children_coords(...)</code>, <code>l1_to_l2_children_indices(...)</code></li> <li><code>l2_to_l1_parent_coord(...)</code>, <code>l2_to_l1_parent_index(...)</code></li> <li><code>l0_to_l2_descendant_indices(...)</code></li> <li>metadata:</li> <li><code>L1L2QuadtreeMeta</code></li> <li><code>L0L1L2QuadtreeMeta</code></li> <li><code>build_l1_l2_quadtree_meta(...)</code></li> <li><code>build_l0_l1_l2_quadtree_meta(...)</code></li> </ul> </li> <li>Defined explicit overlap priority tags and helper:<ul> <li><code>OVERLAP_PRIORITY_L0 = 1</code></li> <li><code>OVERLAP_PRIORITY_L1 = 2</code></li> <li><code>OVERLAP_PRIORITY_L2 = 3</code></li> <li><code>overlap_priority_for_level(...)</code></li> <li>contract enforces <code>L2 &gt; L1 &gt; L0</code></li> </ul> </li> <li>Exported new L2 and priority APIs through <code>apex_x/tiles/__init__.py</code></li> <li>Added/expanded tests:<ul> <li><code>tests/test_tile_quadtree.py</code>:</li> <li>L1-&gt;L2 mapping correctness (including boundary tiles)</li> <li>L2-&gt;L1 reverse mapping correctness</li> <li>L0-&gt;L2 descendant index correctness</li> <li>combined <code>L0/L1/L2</code> metadata consistency</li> <li>priority tag contract checks</li> <li>multi-config and validation coverage for depth-2 shapes</li> <li><code>tests/test_tile_ops.py</code>:</li> <li>overlap behavior check using unpack priorities confirming <code>L2</code> overrides <code>L1</code> and <code>L0</code></li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>Tile selection debug dataclasses implemented:</li> <li>Added <code>apex_x/tiles/selection.py</code> with:<ul> <li><code>TileSelection</code></li> <li>fields: <code>level</code>, <code>indices</code>, <code>ordered_indices</code>, <code>meta</code>, <code>budgets_used</code></li> <li>validation:<ul> <li><code>level</code> constrained to <code>l0/l1/l2</code></li> <li><code>ordered_indices</code> must be a permutation of <code>indices</code></li> <li>budgets must be finite and non-negative</li> </ul> </li> <li>JSON persistence:<ul> <li><code>to_dict()/from_dict()</code></li> <li><code>save_json()/load_json()</code></li> </ul> </li> <li><code>TileSelectionTrace</code> for multi-level selection records</li> <li>fields: <code>selections</code>, <code>run_meta</code></li> <li>helpers:<ul> <li><code>to_dict()/from_dict()</code></li> <li><code>save_json()/load_json()</code></li> <li><code>for_level(level)</code></li> </ul> </li> </ul> </li> <li>JSON serialization includes recursive normalization of NumPy arrays/scalars for debug/ablation dumps.</li> <li>Exported APIs via <code>apex_x/tiles/__init__.py</code>:<ul> <li><code>TileSelection</code></li> <li><code>TileSelectionTrace</code></li> </ul> </li> <li>Added unit tests in <code>tests/test_tile_selection.py</code>:<ul> <li>roundtrip dict serialization</li> <li>file save/load JSON</li> <li>validation error cases</li> <li>trace roundtrip and level lookup</li> <li>non-empty trace guard</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>Tile overlay visualization utility implemented:</li> <li>Added <code>apex_x/utils/visualization.py</code> with dependency-free overlay rendering:<ul> <li><code>draw_selected_tiles_overlay(...)</code></li> <li>supports <code>HWC</code>, <code>CHW</code>, and batch-size-1 image inputs</li> <li>deterministic tile overlay rendering from selected tile indices + grid/tile size</li> <li>fill + border blending with deterministic integer alpha math</li> <li><code>save_overlay_ppm(...)</code></li> <li>saves overlay to <code>.ppm</code> for debug/ablation without extra image libraries</li> <li><code>draw_and_save_selected_tiles_overlay(...)</code></li> <li>convenience wrapper combining render + save</li> </ul> </li> <li>Exported visualization utilities through <code>apex_x/utils/__init__.py</code></li> <li>Added deterministic tests in <code>tests/test_visualization.py</code>:<ul> <li>overlay output shape/dtype checks</li> <li>stable SHA256 hash checks for rendered overlay bytes</li> <li>stable SHA256 hash checks for saved PPM file bytes</li> <li>save-path extension validation for <code>.ppm</code></li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>Cost model interface and reference implementation added:</li> <li>Extended routing interfaces in <code>apex_x/routing/interfaces.py</code>:<ul> <li><code>CostModelProtocol</code></li> <li>backward-compatible alias <code>CostModel</code></li> </ul> </li> <li>Added <code>apex_x/routing/cost_model.py</code>:<ul> <li><code>LevelCost</code>:</li> <li>per-level cost terms:<ul> <li><code>c_cheap (C_c)</code></li> <li><code>c_heavy (C_h)</code></li> <li><code>pack_overhead</code></li> <li><code>unpack_overhead</code></li> <li><code>split_overhead (O_split)</code></li> </ul> </li> <li><code>CalibrationRecord</code>:</li> <li>stores empirical calibration measurements, blend factor, and apply flag</li> <li><code>StaticCostModel</code>:</li> <li>level-aware cost computations:<ul> <li><code>cheap_cost(...)</code></li> <li><code>heavy_cost(...)</code></li> <li><code>delta_cost(...)</code></li> <li><code>split_overhead(...)</code></li> <li><code>expected_level_cost(...)</code></li> <li><code>total_cost(...)</code></li> </ul> </li> <li>optional empirical calibration hook:<ul> <li><code>apply_empirical_calibration(level, measured_timings, blend, apply)</code></li> <li>stores records in <code>calibration_history</code></li> </ul> </li> <li>serialization:<ul> <li><code>to_dict()/from_dict()</code></li> <li><code>save_json()/load_json()</code></li> </ul> </li> </ul> </li> <li>Exported cost model symbols through:<ul> <li><code>apex_x/routing/__init__.py</code></li> <li><code>apex_x/__init__.py</code> (public API now includes <code>CostModel</code>, <code>CostModelProtocol</code>, <code>StaticCostModel</code>)</li> </ul> </li> <li>Added tests in <code>tests/test_cost_model.py</code>:<ul> <li>deterministic cost computations and totals</li> <li>calibration update behavior and history storage</li> <li>JSON serialization roundtrip</li> <li>validation/error paths</li> <li>protocol conformance check (<code>isinstance(..., CostModelProtocol)</code>)</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>Oracle set sampler implemented (<code>S</code> sampling for utility oracle training):</li> <li>Added <code>apex_x/routing/oracle_sampling.py</code>:<ul> <li><code>OracleSetSample</code> dataclass:</li> <li><code>indices</code></li> <li><code>random_indices</code></li> <li><code>uncertainty_indices</code></li> <li><code>sample_oracle_set(u_hat, random_fraction, uncertainty_fraction, seed)</code>:</li> <li>random fraction sampling over all tiles</li> <li>uncertainty-biased sampling over remaining tiles using PV uncertainty <code>u_hat</code></li> <li>deterministic behavior under fixed seed</li> </ul> </li> <li>Exported sampler APIs through <code>apex_x/routing/__init__.py</code>:<ul> <li><code>OracleSetSample</code></li> <li><code>sample_oracle_set</code></li> </ul> </li> <li>Added tests in <code>tests/test_oracle_sampling.py</code>:<ul> <li>seed determinism checks</li> <li>count/uniqueness checks for random + uncertainty components</li> <li>uncertainty-biased distribution check across many seeds</li> <li>validation/error checks for fraction bounds and invalid uncertainty values</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passedprodoljai</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>Stable tie-breaking helper for selections implemented:</li> <li>Updated <code>apex_x/routing/core.py</code>:<ul> <li>added <code>stable_rank_tile_ids(scores)</code> with deterministic ordering policy:</li> <li>primary key: score descending</li> <li>secondary key: tile id ascending</li> <li>wired <code>greedy_utility_per_cost(...)</code> to use <code>stable_rank_tile_ids(...)</code> for score-ratio ranking</li> </ul> </li> <li>Exported helper via <code>apex_x/routing/__init__.py</code>:<ul> <li><code>stable_rank_tile_ids</code></li> </ul> </li> <li>Added tests in <code>tests/test_router.py</code>:<ul> <li>explicit tie-break behavior validation</li> <li>repeat-run determinism checks</li> <li>integration check that greedy selection follows stable tie-breaking under equal utility/cost ratios</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>PV-&gt;FF tile vector aggregation implemented:</li> <li>Added <code>apex_x/routing/aggregation.py</code>:<ul> <li><code>compute_ff_tile_bounds_on_pv_grid(...)</code></li> <li>computes PV-aligned bounds for each FF tile on coarse PV maps</li> <li><code>aggregate_pv_maps_to_ff_tile_vectors(...)</code></li> <li>pools per-tile stats from PV maps aligned to FF grid</li> <li>supported stats: <code>mean</code>, <code>max</code>, <code>var</code></li> <li>deterministic feature ordering using sorted PV map names</li> <li><code>PVTileAggregation</code> dataclass:</li> <li><code>vectors</code> (<code>[B, K, D]</code>)</li> <li><code>tile_bounds_pv</code> (<code>[K,4]</code>)</li> <li><code>feature_layout</code> (feature names per channel/stat/map)</li> </ul> </li> <li>Exported aggregation APIs via <code>apex_x/routing/__init__.py</code>:<ul> <li><code>PVTileAggregation</code></li> <li><code>compute_ff_tile_bounds_on_pv_grid</code></li> <li><code>aggregate_pv_maps_to_ff_tile_vectors</code></li> </ul> </li> <li>Added tests in <code>tests/test_pv_aggregation.py</code>:<ul> <li>alignment on simple integer scale mapping</li> <li>pooled stat correctness (<code>mean/max/var</code>)</li> <li>output shape and feature layout checks</li> <li>deterministic outputs across map ordering</li> <li>non-integer scale edge/boundary alignment</li> <li>validation error paths</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m pytest -q</code> passed</li> <li><code>ruff check .</code> passed</li> <li><code>mypy</code> passed</li> </ul> </li> <li>RouterTinyMLP implemented with utility/split/temporal outputs:</li> <li>Added <code>apex_x/routing/tiny_mlp.py</code>:<ul> <li><code>RouterTinyOutput</code> dataclass carrying per-tile tensors:</li> <li><code>U</code> (<code>[B,K]</code>) utility logits</li> <li><code>S</code> (<code>[B,K]</code>) split utility logits</li> <li>optional <code>T</code> (<code>[B,K]</code>) temporal keep logits</li> <li><code>RouterTinyMLP(nn.Module)</code>:</li> <li>configurable <code>input_dim</code>, <code>hidden_dim</code>, <code>num_layers</code>, <code>temporal_head</code></li> <li>forward contract on <code>x[B,K,D]</code> with strict input validation</li> <li>compatibility method <code>predict_utilities(...)</code> for <code>RouterProtocol</code> usage (<code>input_dim=1</code>)</li> </ul> </li> <li>Exported in <code>apex_x/routing/__init__.py</code>:<ul> <li><code>RouterTinyOutput</code></li> <li><code>RouterTinyMLP</code></li> </ul> </li> <li>Added tests in <code>tests/test_router_tiny_mlp.py</code>:<ul> <li>shape checks for <code>U/S</code> and optional <code>T</code></li> <li>deterministic outputs for fixed seed/model initialization</li> <li>backward/gradient-flow checks through router outputs</li> <li><code>predict_utilities(...)</code> behavior and validation paths</li> <li>runtime protocol conformance (<code>isinstance(..., RouterProtocol)</code>)</li> </ul> </li> <li>Minor typing fix to keep strict typecheck green:<ul> <li><code>apex_x/utils/visualization.py</code> now uses an explicit typed cast in <code>_as_hwc_uint8(...)</code></li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check .</code> passed</li> <li><code>python -m pytest -q</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> </ul> </li> <li>Lightweight spline activation + RouterKANLike implemented:</li> <li>Added <code>apex_x/routing/kan_like.py</code>:<ul> <li><code>LightweightSplineActivation</code>:</li> <li>compact per-feature piecewise-linear spline on fixed knot grid</li> <li>identity initialization for stable startup</li> <li>explicit <code>nan/inf</code> sanitization + bounded input clamp</li> <li><code>RouterKANLike</code>:</li> <li>small-parameter KAN-like router (<code>LayerNorm -&gt; Linear -&gt; Spline -&gt; Linear</code>)</li> <li>outputs <code>U</code>, <code>S</code>, and optional <code>T</code> via <code>RouterKANOutput</code></li> <li>bounded head logits via configurable <code>logit_clip</code></li> <li>protocol-compatible <code>predict_utilities(...)</code> and <code>parameter_count()</code></li> </ul> </li> <li>Exported via <code>apex_x/routing/__init__.py</code>:<ul> <li><code>LightweightSplineActivation</code></li> <li><code>RouterKANOutput</code></li> <li><code>RouterKANLike</code></li> </ul> </li> <li>Added numerical stability tests in <code>tests/test_router_kan_like.py</code>:<ul> <li>finite outputs under extreme/nonnumeric input values</li> <li>finite gradients for spline params and inputs</li> <li>router output shape + finite output checks at large input magnitudes</li> <li>deterministic initialization/output checks under fixed seeds</li> <li>small-parameter-count guard</li> <li><code>RouterProtocol</code> conformance check</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check .</code> passed</li> <li><code>python -m pytest -q</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> </ul> </li> <li>Tensor STE gating implemented for continuous-budget training path:</li> <li>Added <code>apex_x/routing/gating.py</code>:<ul> <li><code>sigmoid_probabilities(U, temperature)</code>:</li> <li>computes <code>p = sigmoid(U)</code> with temperature support</li> <li>clamps/sanitizes extreme and non-finite logits for numerical stability</li> <li><code>ste_hard_gate(p, mode, threshold)</code>:</li> <li>forward hard gate modes:<ul> <li><code>threshold</code>: <code>g = 1[p &gt;= threshold]</code></li> <li><code>bernoulli</code>: <code>g ~ Bernoulli(p)</code></li> </ul> </li> <li>backward straight-through estimator via detach trick (<code>dg/dp = 1</code>)</li> <li><code>ste_gate_from_utilities(U, ...) -&gt; (p, g)</code> convenience function</li> </ul> </li> <li>Exported in <code>apex_x/routing/__init__.py</code>:<ul> <li><code>GateMode</code></li> <li><code>sigmoid_probabilities</code></li> <li><code>ste_hard_gate</code></li> <li><code>ste_gate_from_utilities</code></li> </ul> </li> <li>Added autograd and numerical tests in <code>tests/test_ste_gating.py</code>:<ul> <li>non-zero and finite gradient checks w.r.t. <code>U</code> in threshold mode</li> <li>non-zero and finite gradient checks w.r.t. <code>U</code> in Bernoulli mode</li> <li>explicit gradient-form check against sigmoid derivative under linear loss</li> <li>finite probability checks under extreme/non-finite utility inputs</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check .</code> passed</li> <li><code>python -m pytest -q</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> </ul> </li> <li>BudgetDualController implemented for continuous-budget dual optimization:</li> <li>Added <code>apex_x/routing/dual_budget.py</code>:<ul> <li><code>BudgetDualController</code> (stateful dual variable controller) with:</li> <li>expected cost:<ul> <li><code>E[C] = sum_i(p_i * C_h + (1 - p_i) * C_c)</code> via <code>expected_cost(...)</code></li> </ul> </li> <li>budget term:<ul> <li><code>L_budget = mu * (E[C] - B)</code> via <code>budget_loss(...)</code></li> </ul> </li> <li>projected/clamped dual update:<ul> <li><code>mu &lt;- clamp(mu + mu_lr * (E[C] - B), [mu_min, mu_max])</code> via <code>update_mu(...)</code></li> </ul> </li> <li>structured debug logging on each update (<code>event='dual_mu_update'</code>) including:<ul> <li><code>expected_cost</code>, <code>budget</code>, <code>delta</code>, <code>mu_prev</code>, <code>mu_next</code>, <code>clamped</code></li> </ul> </li> <li>support for both sequence and tensor probabilities in expected-cost path</li> </ul> </li> <li>Exported in <code>apex_x/routing/__init__.py</code>:<ul> <li><code>BudgetDualController</code></li> </ul> </li> <li>Added tests in <code>tests/test_budget_dual_controller.py</code>:<ul> <li>expected-cost and budget-loss formula checks</li> <li><code>mu</code> moves in correct direction (<code>E[C] &gt; B</code> increases, <code>E[C] &lt; B</code> decreases)</li> <li><code>mu</code> clamp bounds respected (<code>mu_min</code> / <code>mu_max</code>)</li> <li>tensor-path budget loss backpropagates with finite non-zero gradients</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check .</code> passed</li> <li><code>python -m pytest -q</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> </ul> </li> <li>Deterministic greedy inference budgeting implemented with explicit Kmax buffer contract:</li> <li>Added <code>apex_x/routing/inference_budget.py</code>:<ul> <li><code>deterministic_greedy_selection(...)</code>:</li> <li>computes scores exactly as <code>score_i = U_i / DeltaC_i</code></li> <li>deterministic ordering by:<ul> <li>primary: score descending</li> <li>secondary: tile id ascending</li> </ul> </li> <li>selects until budget exhausted and <code>kmax</code> reached</li> <li>returns <code>GreedySelectionResult</code> with:<ul> <li><code>selected_indices</code></li> <li><code>spent_budget</code></li> <li><code>ordered_candidates</code></li> <li><code>scores</code></li> <li><code>kmax_buffer</code> (fixed-length padded buffer)</li> <li><code>valid_count</code></li> </ul> </li> <li><code>build_kmax_buffer(...)</code> helper for fixed-size runtime buffers</li> </ul> </li> <li>Wired existing public helper to the new deterministic path:<ul> <li><code>apex_x/routing/core.py::greedy_utility_per_cost(...)</code> now delegates to   <code>deterministic_greedy_selection(...)</code> and preserves existing return signature</li> </ul> </li> <li>Exported in <code>apex_x/routing/__init__.py</code>:<ul> <li><code>GreedySelectionResult</code></li> <li><code>build_kmax_buffer</code></li> <li><code>deterministic_greedy_selection</code></li> </ul> </li> <li>Added tests in <code>tests/test_inference_budget.py</code>:<ul> <li>repeat-run determinism for ordering and selections</li> <li>budget enforcement and <code>kmax</code> cap behavior</li> <li>stable tie handling (<code>tile_id</code> ascending under equal scores)</li> <li>Kmax buffer padding/truncation semantics</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check .</code> passed</li> <li><code>python -m pytest -q</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> </ul> </li> <li>Two-stage nesting selection implemented (<code>L0</code> under <code>B1</code>, split under <code>B2</code>):</li> <li>Extended <code>apex_x/routing/inference_budget.py</code>:<ul> <li><code>TwoStageSelectionResult</code> dataclass carrying:</li> <li><code>l0</code> selection result</li> <li>split parent ordering/selection</li> <li>spent split budget</li> <li>generated <code>L1</code> indices</li> <li>ordered <code>L1</code> indices + fixed-size <code>L1</code> Kmax buffer</li> <li><code>deterministic_two_stage_selection(...)</code>:</li> <li>stage 1: deterministic <code>L0</code> selection using <code>U / DeltaC</code> under <code>budget_b1</code> + <code>kmax_l0</code></li> <li>stage 2: split parent ranking by <code>S / O_split</code> under <code>budget_b2</code></li> <li>expands selected <code>L0</code> parents to <code>L1</code> children via quadtree mapping</li> <li>enforces <code>kmax_l1</code> capacity during split expansion</li> <li>applies deterministic <code>L1</code> ordering via configured order mode (Hilbert/scan)</li> </ul> </li> <li>Exported in <code>apex_x/routing/__init__.py</code>:<ul> <li><code>TwoStageSelectionResult</code></li> <li><code>deterministic_two_stage_selection</code></li> </ul> </li> <li>Added tests in <code>tests/test_two_stage_selection.py</code>:<ul> <li><code>L0</code> selection under <code>B1</code></li> <li>split candidate selection under <code>B2</code> with <code>S/O_split</code></li> <li>deterministic tie handling on split parents (<code>tile_id</code> ascending)</li> <li><code>L1</code> children generation and ordering behavior</li> <li>determinism across repeated runs</li> <li><code>kmax_l1</code> capacity/buffer enforcement</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check .</code> passed</li> <li><code>python -m pytest -q</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> </ul> </li> <li>Temporal hysteresis rollout and toggle analysis implemented:</li> <li>Extended <code>apex_x/routing/core.py</code> hysteresis APIs:<ul> <li><code>hysteresis_update(...)</code> now validates:</li> <li><code>theta_on &gt; theta_off</code></li> <li>equal lengths for <code>utilities_t</code> and <code>prev_mask</code></li> <li>added <code>hysteresis_rollout(...)</code>:</li> <li>applies rule over full time sequence with carried <code>z(t-1)</code> state</li> <li>added <code>count_mask_toggles(...)</code>:</li> <li>counts total 0/1 transitions across time (for anti-flicker evaluation)</li> </ul> </li> <li>Exported in <code>apex_x/routing/__init__.py</code>:<ul> <li><code>hysteresis_rollout</code></li> <li><code>count_mask_toggles</code></li> </ul> </li> <li>Added tests in <code>tests/test_hysteresis_temporal.py</code>:<ul> <li>deadband behavior preserves previous mask state (<code>z(t-1)</code>) when utility remains between thresholds</li> <li>synthetic noisy sequence shows reduced toggling vs single-threshold baseline</li> <li>validation/error-path checks for threshold ordering and shape consistency</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check .</code> passed</li> <li><code>python -m pytest -q</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> </ul> </li> <li>Routing diagnostics implemented and surfaced in model/train/infer paths:</li> <li>Added <code>apex_x/routing/diagnostics.py</code>:<ul> <li><code>utility_histogram(...)</code> for per-level utility histogram summaries</li> <li><code>build_routing_diagnostics(...)</code> producing:</li> <li>selected ratios/counts per level</li> <li>utility histograms per level</li> <li>budget usage (<code>used</code>, <code>budget</code>, <code>ratio</code>)</li> <li>dual variable history (<code>mu_history</code>)</li> </ul> </li> <li>Exported diagnostics APIs through <code>apex_x/routing/__init__.py</code></li> <li>Integrated diagnostics into <code>apex_x/model/core.py</code>:<ul> <li>model outputs now include <code>routing_diagnostics</code></li> <li>dual controller (<code>BudgetDualController</code>) state tracked in <code>mu_history</code></li> <li>optional dual update path enabled via <code>forward(..., update_dual=True)</code></li> <li>structured logs now include selected ratio, budget usage ratio, and latest <code>mu</code></li> </ul> </li> <li>Updated <code>apex_x/train/__init__.py</code> and <code>apex_x/infer/__init__.py</code>:<ul> <li>train placeholder logs/returns routing diagnostics summary fields</li> <li>infer placeholder returns diagnostics from model outputs</li> </ul> </li> <li>Updated CLI integration in <code>apex_x/cli.py</code>:<ul> <li><code>train</code> now performs short warmup forwards with dual updates and reports diagnostics</li> <li><code>predict</code> now reads infer diagnostics and logs budget usage ratio</li> </ul> </li> <li>Added tests in <code>tests/test_routing_diagnostics.py</code>:<ul> <li>diagnostics presence in inference outputs</li> <li>diagnostics propagation through train/infer placeholders</li> <li><code>mu_history</code> progression when dual updates are enabled</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check .</code> passed</li> <li><code>python -m pytest -q</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> </ul> </li> <li>Config-driven feature toggles added for forced-off routing/training paths:</li> <li>Extended <code>apex_x/config/schema.py</code>:<ul> <li><code>ModelConfig</code> toggles:</li> <li><code>force_dense_routing</code> (router off =&gt; dense tile activation)</li> <li><code>disable_nesting</code> (effective nesting depth forced to 0)</li> <li><code>disable_ssm</code> (bypass Tile-SSM mixing)</li> <li><code>TrainConfig</code> toggles:</li> <li><code>disable_distill</code></li> <li><code>disable_pcgradpp</code></li> <li>Added helper methods:</li> <li><code>ModelConfig.effective_nesting_depth()</code></li> <li><code>ModelConfig.router_enabled()</code></li> <li><code>ModelConfig.ssm_enabled()</code></li> <li><code>TrainConfig.distill_enabled()</code></li> <li><code>TrainConfig.pcgradpp_enabled()</code></li> <li>Validation updated so <code>disable_nesting=true</code> can force nesting off without requiring manual <code>kmax_l1/l2</code> and <code>budget_b3</code> cleanup.</li> </ul> </li> <li>Updated <code>apex_x/model/core.py</code> runtime behavior:<ul> <li>router-off mode forces dense <code>L0</code> selection and skips budget controller routing selection</li> <li>no-SSM mode bypasses <code>tile_ssm_scan(...)</code> and uses zero modulation/state</li> <li>no-nesting mode uses effective depth for diagnostics totals (<code>L1/L2</code> counts become zero)</li> <li>output now includes <code>feature_toggles</code> summary for debug/smoke assertions</li> </ul> </li> <li>Updated <code>apex_x/train/__init__.py</code> + <code>apex_x/cli.py</code>:<ul> <li>train placeholder now accepts config and reports effective distill/PCGrad++ enabled flags</li> <li>CLI <code>train</code> passes config through to preserve toggle behavior in logs/output paths</li> </ul> </li> <li>Added smoke coverage:<ul> <li><code>tests/test_feature_toggle_smoke.py</code></li> <li>validates override behavior for <code>disable_nesting</code></li> <li>executes all <code>2^5=32</code> toggle combinations:<ul> <li>router off / on</li> <li>no nesting / nesting</li> <li>no SSM / SSM</li> <li>no distill / distill</li> <li>no PCGrad++ / PCGrad++</li> </ul> </li> <li>asserts forward + train placeholder run without crashes and toggle semantics hold</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check .</code> passed</li> <li><code>python -m pytest -q</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> </ul> </li> <li>Torch tile packer implemented with contiguous output contract:</li> <li>Added <code>apex_x/tiles/torch_ops.py</code>:<ul> <li><code>TilePackTorch.pack(...)</code>:</li> <li>input: <code>F[B,C,H,W]</code>, <code>idx[B,K]</code>, <code>tile_size</code></li> <li>output: <code>P[B,K,C,t,t]</code>, <code>meta</code></li> <li>deterministic ordering via shared ordering dispatcher (<code>order_idx(...)</code>)</li> <li>strict shape/dtype/bounds validation</li> <li>guarantees contiguous packed tensor via <code>.contiguous()</code></li> <li><code>pack_tiles_torch(...)</code> convenience wrapper</li> <li><code>TorchTileMeta</code> type alias (<code>dict[str, torch.Tensor]</code>)</li> </ul> </li> <li>Exported through <code>apex_x/tiles/__init__.py</code>:<ul> <li><code>TorchTileMeta</code></li> <li><code>TilePackTorch</code></li> <li><code>pack_tiles_torch</code></li> </ul> </li> <li>Added tests in <code>tests/test_tile_pack_torch.py</code>:<ul> <li>correctness vs NumPy reference <code>pack_tiles(...)</code></li> <li>contiguous output assertion</li> <li>autograd gradient-flow check from packed output back to source feature map</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check ...</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_tile_pack_torch.py tests/test_tile_ops.py</code> passed</li> </ul> </li> <li>Torch tile unpacker implemented with overlap priority modes:</li> <li>Extended <code>apex_x/tiles/torch_ops.py</code>:<ul> <li><code>TileUnpackTorch.unpack(...)</code>:</li> <li>input: <code>base_map[B,C,H,W]</code>, <code>packed_out[B,K,C,t,t]</code>, <code>meta</code>, <code>level_priority</code>, optional <code>priority_map</code></li> <li>overlap modes:<ul> <li><code>override</code>: incoming tile values replace existing values where priority allows</li> <li><code>blend</code>: weighted fusion <code>out = (1-alpha)*current + alpha*incoming</code> where priority allows</li> </ul> </li> <li>priority contract preserved via per-pixel <code>priority_map</code> updates</li> <li>strict validation for tensor ranks/shapes, bounds, and mode/alpha values</li> <li><code>unpack_tiles_torch(...)</code> convenience wrapper</li> <li><code>OverlapMode</code> type alias</li> </ul> </li> <li>Exported through <code>apex_x/tiles/__init__.py</code>:<ul> <li><code>OverlapMode</code></li> <li><code>TileUnpackTorch</code></li> <li><code>unpack_tiles_torch</code></li> </ul> </li> <li>Added tests in <code>tests/test_tile_unpack_torch.py</code>:<ul> <li>override-mode parity vs NumPy <code>unpack_tiles(...)</code></li> <li>overlap priority enforcement and blend-mode numeric behavior</li> <li>autograd gradient-flow checks for blend mode (<code>base</code> and <code>packed_out</code>)</li> <li>helper/function parity (<code>TileUnpackTorch().unpack</code> vs <code>unpack_tiles_torch</code>)</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/tiles/torch_ops.py apex_x/tiles/__init__.py tests/test_tile_unpack_torch.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_tile_pack_torch.py tests/test_tile_unpack_torch.py tests/test_tile_ops.py</code> passed</li> </ul> </li> <li>Fusion gate implemented with boundary/uncertainty-conditioned alpha:</li> <li>Added <code>apex_x/model/fusion_gate.py</code>:<ul> <li><code>FusionGate(nn.Module)</code> computes spatial gate:</li> <li><code>alpha [B,1,H,W] = sigmoid(w_b * boundary_proxy + w_u * uncertainty_proxy + bias)</code></li> <li>positive monotonic proxy weights enforced via <code>softplus(...)</code></li> <li>outputs fused features:</li> <li><code>fused = base + alpha * (heavy - base)</code></li> <li>validates proxy shape contracts and aligns proxies to feature dtype/device</li> </ul> </li> <li>Exported in <code>apex_x/model/__init__.py</code>:<ul> <li><code>FusionGate</code></li> </ul> </li> <li>Added tests in <code>tests/test_fusion_gate.py</code>:<ul> <li>alpha shape/range and fusion formula correctness</li> <li>sensitivity checks: increasing boundary/uncertainty proxies increases mean alpha</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/fusion_gate.py tests/test_fusion_gate.py apex_x/model/__init__.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_fusion_gate.py</code> passed</li> </ul> </li> <li>Cheap block implemented (<code>1x1 + norm + ReGLU + optional residual</code>):</li> <li>Added <code>apex_x/model/cheap_block.py</code>:<ul> <li><code>CheapBlock(nn.Module)</code>:</li> <li>path: <code>Conv2d(1x1) -&gt; GroupNorm -&gt; ReGLU</code></li> <li>optional residual add</li> <li>automatic residual projection (<code>1x1</code>) when <code>in_channels != out_channels</code></li> <li>validation for input channels and normalization group divisibility</li> </ul> </li> <li>Exported in <code>apex_x/model/__init__.py</code>:<ul> <li><code>CheapBlock</code></li> </ul> </li> <li>Added tests in <code>tests/test_cheap_block.py</code>:<ul> <li>shape checks with residual projection path</li> <li>residual identity behavior when main path is zeroed</li> <li>no-residual zero-output behavior when main path is zeroed</li> <li>gradient-flow checks for input and block parameters</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/cheap_block.py apex_x/model/__init__.py tests/test_cheap_block.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_cheap_block.py tests/test_model.py</code> passed</li> </ul> </li> <li>Tile refine block implemented for packed tiles:</li> <li>Added <code>apex_x/model/tile_refine_block.py</code>:<ul> <li><code>TileRefineBlock(nn.Module)</code> operating on packed tensors <code>[B,K,C,t,t]</code></li> <li>local refine path per tile:</li> <li>depthwise conv (<code>k x k</code>)</li> <li>pointwise conv</li> <li><code>GroupNorm</code></li> <li>ReGLU activation</li> <li>optional residual (with automatic projection when channels differ)</li> <li>implementation flattens <code>B*K</code> for conv processing and restores <code>[B,K,...]</code>, preserving per-tile independence</li> </ul> </li> <li>Exported in <code>apex_x/model/__init__.py</code>:<ul> <li><code>TileRefineBlock</code></li> </ul> </li> <li>Added tests in <code>tests/test_tile_refine_block.py</code>:<ul> <li>shape/projection path checks</li> <li>residual identity when main path is zeroed</li> <li>per-tile independence (no cross-tile mixing)</li> <li>gradient-flow checks for inputs and parameters</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/tile_refine_block.py apex_x/model/__init__.py tests/test_tile_refine_block.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_tile_refine_block.py tests/test_cheap_block.py</code> passed</li> </ul> </li> <li>PV backbone implemented with P3/P4/P5 feature outputs:</li> <li>Added <code>apex_x/model/pv_backbone.py</code>:<ul> <li><code>PVBackbone(nn.Module)</code> returning feature dict:</li> <li><code>P3</code> (stride 8)</li> <li><code>P4</code> (stride 16)</li> <li><code>P5</code> (stride 32)</li> <li>uses lightweight staged downsampling + <code>CheapBlock</code> refinement per level</li> <li>validates input shape/channel contract and minimum spatial size</li> </ul> </li> <li>Exported in <code>apex_x/model/__init__.py</code>:<ul> <li><code>PVBackbone</code></li> </ul> </li> <li>Added tests in <code>tests/test_pv_backbone.py</code>:<ul> <li>parameterized shape checks across multiple input sizes</li> <li>gradient-flow check</li> <li>input validation checks</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/pv_backbone.py apex_x/model/__init__.py tests/test_pv_backbone.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_pv_backbone.py</code> passed</li> </ul> </li> <li>PV coarse heads implemented with explicit uncertainty proxy definition:</li> <li>Added <code>apex_x/model/pv_coarse_heads.py</code>:<ul> <li><code>PVCoarseHeads(nn.Module)</code> producing coarse PV maps from a selected backbone level (default <code>P4</code>):</li> <li><code>objectness_logits</code></li> <li><code>objectness = sigmoid(objectness_logits)</code></li> <li><code>boundary_proxy = sigmoid(boundary_logits)</code></li> <li><code>variance_proxy = softplus(variance_logits)</code></li> <li><code>uncertainty_proxy</code></li> <li>clear uncertainty definition:</li> <li><code>u_hat = 4 * p * (1 - p)</code> where <code>p = objectness</code></li> <li>normalized Bernoulli variance (<code>u_hat=1</code> at <code>p=0.5</code>, <code>u_hat=0</code> at <code>p in {0,1}</code>)</li> <li>output typed via <code>PVCoarseOutput</code> dataclass</li> </ul> </li> <li>Exported in <code>apex_x/model/__init__.py</code>:<ul> <li><code>PVCoarseHeads</code></li> <li><code>PVCoarseOutput</code></li> </ul> </li> <li>Added tests in <code>tests/test_pv_coarse_heads.py</code>:<ul> <li>parameterized shape/range checks across multiple image sizes (via <code>PVBackbone</code> + <code>P4</code>)</li> <li>uncertainty sensitivity checks with controlled objectness logits</li> <li>direct uncertainty formula parity check</li> <li>gradient-flow check through backbone + heads</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/pv_coarse_heads.py apex_x/model/__init__.py tests/test_pv_coarse_heads.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_pv_coarse_heads.py</code> passed</li> </ul> </li> <li>PV module wired with backbone + coarse heads:</li> <li>Added <code>apex_x/model/pv_module.py</code>:<ul> <li><code>PVModule</code> composes:</li> <li><code>PVBackbone</code> for <code>P3/P4/P5</code> feature extraction</li> <li><code>PVCoarseHeads</code> for coarse proxies</li> <li><code>PVModule.forward()</code> now returns <code>PVModuleOutput</code> containing:</li> <li><code>features</code> dict (<code>P3/P4/P5</code>)</li> <li><code>coarse</code> maps from selected level (<code>coarse_level</code>: <code>P3</code>/<code>P4</code>/<code>P5</code>)</li> <li><code>proxy_maps</code> dict for routing-facing signals:<ul> <li><code>objectness</code></li> <li><code>uncertainty</code></li> <li><code>boundary</code></li> <li><code>variance</code></li> </ul> </li> </ul> </li> <li>Exported in <code>apex_x/model/__init__.py</code>:<ul> <li><code>PVModule</code></li> <li><code>PVModuleOutput</code></li> </ul> </li> <li>Added tests in <code>tests/test_pv_module.py</code>:<ul> <li>shape checks across multiple input sizes</li> <li>coarse-level selection checks (<code>P3</code> vs <code>P5</code>)</li> <li><code>proxy_maps</code> key/shape checks</li> <li>finite-output assertions</li> </ul> </li> <li>Added dedicated CPU smoke test in <code>tests/test_pv_module_smoke_cpu.py</code>:<ul> <li>validates CPU forward path and proxy-map outputs are finite with expected shapes</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/pv_module.py apex_x/model/__init__.py tests/test_pv_module.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_pv_module.py tests/test_pv_module_smoke_cpu.py tests/test_pv_backbone.py tests/test_pv_coarse_heads.py</code> passed</li> </ul> </li> <li>Stable state-space-like scan implemented with constrained parameters:</li> <li>Extended <code>apex_x/utils/ssm.py</code>:<ul> <li><code>StableStateSpaceScan(nn.Module)</code> with constrained recurrent parameters:</li> <li>decay constrained to <code>(min_decay, max_decay)</code> via sigmoid mapping</li> <li>positive input/output gains via softplus</li> <li>numerically safe token sanitization/clamp path</li> <li><code>SSMScanStats</code> for explicit scan complexity accounting:</li> <li><code>steps</code></li> <li><code>recurrent_updates</code></li> <li><code>pairwise_updates</code></li> <li><code>tile_ssm_scan(...)</code> now clamps alpha to stable bounds</li> </ul> </li> <li>Exported in <code>apex_x/utils/__init__.py</code>:<ul> <li><code>StableStateSpaceScan</code></li> <li><code>SSMScanStats</code></li> </ul> </li> <li>Added tests in <code>tests/test_stable_ssm_scan.py</code>:<ul> <li>no-NaN/finite checks on extreme inputs</li> <li>gradient-flow checks for inputs and scan parameters</li> <li>O(K) behavior checks via linear recurrent-update accounting and zero pairwise updates</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/utils/ssm.py apex_x/utils/__init__.py tests/test_stable_ssm_scan.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_stable_ssm_scan.py</code> passed</li> </ul> </li> <li>Bidirectional scan and merge gate added on top of stable scan:</li> <li>Extended <code>apex_x/utils/ssm.py</code>:<ul> <li><code>StableBidirectionalStateSpaceScan(nn.Module)</code>:</li> <li>forward-direction stable scan</li> <li>backward-direction stable scan (reverse sequence + reverse outputs back)</li> <li>channel-wise constrained merge gate:<ul> <li><code>gate = sigmoid(merge_gate_logit)</code> in <code>[0,1]</code></li> <li>merged output: <code>gate * y_fwd + (1 - gate) * y_bwd</code></li> </ul> </li> <li>complexity stats preserved as linear-time recurrent updates (no pairwise updates)</li> </ul> </li> <li>Exported in <code>apex_x/utils/__init__.py</code>:<ul> <li><code>StableBidirectionalStateSpaceScan</code></li> </ul> </li> <li>Added tests in <code>tests/test_bidirectional_ssm_scan.py</code>:<ul> <li>finite/no-NaN checks with extreme inputs</li> <li>merge formula correctness vs explicit gate-weighted combination</li> <li>gradient-flow checks for inputs and parameters (including merge gate path)</li> <li>O(K) scaling checks from recurrent update counts</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/utils/ssm.py apex_x/utils/__init__.py tests/test_bidirectional_ssm_scan.py tests/test_stable_ssm_scan.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_stable_ssm_scan.py tests/test_bidirectional_ssm_scan.py</code> passed</li> </ul> </li> <li>FiLM modulation implemented from tokens to packed tiles:</li> <li>Added <code>apex_x/model/film.py</code>:<ul> <li><code>TileFiLM(nn.Module)</code>:</li> <li>computes FiLM parameters from tokens <code>tokens[B,K,Ct]</code></li> <li>bounded gain path: <code>gamma = tanh(gamma_raw) * gamma_limit</code></li> <li>shift path: <code>beta</code></li> <li>applies modulation to packed tiles: <code>out = (1 + gamma) * tiles + beta</code></li> <li><code>apply_film(...)</code> functional helper with strict shape validation</li> </ul> </li> <li>Updated <code>apex_x/model/core.py</code>:<ul> <li>replaced additive-only packed modulation with FiLM-style modulation using SSM mixed tokens:</li> <li><code>gamma = tanh(mixed)</code></li> <li><code>beta = mixed</code></li> <li><code>packed_out = (1 + gamma) * packed + beta</code></li> </ul> </li> <li>Exported in <code>apex_x/model/__init__.py</code>:<ul> <li><code>TileFiLM</code></li> <li><code>apply_film</code></li> </ul> </li> <li>Added tests in <code>tests/test_film_modulation.py</code>:<ul> <li>formula and shape checks</li> <li>gamma range bound checks</li> <li>deterministic forward under fixed inputs</li> <li>gradient-flow checks through tokens, tiles, and module parameters</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/film.py apex_x/model/core.py apex_x/model/__init__.py tests/test_film_modulation.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_film_modulation.py tests/test_model.py</code> passed</li> </ul> </li> <li>FF heavy path implemented end-to-end with aligned detail map output:</li> <li>Added <code>apex_x/model/ff_heavy_path.py</code>:<ul> <li><code>FFHeavyPath(nn.Module)</code> pipeline:</li> <li><code>TilePackTorch</code> gather on selected FF tile indices</li> <li>tile tokenization via spatial pooling: <code>tokens[B,K,C]</code></li> <li>stable scan (<code>forward</code> or <code>bidirectional</code>) over tokens</li> <li>FiLM modulation (<code>gamma</code>, <code>beta</code>) from mixed tokens to packed tiles</li> <li>local packed-tile refine via <code>TileRefineBlock</code></li> <li><code>TileUnpackTorch</code> scatter back to dense map shape</li> <li>optional proxy-conditioned fusion gate (<code>FusionGate</code>)</li> <li>output contract via <code>FFHeavyPathOutput</code>:</li> <li><code>heavy_features[B,C,H,W]</code></li> <li><code>detail_map[B,C,H,W]</code> aligned to dense features</li> <li><code>alpha[B,1,H,W]</code></li> <li>diagnostics tensors (<code>tokens</code>, <code>mixed_tokens</code>, <code>gamma</code>, <code>beta</code>, <code>state</code>)</li> <li>includes robust CPU behavior for empty tile selections (<code>K=0</code>) with zero detail contribution.</li> </ul> </li> <li>Updated exports in <code>apex_x/model/__init__.py</code>:<ul> <li><code>FFHeavyPath</code></li> <li><code>FFHeavyPathOutput</code></li> </ul> </li> <li>Added CPU tests in <code>tests/test_ff_heavy_path.py</code>:<ul> <li>shape/alignment checks and finite outputs</li> <li>empty-selection behavior (<code>K=0</code>) -&gt; zero <code>detail_map</code></li> <li>deterministic repeated forward and gradient-flow checks</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/ff_heavy_path.py apex_x/model/__init__.py tests/test_ff_heavy_path.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_ff_heavy_path.py</code> passed</li> <li>full project checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>./.venv/bin/python -m mypy</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>FF module implemented for train/infer routing execution:</li> <li>Added <code>apex_x/model/ff_module.py</code>:<ul> <li><code>FFModule(nn.Module)</code> with two explicit entrypoints:</li> <li><code>forward_train(...)</code>:<ul> <li>STE routing gates via <code>ste_gate_from_utilities(...)</code></li> <li>expected-cost computation via <code>BudgetDualController.expected_cost(...)</code></li> <li>budget loss term via <code>BudgetDualController.budget_loss(...)</code></li> <li>optional dual-<code>mu</code> update with persistent <code>mu_history</code></li> <li>routed L0 heavy execution through <code>FFHeavyPath</code></li> </ul> </li> <li><code>forward_infer(...)</code>:<ul> <li>deterministic L0 greedy selection under <code>B1</code>/<code>Kmax_l0</code></li> <li>optional L0-&gt;L1 two-stage selection under <code>B2</code>/<code>Kmax_l1</code></li> <li>optional nesting execution (L1 heavy pass) when split utilities provided</li> </ul> </li> <li>diagnostics integrated in both paths through <code>build_routing_diagnostics(...)</code>:</li> <li>selected counts/ratios</li> <li>utility histograms</li> <li>per-budget usage (<code>b1/b2/b3/total</code>)</li> <li><code>mu_history</code></li> <li>output dataclasses:</li> <li><code>FFTrainOutput</code></li> <li><code>FFInferOutput</code></li> </ul> </li> <li>Updated exports in <code>apex_x/model/__init__.py</code>:<ul> <li><code>FFModule</code></li> <li><code>FFTrainOutput</code></li> <li><code>FFInferOutput</code></li> </ul> </li> <li>Added tests in <code>tests/test_ff_module.py</code>:<ul> <li>train path:</li> <li>STE + expected-cost + budget-loss outputs</li> <li>diagnostics presence</li> <li>dual-<code>mu</code> history update</li> <li>inference path:</li> <li>deterministic budgeted L0 selection</li> <li>optional nesting with deterministic L1 child selection under <code>B2</code></li> <li>diagnostics coverage</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/ff_module.py apex_x/model/__init__.py tests/test_ff_module.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_ff_module.py</code> passed</li> <li>full project checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>./.venv/bin/python -m mypy</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Dual-path FPN implemented to combine PV low-res with FF high-res:</li> <li>Added <code>apex_x/model/fpn.py</code>:<ul> <li><code>DualPathFPN(nn.Module)</code>:</li> <li>inputs:<ul> <li>PV features dict <code>P3/P4/P5</code></li> <li>FF high-res feature/detail map</li> </ul> </li> <li>fusion path:<ul> <li>lateral 1x1 projections for PV <code>P3/P4/P5</code></li> <li>lateral 1x1 projection for FF branch</li> <li>top-down FPN merge (<code>P5 -&gt; P4 -&gt; P3</code>)</li> <li>explicit FF injection at <code>P3</code> after spatial alignment</li> <li>smoothing with <code>CheapBlock</code> on <code>P3/P4/P5</code></li> </ul> </li> <li>output contract via <code>DualPathFPNOutput</code>:<ul> <li>fused pyramid dict <code>P3/P4/P5</code></li> <li>aligned FF feature map at P3 resolution (<code>ff_aligned</code>)</li> </ul> </li> </ul> </li> <li>Updated exports in <code>apex_x/model/__init__.py</code>:<ul> <li><code>DualPathFPN</code></li> <li><code>DualPathFPNOutput</code></li> </ul> </li> <li>Added tests in <code>tests/test_fpn.py</code>:<ul> <li>CPU shape and finite-value checks</li> <li>FF-branch sensitivity (changing FF input changes fused <code>P3</code>)</li> <li>gradient-flow checks through PV inputs, FF input, and FPN params</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/fpn.py apex_x/model/__init__.py tests/test_fpn.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_fpn.py</code> passed</li> <li>full project checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>./.venv/bin/python -m mypy</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>DET head implemented over P3..P7 (cls/box/quality):</li> <li>Added <code>apex_x/model/det_head.py</code>:<ul> <li><code>DetHead(nn.Module)</code>:</li> <li>consumes pyramid features with required <code>P3/P4/P5</code></li> <li>supports optional provided <code>P6/P7</code>; auto-generates missing levels from <code>P5</code>/<code>P6</code></li> <li>per-level outputs:<ul> <li><code>cls_logits</code>: <code>[B, num_classes, H, W]</code></li> <li><code>box_reg</code>: <code>[B, 4, H, W]</code></li> <li><code>quality</code>: <code>[B, 1, H, W]</code></li> </ul> </li> <li>uses shared tower structure for cls/box/quality with GroupNorm + SiLU and export-friendly conv outputs</li> <li>output contract via <code>DetHeadOutput</code>:</li> <li>per-level dicts for <code>cls_logits</code>, <code>box_reg</code>, <code>quality</code></li> <li>normalized <code>features</code> dict for effective <code>P3..P7</code> levels used by the head</li> </ul> </li> <li>Updated exports in <code>apex_x/model/__init__.py</code>:<ul> <li><code>DetHead</code></li> <li><code>DetHeadOutput</code></li> </ul> </li> <li>Added tests in <code>tests/test_det_head.py</code>:<ul> <li>shape checks across all output levels <code>P3..P7</code> when only <code>P3..P5</code> are provided</li> <li>behavior check that explicitly provided <code>P6/P7</code> are used as-is</li> <li>gradient-flow checks through inputs and DET-head parameters</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/det_head.py apex_x/model/__init__.py tests/test_det_head.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_det_head.py</code> passed</li> <li>full project checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>./.venv/bin/python -m mypy</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>SimOTA/OTA cost components implemented for DET matching:</li> <li>Added <code>apex_x/losses/simota.py</code>:<ul> <li>classification cost:</li> <li><code>classification_cost(...)</code> with modes:<ul> <li>BCE-based positive-class cost</li> <li>focal-based positive-class cost</li> </ul> </li> <li>IoU cost:</li> <li><code>iou_cost(...)</code> implementing <code>1 - IoU</code> over pairwise GT/anchor boxes</li> <li>center prior cost:</li> <li><code>center_prior_cost(...)</code> from GT center to anchor center distance (normalized by GT size)</li> <li>per-GT candidate generation:</li> <li><code>topk_center_candidates(...)</code> selecting top-k nearest anchor centers per GT</li> <li><code>candidate_mask_from_indices(...)</code> to build per-GT candidate masks</li> <li>integrated cost assembly:</li> <li><code>compute_simota_cost(...)</code> combining weighted components and candidate masking</li> <li><code>SimOTACostOutput</code> dataclass containing component matrices, combined cost, and candidates</li> </ul> </li> <li>Updated exports in <code>apex_x/losses/__init__.py</code>:<ul> <li><code>classification_cost</code>, <code>iou_cost</code>, <code>center_prior_cost</code></li> <li><code>topk_center_candidates</code>, <code>candidate_mask_from_indices</code></li> <li><code>compute_simota_cost</code>, <code>SimOTACostOutput</code>, <code>ClassificationCostType</code></li> </ul> </li> <li>Added tests in <code>tests/test_simota_cost.py</code>:<ul> <li>per-GT top-k center candidate selection correctness on synthetic anchors/GT</li> <li>classification-cost ranking behavior (higher positive logit -&gt; lower cost)</li> <li>IoU cost sanity (<code>1 - IoU</code>)</li> <li>combined SimOTA ranking on synthetic setup (reasonable anchor wins; non-candidates penalized)</li> <li>center-prior ranking preference for nearby anchor centers</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/losses/simota.py apex_x/losses/__init__.py tests/test_simota_cost.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_simota_cost.py</code> passed</li> <li>full project checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>./.venv/bin/python -m mypy</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Dynamic-K SimOTA matching implemented (including conflict resolution):</li> <li>Extended <code>apex_x/losses/simota.py</code>:<ul> <li><code>DynamicKMatchingOutput</code> dataclass with:</li> <li><code>dynamic_ks</code></li> <li><code>matching_matrix</code></li> <li><code>foreground_mask</code></li> <li><code>matched_gt_indices</code></li> <li><code>assigned_cost</code></li> <li><code>num_foreground</code></li> <li><code>dynamic_k_from_top_ious(...)</code>:</li> <li>computes per-GT <code>dynamic_k</code> from sum of top IoUs (with configurable <code>topk</code> and <code>min_k</code>)</li> <li>supports optional candidate mask</li> <li><code>dynamic_k_matching(...)</code>:</li> <li>selects <code>dynamic_k</code> anchors per GT by minimal total cost</li> <li>resolves multi-GT conflicts by keeping the minimal-cost GT assignment per anchor</li> <li>outputs deterministic one-to-one anchor-to-GT assignment for foreground anchors</li> </ul> </li> <li>Updated exports in <code>apex_x/losses/__init__.py</code>:<ul> <li><code>DynamicKMatchingOutput</code></li> <li><code>dynamic_k_from_top_ious</code></li> <li><code>dynamic_k_matching</code></li> </ul> </li> <li>Expanded <code>tests/test_simota_cost.py</code>:<ul> <li>verifies dynamic-k computation from top-IoU sums</li> <li>verifies crowded conflict resolution picks minimal-cost GT per anchor</li> <li>verifies candidate-mask constraints are respected in crowded settings</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/losses/simota.py apex_x/losses/__init__.py tests/test_simota_cost.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_simota_cost.py</code> passed</li> <li>full project checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>./.venv/bin/python -m mypy</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>SimOTA assignment integrated into DET loss with target generation:</li> <li>Added <code>apex_x/losses/det_loss.py</code>:<ul> <li><code>build_simota_targets_for_anchors(...)</code>:</li> <li>uses SimOTA cost + dynamic-k matching to select positive anchors</li> <li>builds per-anchor targets:<ul> <li><code>cls_target</code> (one-hot positives, zero background)</li> <li><code>box_target</code> (assigned GT boxes)</li> <li><code>quality_target</code> (matched IoU targets)</li> </ul> </li> <li>returns <code>SimOTATargets</code> with matching diagnostics</li> <li><code>det_loss_with_simota(...)</code>:</li> <li>computes DET loss from assignment targets:<ul> <li>classification loss (BCE or focal)</li> <li>box loss (<code>1 - IoU</code>) on positives</li> <li>quality BCE loss</li> </ul> </li> <li>returns <code>DetLossOutput</code> with component losses and targets</li> <li>stability features:</li> <li>canonicalized box ordering for robust IoU math</li> <li>optional assignment on detached predictions</li> <li>small-object positive weighting with clipped inverse-sqrt area scaling</li> <li>dynamic-k conflict-resolved assignment for crowded scenes</li> </ul> </li> <li>Updated exports in <code>apex_x/losses/__init__.py</code>:<ul> <li><code>SimOTATargets</code></li> <li><code>DetLossOutput</code></li> <li><code>build_simota_targets_for_anchors</code></li> <li><code>det_loss_with_simota</code></li> <li><code>ClsLossType</code></li> </ul> </li> <li>Added tests in <code>tests/test_det_loss_simota.py</code>:<ul> <li>target generation and per-anchor labeling correctness</li> <li>finite/stable loss on tiny-object crowded synthetic setup</li> <li>toy optimization loop verifying DET loss decreases over training steps</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/losses/det_loss.py apex_x/losses/__init__.py tests/test_det_loss_simota.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_det_loss_simota.py</code> passed</li> <li>full project checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>./.venv/bin/python -m mypy</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>DET losses hardened for numerical stability and quality-focal support:</li> <li>Updated <code>apex_x/losses/det_loss.py</code>:<ul> <li>added logit sanitization/clipping via <code>_sanitize_logits(...)</code></li> <li>added <code>QualityLossType</code> with <code>bce</code> and <code>qfl</code> modes</li> <li><code>det_loss_with_simota(...)</code> now supports:</li> <li><code>quality_loss_type=\\\"qfl\\\"</code></li> <li><code>quality_focal_beta</code></li> <li><code>logit_clip</code></li> <li>focal/BCE classification and QFL/BCE quality paths now run on sanitized logits for stable behavior under extreme values</li> </ul> </li> <li>Updated exports in <code>apex_x/losses/__init__.py</code>:<ul> <li><code>QualityLossType</code></li> </ul> </li> <li>Expanded <code>tests/test_det_loss_simota.py</code>:<ul> <li>toy decreasing-loss case now also exercises quality-focal path</li> <li>added extreme-logit + tiny-box finite test with backward gradient finiteness checks</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/losses/det_loss.py apex_x/losses/__init__.py tests/test_det_loss_simota.py</code> passed</li> <li><code>./.venv/bin/python -m mypy</code> passed</li> <li><code>python -m pytest -q tests/test_det_loss_simota.py</code> passed</li> <li>full project checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>./.venv/bin/python -m mypy</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Deterministic DET decode + NMS implemented:</li> <li>Added <code>apex_x/infer/detection.py</code>:<ul> <li><code>decode_anchor_free_candidates(...)</code>:</li> <li>decodes anchor-free <code>DetHeadOutput</code> maps into per-image candidate tensors</li> <li>supports configurable level strides and image clipping</li> <li>applies stable candidate ranking with deterministic tie behavior</li> <li><code>deterministic_nms(...)</code>:</li> <li>class-wise NMS with deterministic ordering</li> <li>tie-breaking policy: score desc, then candidate index asc</li> <li><code>batched_deterministic_nms(...)</code>:</li> <li>fixed-shape batched outputs with padding and <code>valid_counts</code></li> <li><code>decode_and_nms(...)</code>:</li> <li>end-to-end decode + NMS convenience entrypoint</li> <li>output dataclasses:</li> <li><code>DetectionCandidates</code></li> <li><code>DetectionBatch</code></li> </ul> </li> <li>Updated exports in <code>apex_x/infer/__init__.py</code>:<ul> <li><code>DetectionCandidates</code></li> <li><code>DetectionBatch</code></li> <li><code>decode_anchor_free_candidates</code></li> <li><code>deterministic_nms</code></li> <li><code>batched_deterministic_nms</code></li> <li><code>decode_and_nms</code></li> </ul> </li> <li>Added tests in <code>tests/test_det_decode_nms.py</code>:<ul> <li>end-to-end decode + NMS determinism and class-wise suppression behavior</li> <li>deterministic tie handling in NMS (equal scores -&gt; lower index first)</li> <li>cross-class overlap handling (no cross-class suppression)</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/infer/detection.py apex_x/infer/__init__.py tests/test_det_decode_nms.py</code> passed</li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/infer/detection.py apex_x/infer/__init__.py</code> passed</li> <li><code>python -m pytest -q tests/test_det_decode_nms.py tests/test_import_smoke.py</code> passed</li> <li><code>python -m pytest -q</code> passed</li> </ul> </li> <li>Prototype-based instance segmentation forward path and mask assembly implemented:</li> <li>Added <code>apex_x/model/inst_seg_head.py</code>:<ul> <li><code>PrototypeInstanceSegHead(nn.Module)</code>:</li> <li>prototype generator from feature maps (<code>prototypes: [B,M,Hp,Wp]</code>)</li> <li>per-instance coefficient prediction (<code>coefficients: [B,N,M]</code>) from:<ul> <li>ROI mean-pooled feature regions derived from <code>boxes_xyxy</code>, or</li> <li>explicit <code>instance_embeddings</code></li> </ul> </li> <li>mask assembly by linear prototype combination:<ul> <li><code>mask_logits_lowres = einsum(coefficients, prototypes) -&gt; [B,N,Hp,Wp]</code></li> </ul> </li> <li>output resizing to requested mask resolution</li> <li>optional box cropping with configurable fill value for stable masked logits</li> <li>per-instance <code>mask_scores</code> from masked probability averages</li> <li>helper functions:</li> <li><code>assemble_mask_logits_from_prototypes(...)</code></li> <li><code>rasterize_box_masks(...)</code></li> <li>output dataclass:</li> <li><code>InstanceSegOutput</code></li> </ul> </li> <li>Updated exports in <code>apex_x/model/__init__.py</code>:<ul> <li><code>PrototypeInstanceSegHead</code></li> <li><code>InstanceSegOutput</code></li> <li><code>assemble_mask_logits_from_prototypes</code></li> <li><code>rasterize_box_masks</code></li> </ul> </li> <li>Added tests in <code>tests/test_inst_seg_head.py</code>:<ul> <li>prototype-mask assembly correctness vs expected weighted combinations</li> <li>forward-path shape/range checks and finite outputs</li> <li>deterministic repeatability for same weights/inputs</li> <li>gradient-flow checks (features + instance embeddings + parameters)</li> <li>crop-to-box fill behavior outside ROI regions</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_head.py</code> passed</li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_head.py</code> passed</li> <li><code>python -m pytest -q tests/test_inst_seg_head.py</code> passed</li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code> passed</li> </ul> </li> <li>Segmentation losses implemented (BCE + Dice + boundary DT surrogate):</li> <li>Added <code>apex_x/losses/seg_loss.py</code>:<ul> <li><code>mask_bce_loss(...)</code>:</li> <li>BCEWithLogits per mask with optional per-instance weighting <code>[B,N]</code></li> <li><code>mask_dice_loss(...)</code>:</li> <li>soft Dice loss over <code>[B,N,H,W]</code> masks with optional per-instance weighting</li> <li><code>soft_boundary_distance_transform(...)</code>:</li> <li>differentiable approximation of boundary distance transform using iterative     soft-min neighborhood propagation</li> <li><code>boundary_distance_transform_surrogate_loss(...)</code>:</li> <li>boundary mismatch weighted by target soft distance transform</li> <li><code>instance_segmentation_losses(...)</code>:</li> <li>combined BCE + Dice + boundary loss returning <code>SegLossOutput</code></li> </ul> </li> <li>Updated exports in <code>apex_x/losses/__init__.py</code>:<ul> <li><code>SegLossOutput</code></li> <li><code>mask_bce_loss</code></li> <li><code>mask_dice_loss</code></li> <li><code>soft_boundary_distance_transform</code></li> <li><code>boundary_distance_transform_surrogate_loss</code></li> <li><code>instance_segmentation_losses</code></li> </ul> </li> <li>Added tests in <code>tests/test_seg_loss.py</code>:<ul> <li>BCE/Dice near-zero behavior for near-perfect logits</li> <li>soft boundary-DT monotonicity sanity check</li> <li>boundary surrogate sensitivity to shifted boundaries</li> <li>toy optimization loop showing combined loss decreases with finite gradients</li> <li>instance-weight support path</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/losses/seg_loss.py apex_x/losses/__init__.py tests/test_seg_loss.py</code> passed</li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/losses/seg_loss.py apex_x/losses/__init__.py tests/test_seg_loss.py</code> passed</li> <li><code>python -m pytest -q tests/test_seg_loss.py</code> passed</li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>FF high-resolution tile-gated refinement hook implemented for instance masks:</li> <li>Updated <code>apex_x/model/inst_seg_head.py</code>:<ul> <li>added <code>FFTileRefinementHook(nn.Module)</code>:</li> <li>inputs: <code>mask_logits [B,N,H,W]</code>, <code>ff_highres_features [B,C,H,W]</code>, <code>active_tile_indices [B,K]</code></li> <li>packs only active tiles, applies FF-conditioned additive refinement on packed tiles, and unpacks back</li> <li>guarantees inactive tiles remain unchanged via tile-scatter semantics</li> <li>integrated optional hook into <code>PrototypeInstanceSegHead</code>:</li> <li>new init toggles:<ul> <li><code>enable_ff_refine</code></li> <li><code>ff_refine_tile_size</code></li> <li><code>ff_refine_order_mode</code></li> <li><code>ff_refine_overlap_mode</code></li> <li><code>ff_refine_blend_alpha</code></li> <li><code>ff_refine_strength_init</code></li> </ul> </li> <li>new forward args:<ul> <li><code>ff_highres_features</code></li> <li><code>active_tile_indices</code></li> </ul> </li> <li>refinement is applied only when enabled and both FF features + active indices are provided</li> </ul> </li> <li>Updated exports in <code>apex_x/model/__init__.py</code>:<ul> <li><code>FFTileRefinementHook</code></li> </ul> </li> <li>Added tests in <code>tests/test_inst_seg_refinement_hook.py</code>:<ul> <li>hook updates only selected tiles and leaves non-selected tiles exactly unchanged</li> <li>empty active-tile indices produce no-op behavior</li> <li>head-level integration verifies refinement delta is confined to active tile regions</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_refinement_hook.py</code> passed</li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/model/inst_seg_head.py apex_x/model/__init__.py tests/test_inst_seg_refinement_hook.py</code> passed</li> <li><code>python -m pytest -q tests/test_inst_seg_refinement_hook.py tests/test_inst_seg_head.py</code> passed</li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Panoptic output generation implemented (semantic + instance fusion):</li> <li>Added <code>apex_x/infer/panoptic.py</code>:<ul> <li><code>generate_panoptic_output(...)</code>:</li> <li>combines semantic logits with instance masks into deterministic panoptic maps</li> <li>deterministic overlap fusion:<ul> <li>thing instances fused first, sorted by <code>(score desc, instance_index asc)</code></li> <li>overlap resolution keeps higher-ranked instance pixels</li> </ul> </li> <li>deterministic thing/stuff rules:<ul> <li>only classes in <code>thing_class_ids</code> are accepted as thing instances</li> <li>remaining unassigned pixels are filled by semantic stuff classes in ascending class-id order</li> <li>segment id <code>0</code> reserved as void/unassigned</li> </ul> </li> <li>supports:<ul> <li>mask threshold, score threshold</li> <li>minimum thing/stuff area filters</li> <li>optional <code>masks_are_logits</code> for instance-mask logits input</li> </ul> </li> <li>dataclasses:</li> <li><code>PanopticSegmentInfo</code> (<code>id</code>, <code>category_id</code>, <code>isthing</code>, <code>area</code>, optional score/index)</li> <li><code>PanopticOutput</code> (<code>panoptic_map</code>, <code>segments_info</code>, <code>semantic_labels</code>)</li> </ul> </li> <li>Updated exports in <code>apex_x/infer/__init__.py</code>:<ul> <li><code>PanopticSegmentInfo</code></li> <li><code>PanopticOutput</code></li> <li><code>generate_panoptic_output</code></li> </ul> </li> <li>Added tests in <code>tests/test_panoptic_generation.py</code>:<ul> <li>deterministic overlap resolution on synthetic overlapping thing instances</li> <li>thing/stuff rule verification (non-thing instances ignored, stuff preserved)</li> <li>output contract checks on synthetic batched scenes:</li> <li>map shape/type</li> <li>unique segment IDs</li> <li>per-segment area parity with panoptic map pixels</li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/infer/panoptic.py apex_x/infer/__init__.py tests/test_panoptic_generation.py</code> passed</li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/infer/panoptic.py apex_x/infer/__init__.py tests/test_panoptic_generation.py</code> passed</li> <li><code>python -m pytest -q tests/test_panoptic_generation.py</code> passed</li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Panoptic PQ evaluation wrapper implemented (official API + fallback):</li> <li>Added <code>apex_x/infer/pq_eval.py</code>:<ul> <li><code>evaluate_panoptic_quality(...)</code>:</li> <li>uses official <code>panopticapi</code> evaluator when available and paths are provided</li> <li>otherwise falls back to an in-memory deterministic minimal PQ implementation</li> <li>deterministic fallback behavior:</li> <li>per-category matching with IoU threshold</li> <li>one-to-one matches with deterministic tie handling</li> <li>computes per-class <code>(PQ, SQ, RQ)</code> and aggregate all/things/stuff metrics</li> <li>dataclasses:</li> <li><code>PQClassMetrics</code></li> <li><code>PQMetrics</code></li> <li><code>OfficialPQPaths</code></li> </ul> </li> <li>Updated exports in <code>apex_x/infer/__init__.py</code>:<ul> <li><code>PQClassMetrics</code></li> <li><code>PQMetrics</code></li> <li><code>OfficialPQPaths</code></li> <li><code>evaluate_panoptic_quality</code></li> </ul> </li> <li>CLI integration hook added:<ul> <li>updated <code>apex_x/cli.py</code> <code>eval</code> command with:</li> <li><code>--panoptic-pq</code> flag</li> <li>runs panoptic PQ hook and prints <code>panoptic_pq=&lt;value&gt;</code> and source (<code>official</code>/<code>fallback</code>)</li> </ul> </li> <li>Added fixtures:<ul> <li><code>tests/fixtures/pq_case_perfect.json</code></li> <li><code>tests/fixtures/pq_case_partial.json</code></li> </ul> </li> <li>Added tests:<ul> <li><code>tests/test_pq_eval.py</code></li> <li>fixture-driven perfect and partial overlap PQ checks</li> <li>verifies official-path attempt cleanly falls back when official API is unavailable/invalid</li> <li><code>tests/test_cli.py</code></li> <li>new parse test for <code>eval --panoptic-pq</code></li> </ul> </li> <li>Verification status:<ul> <li><code>python -m ruff check apex_x/infer/pq_eval.py apex_x/infer/__init__.py apex_x/cli.py tests/test_pq_eval.py tests/test_cli.py</code> passed</li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/infer/pq_eval.py apex_x/infer/__init__.py apex_x/cli.py tests/test_pq_eval.py tests/test_cli.py</code> passed</li> <li><code>python -m pytest -q tests/test_pq_eval.py tests/test_cli.py</code> passed</li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Tracking embedding head and association interfaces implemented:</li> <li>Added <code>apex_x/model/track_head.py</code>:<ul> <li><code>TrackEmbeddingHead</code>:</li> <li>accepts feature tensor or feature dict</li> <li>projects features, ROI-pools detections, and emits L2-normalized embeddings</li> <li><code>TrackEmbeddingOutput</code> dataclass with:</li> <li><code>embeddings</code></li> <li><code>raw_embeddings</code></li> <li><code>pooled_features</code></li> </ul> </li> <li>Added <code>apex_x/infer/tracking.py</code>:<ul> <li><code>TrackState</code> dataclass (validated tracker state contract)</li> <li><code>AssociationResult</code> dataclass</li> <li><code>AssociationProtocol</code> interface</li> <li>deterministic <code>GreedyCosineAssociator</code> implementation</li> <li>compatibility aliases:</li> <li><code>TrackAssociatorProtocol</code></li> <li><code>TrackAssociator</code></li> </ul> </li> <li>Updated exports:<ul> <li><code>apex_x/model/__init__.py</code> exports <code>TrackEmbeddingHead</code>, <code>TrackEmbeddingOutput</code></li> <li><code>apex_x/infer/__init__.py</code> exports tracking dataclasses/protocols/associator</li> <li><code>apex_x/__init__.py</code> exports <code>TrackState</code> and <code>AssociationProtocol</code></li> </ul> </li> <li>Added tests:<ul> <li><code>tests/test_track_head.py</code>:</li> <li>output shape checks</li> <li>embedding unit-norm checks</li> <li>deterministic forward checks</li> <li>gradient flow checks</li> <li><code>tests/test_tracking_interfaces.py</code>:</li> <li><code>TrackState.empty(...)</code> contract</li> <li>protocol conformance checks</li> <li>deterministic greedy matching/new-track behavior</li> <li>aging/removal behavior with no detections</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/model/track_head.py apex_x/infer/tracking.py tests/test_track_head.py tests/test_tracking_interfaces.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/model/track_head.py apex_x/infer/tracking.py tests/test_track_head.py tests/test_tracking_interfaces.py</code></li> <li><code>python -m pytest -q tests/test_track_head.py tests/test_tracking_interfaces.py</code></li> <li>project checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> <li>note:</li> <li><code>python -m black --check .</code> currently reports unrelated pre-existing formatting diffs in legacy files not touched in this change:<ul> <li><code>apex_x/routing/interfaces.py</code></li> <li><code>apex_x/train/__init__.py</code></li> <li><code>tests/test_pq_eval.py</code></li> <li><code>apex_x/config/schema.py</code></li> <li><code>apex_x/infer/pq_eval.py</code></li> <li><code>apex_x/losses/det_loss.py</code></li> <li><code>apex_x/model/inst_seg_head.py</code></li> </ul> </li> </ul> </li> <li>Hungarian association with lifecycle and memory-bank updates implemented:</li> <li>Updated <code>apex_x/infer/tracking.py</code>:<ul> <li>added Hungarian solver utility:</li> <li><code>hungarian_assignment(...)</code></li> <li>added <code>HungarianAssociator</code> implementing:</li> <li>IoU + embedding-distance gating for candidate matches</li> <li>global cost minimization via Hungarian assignment</li> <li>track lifecycle (<code>init</code> / <code>update</code> / <code>terminate</code>) with <code>max_age</code></li> <li>embedding memory-bank update per track with fixed bank size</li> <li>bank-size normalization for legacy states to keep runtime stable</li> <li>extended <code>TrackState</code> with optional lifecycle/memory fields:</li> <li><code>hit_counts</code></li> <li><code>memory_bank</code></li> <li><code>memory_counts</code></li> <li>extended <code>AssociationResult</code> with lifecycle debug outputs:</li> <li><code>terminated_track_indices</code></li> <li><code>terminated_track_ids</code></li> <li><code>created_track_ids</code></li> <li>kept backward compatibility:</li> <li><code>GreedyCosineAssociator</code> now delegates to Hungarian with cosine-only cost</li> <li>protocol aliases preserved</li> </ul> </li> <li>Updated <code>apex_x/infer/__init__.py</code> exports:<ul> <li><code>HungarianAssociator</code></li> <li><code>hungarian_assignment</code></li> </ul> </li> <li>Added tests in <code>tests/test_tracking_hungarian.py</code>:<ul> <li>Hungarian global-optimum assignment on synthetic cost matrix</li> <li>IoU + embedding-distance gating behavior</li> <li>lifecycle termination after <code>max_age</code></li> <li>memory-bank update/cap behavior</li> <li>multi-frame moving-object ID consistency across reordered detections</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/infer/tracking.py apex_x/infer/__init__.py tests/test_tracking_interfaces.py tests/test_tracking_hungarian.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/infer/tracking.py apex_x/infer/__init__.py tests/test_tracking_interfaces.py tests/test_tracking_hungarian.py</code></li> <li><code>python -m pytest -q tests/test_tracking_interfaces.py tests/test_tracking_hungarian.py tests/test_track_head.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>PCGrad++ shared-trunk conflict projection implemented:</li> <li>Added <code>apex_x/train/pcgrad.py</code>:<ul> <li>canonical grouped loss ordering:</li> <li><code>det_cls</code>, <code>det_box</code>, <code>seg_mask</code>, <code>seg_boundary</code>, then sorted extras</li> <li><code>LossGroup</code> dataclass and <code>group_loss_terms(...)</code></li> <li><code>apply_pcgradpp(...)</code>:</li> <li>computes per-group gradients</li> <li>applies projection only to shared trunk parameter gradients when <code>cos &lt; 0</code></li> <li>leaves task-head parameter gradients as standard total-loss gradients</li> <li><code>PCGradDiagnostics</code> + <code>diagnostics_to_dict(...)</code> for logging/debug</li> </ul> </li> <li>Updated exports in <code>apex_x/train/__init__.py</code>:<ul> <li><code>DEFAULT_LOSS_GROUP_ORDER</code></li> <li><code>LossGroup</code></li> <li><code>PCGradDiagnostics</code></li> <li><code>group_loss_terms</code></li> <li><code>apply_pcgradpp</code></li> <li><code>diagnostics_to_dict</code></li> </ul> </li> <li>Added tests in <code>tests/test_pcgradpp.py</code>:<ul> <li>deterministic grouped ordering for canonical loss groups + extra loss terms</li> <li>tiny-network synthetic conflicting gradients test:</li> <li>confirms projection resolves shared-trunk conflict</li> <li>confirms head gradients match standard total-loss gradients (no projection on heads)</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/train/pcgrad.py apex_x/train/__init__.py tests/test_pcgradpp.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/train/pcgrad.py apex_x/train/__init__.py</code></li> <li><code>python -m pytest -q tests/test_pcgradpp.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Distillation losses implemented (logits KL + feature L2 + boundary distill):</li> <li>Added <code>apex_x/losses/distill.py</code>:<ul> <li><code>logits_kl_distill(...)</code>:</li> <li>KL divergence distillation on logits with temperature scaling (<code>T^2</code> factor)</li> <li><code>feature_l2_distill(...)</code>:</li> <li>layer-selective feature L2 distillation with optional per-layer weights</li> <li>supports optional feature normalization before L2</li> <li><code>boundary_distill_loss(...)</code>:</li> <li>boundary-focused distillation using Sobel-based soft boundary maps</li> <li>teacher boundary distance-transform weighting</li> <li><code>distillation_losses(...)</code>:</li> <li>combined wrapper returning <code>DistillationLossOutput</code></li> </ul> </li> <li>Updated exports in <code>apex_x/losses/__init__.py</code>:<ul> <li><code>DistillationLossOutput</code></li> <li><code>logits_kl_distill</code></li> <li><code>feature_l2_distill</code></li> <li><code>boundary_distill_loss</code></li> <li><code>distillation_losses</code></li> </ul> </li> <li>Added tests in <code>tests/test_distill_loss.py</code>:<ul> <li>KL distill near-zero when student/teacher logits match</li> <li>feature L2 selected-layer behavior with layer weights</li> <li>boundary distill penalizes shifted boundaries more than aligned boundaries</li> <li>combined distillation loss decreases in toy optimization</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/losses/distill.py apex_x/losses/__init__.py tests/test_distill_loss.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/losses/distill.py apex_x/losses/__init__.py</code></li> <li><code>python -m pytest -q tests/test_distill_loss.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Oracle \u0394Loss targets and router utility supervision implemented:</li> <li>Added <code>apex_x/routing/oracle_distill.py</code>:<ul> <li><code>compute_oracle_delta_targets(...)</code>:</li> <li>computes sampled-tile oracle targets:<ul> <li><code>\u0394_i = L_distill(cheap, teacher) - L_distill(heavy, teacher)</code></li> </ul> </li> <li>supports sampled indices <code>[S]</code> or batched <code>[B,S]</code></li> <li>returns detached (stop-grad) oracle targets</li> <li>optional clamping for outlier robustness</li> <li><code>utility_regression_loss(...)</code>:</li> <li>regression loss (<code>l1</code> / <code>mse</code> / <code>smooth_l1</code>) between router utility logits and detached \u0394 targets</li> <li><code>utility_ranking_loss(...)</code>:</li> <li>pairwise hinge ranking loss over sampled tiles to preserve oracle ordering</li> <li><code>utility_oracle_loss(...)</code>:</li> <li>combined regression + ranking objective with diagnostics (<code>num_pairs</code>)</li> <li>dataclasses:</li> <li><code>OracleDeltaTargets</code></li> <li><code>UtilityOracleLossOutput</code></li> </ul> </li> <li>Updated routing exports in <code>apex_x/routing/__init__.py</code>:<ul> <li><code>RegressionLossType</code></li> <li><code>OracleDeltaTargets</code></li> <li><code>UtilityOracleLossOutput</code></li> <li><code>compute_oracle_delta_targets</code></li> <li><code>utility_regression_loss</code></li> <li><code>utility_ranking_loss</code></li> <li><code>utility_oracle_loss</code></li> </ul> </li> <li>Added tests in <code>tests/test_oracle_distill.py</code>:<ul> <li>sign sanity for \u0394 targets (positive when heavy distill loss is lower than cheap)</li> <li>stop-grad behavior (no gradients into cheap/heavy distill losses via utility supervision)</li> <li>ranking sign sanity (correct utility order yields lower ranking loss)</li> <li>sampled-index regression correctness (only sampled tiles influence loss)</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/routing/oracle_distill.py apex_x/routing/__init__.py tests/test_oracle_distill.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/routing/oracle_distill.py apex_x/routing/__init__.py</code></li> <li><code>python -m pytest -q tests/test_oracle_distill.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>TeacherModel implemented for full-compute distillation outputs with optional EMA:</li> <li>Added <code>apex_x/model/teacher.py</code>:<ul> <li><code>TeacherModel</code>:</li> <li>dense/full-compute teacher forward path (PV -&gt; FPN -&gt; DET) without sparse routing</li> <li>standardized distillation output bundle:<ul> <li>flattened logits (<code>logits</code>)</li> <li>per-level logits (<code>logits_by_level</code>)</li> <li>selected feature layers (<code>features</code>)</li> <li>boundary proxy map aligned to input size (<code>boundaries</code>)</li> </ul> </li> <li>optional EMA shadow modules:<ul> <li>configurable <code>ema_decay</code></li> <li><code>update_ema(...)</code> for parameter/buffer updates</li> <li>runtime switch to use online or EMA weights in <code>forward(...)</code></li> </ul> </li> <li><code>TeacherDistillOutput</code> dataclass</li> <li><code>flatten_logits_for_distill(...)</code> helper with deterministic level order</li> </ul> </li> <li>Updated exports in <code>apex_x/model/__init__.py</code>:<ul> <li><code>TeacherModel</code></li> <li><code>TeacherDistillOutput</code></li> <li><code>flatten_logits_for_distill</code></li> </ul> </li> <li>Added tests in <code>tests/test_teacher_model.py</code>:<ul> <li>full-compute standardized output contract checks</li> <li>deterministic logits-flatten ordering checks</li> <li>EMA behavior checks:</li> <li>initial online/EMA parity</li> <li>EMA lag + update movement toward online model</li> <li>frozen EMA parameter requirements</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/model/teacher.py apex_x/model/__init__.py tests/test_teacher_model.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/model/teacher.py apex_x/model/__init__.py</code></li> <li><code>python -m pytest -q tests/test_teacher_model.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Staged trainer pipeline implemented and wired into CLI <code>train</code> flow:</li> <li>Added <code>apex_x/train/trainer.py</code>:<ul> <li><code>ApexXTrainer</code> with required stages:</li> <li>stage 0: baseline warmup</li> <li>stage 1: teacher training (full compute)</li> <li>stage 2: oracle bootstrapping</li> <li>stage 3: continuous budgeting with dual <code>mu</code></li> <li>stage 4: deterministic inference emulation</li> <li>stage/result dataclasses:</li> <li><code>StageResult</code></li> <li><code>StagedTrainResult</code></li> </ul> </li> <li>Updated exports in <code>apex_x/train/__init__.py</code>:<ul> <li><code>ApexXTrainer</code></li> <li><code>StageResult</code></li> <li><code>StagedTrainResult</code></li> </ul> </li> <li>Updated CLI <code>train</code> command in <code>apex_x/cli.py</code>:<ul> <li>now runs staged trainer instead of placeholder-only loop</li> <li>new option: <code>--steps-per-stage</code></li> <li>output includes <code>stage_count=5</code></li> </ul> </li> <li>Added staged-train CPU smoke script:<ul> <li><code>examples/train_stages_smoke.py</code></li> </ul> </li> <li>Added validation coverage:<ul> <li><code>tests/test_trainer_stages.py</code> (stage completeness + seed repeatability)</li> <li><code>tests/test_train_stages_smoke.py</code> (subprocess smoke run)</li> <li>updated <code>tests/test_cli.py</code> to assert staged output includes <code>stage_count=5</code></li> </ul> </li> <li>Updated docs:<ul> <li><code>README.md</code> now includes staged trainer quickstart/dev commands</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/train/trainer.py apex_x/cli.py tests/test_trainer_stages.py tests/test_train_stages_smoke.py tests/test_cli.py examples/train_stages_smoke.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/train/trainer.py apex_x/cli.py</code></li> <li><code>python -m pytest -q tests/test_trainer_stages.py tests/test_train_stages_smoke.py tests/test_cli.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Exact COCO compatibility layer implemented with strict schema checks and mask parsing:</li> <li>Added <code>apex_x/data/coco.py</code> with a strict COCO loader:<ul> <li><code>load_coco_dataset(path, strict=True, use_cache=True)</code></li> <li>strict top-level/record key validation (<code>images</code>, <code>annotations</code>, <code>categories</code>)</li> <li>type/range checks for ids, bbox, area, iscrowd, segmentation payloads</li> <li>referential integrity checks for <code>annotation.image_id</code> and <code>annotation.category_id</code></li> </ul> </li> <li>Added complete parsing contracts:<ul> <li>bbox parsing into <code>CocoBBox</code></li> <li>polygon segmentation parsing into <code>CocoSegmentation(kind=\\\"polygon\\\")</code></li> <li>RLE parsing for uncompressed list counts and compressed string counts into   <code>CocoSegmentation(kind=\\\"rle\\\")</code></li> <li>deterministic mask decode path via <code>segmentation_to_mask(...)</code></li> </ul> </li> <li>Added category mapping + caching:<ul> <li><code>CocoDataset.category_mapping()</code> caches contiguous category mapping:</li> <li><code>original_to_contiguous</code></li> <li><code>contiguous_to_original</code></li> <li>category name lookup maps</li> <li>loader cache for parsed datasets with mtime/size cache key</li> <li><code>clear_coco_dataset_cache()</code> helper</li> </ul> </li> <li>Updated exports in <code>apex_x/data/__init__.py</code> for COCO dataclasses/helpers.</li> <li>Added fixture-based tests:<ul> <li><code>tests/test_coco_compat.py</code></li> <li>fixtures:</li> <li><code>tests/fixtures/coco_valid_mixed.json</code></li> <li><code>tests/fixtures/coco_invalid_missing_top_keys.json</code></li> <li><code>tests/fixtures/coco_invalid_bad_rle.json</code></li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/data/coco.py apex_x/data/__init__.py tests/test_coco_compat.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/data/coco.py apex_x/data/__init__.py tests/test_coco_compat.py</code></li> <li><code>python -m pytest -q tests/test_coco_compat.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Data transforms pipeline and Mosaic-v2 heuristic implemented:</li> <li>Added <code>apex_x/data/transforms.py</code>:<ul> <li>shared sample contract:</li> <li><code>TransformSample</code> (image + <code>boxes_xyxy</code> + <code>class_ids</code> + optional masks)</li> <li>pipeline + base transforms:</li> <li><code>TransformPipeline</code></li> <li><code>RandomHorizontalFlip</code></li> <li><code>ClipBoxesAndMasks</code></li> <li><code>sanitize_sample(...)</code> for clipping/filtering invalid boxes/masks</li> <li>Mosaic-v2:</li> <li><code>MosaicV2(...)</code> 4-image composition with configurable split jitter</li> <li>heuristic crop-origin policy to protect important instances     (by area threshold fallback-to-largest instance)</li> <li>visibility-aware bbox filtering and mask-aware validity checks</li> </ul> </li> <li>Updated exports in <code>apex_x/data/__init__.py</code>:<ul> <li><code>TransformSample</code>, <code>Transform</code>, <code>TransformPipeline</code></li> <li><code>RandomHorizontalFlip</code>, <code>ClipBoxesAndMasks</code>, <code>MosaicV2</code>, <code>sanitize_sample</code></li> </ul> </li> <li>Added tests in <code>tests/test_data_transforms.py</code>:<ul> <li>transform-pipeline bbox/mask validity checks</li> <li>mosaic output validity for bbox/mask contracts</li> <li>heuristic regression test showing protected mosaic keeps significantly more   important-instance area than unprotected crop selection</li> <li>sanitize clipping behavior on out-of-bounds boxes</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/data/transforms.py apex_x/data/__init__.py tests/test_data_transforms.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/data/transforms.py apex_x/data/__init__.py tests/test_data_transforms.py</code></li> <li><code>python -m pytest -q tests/test_data_transforms.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Eval pipeline implemented for DET/INST-SEG/SEM-SEG/PANO with report emission:</li> <li>Added <code>apex_x/infer/eval_metrics.py</code>:<ul> <li>metric computation for tiny-fixture evaluation:</li> <li>COCO-style mAP (DET) over IoU thresholds <code>0.50:0.05:0.95</code></li> <li>COCO-style mask mAP (INST-SEG) over IoU thresholds <code>0.50:0.05:0.95</code></li> <li>mIoU (SEM-SEG)</li> <li>PQ (PANO) via existing <code>evaluate_panoptic_quality(...)</code></li> <li>fixture evaluators:</li> <li><code>evaluate_fixture_payload(...)</code></li> <li><code>evaluate_fixture_file(...)</code></li> <li>built-in fallback payload <code>tiny_eval_fixture_payload()</code></li> <li>report writers:</li> <li><code>write_eval_reports(...)</code> emitting JSON + Markdown</li> <li>structured summary dataclass <code>EvalSummary</code></li> </ul> </li> <li>Updated exports in <code>apex_x/infer/__init__.py</code>:<ul> <li><code>EvalSummary</code>, <code>evaluate_fixture_file</code>, <code>evaluate_fixture_payload</code>,   <code>tiny_eval_fixture_payload</code>, <code>write_eval_reports</code></li> </ul> </li> <li>Updated CLI eval command in <code>apex_x/cli.py</code>:<ul> <li>supports:</li> <li><code>--fixture</code> (optional fixture JSON; defaults to built-in tiny payload)</li> <li><code>--report-json</code></li> <li><code>--report-md</code></li> <li>always emits metrics in stdout:</li> <li><code>det_map</code>, <code>mask_map</code>, <code>miou</code>, <code>panoptic_pq</code></li> <li>writes JSON and markdown report files per invocation</li> <li>keeps <code>--panoptic-pq</code> flag as compatibility no-op</li> </ul> </li> <li>Added tiny fixture + tests:<ul> <li>fixture: <code>tests/fixtures/eval_tiny_fixture.json</code></li> <li><code>tests/test_eval_metrics.py</code>:</li> <li>metric values on perfect tiny fixture</li> <li>JSON/Markdown report emission</li> <li>built-in tiny payload validation</li> <li><code>tests/test_cli.py</code>:</li> <li>eval command smoke with fixture + output report paths</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/infer/eval_metrics.py apex_x/infer/__init__.py apex_x/cli.py tests/test_eval_metrics.py tests/test_cli.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/infer/eval_metrics.py apex_x/infer/__init__.py apex_x/cli.py tests/test_eval_metrics.py tests/test_cli.py</code></li> <li><code>python -m pytest -q tests/test_eval_metrics.py tests/test_cli.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Ablation grid runner implemented in CLI with toggle sweeps and reports:</li> <li>Added <code>apex_x/train/ablation.py</code>:<ul> <li>toggle grid definitions for:</li> <li><code>router</code>, <code>budgeting</code>, <code>nesting</code>, <code>ssm</code>, <code>distill</code>,     <code>pcgrad</code>, <code>qat</code>, <code>panoptic</code>, <code>tracking</code></li> <li>deterministic grid builder:</li> <li><code>build_ablation_grid(...)</code> with per-toggle modes (<code>on/off/both</code>) and max-cap</li> <li>ablation execution:</li> <li>fixed-seed runs over grid combinations</li> <li>trainer invocation with <code>enable_budgeting</code> switch</li> <li>metrics aggregation (DET mAP, mask mAP, semantic mIoU, PQ, tracking consistency)</li> <li>routing stat aggregation (selected ratios, budget usage ratio, <code>mu_last</code>)</li> <li>report writers:</li> <li>CSV aggregate report</li> <li>markdown summary report</li> </ul> </li> <li>Updated <code>ApexXTrainer.run(...)</code> in <code>apex_x/train/trainer.py</code>:<ul> <li>added <code>enable_budgeting</code> flag for explicit budgeting on/off ablations</li> </ul> </li> <li>Updated exports in <code>apex_x/train/__init__.py</code>:<ul> <li>ablation dataclasses/functions (<code>AblationToggleSet</code>, grid runner, report writer)</li> </ul> </li> <li>Updated CLI <code>ablate</code> command in <code>apex_x/cli.py</code>:<ul> <li>added per-toggle mode flags (<code>--router/--budgeting/.../--tracking</code>)</li> <li>added fixed seed support via repeated <code>--seed</code></li> <li>added <code>--steps-per-stage</code>, <code>--max-experiments</code></li> <li>added report outputs:</li> <li><code>--output-csv</code></li> <li><code>--output-md</code></li> <li>command now runs grid + writes CSV/MD reports</li> </ul> </li> <li>Added tests:<ul> <li><code>tests/test_ablation.py</code>:</li> <li>grid construction behavior</li> <li>smoke run + CSV/MD output assertions</li> <li>updated <code>tests/test_cli.py</code>:</li> <li><code>ablate</code> command smoke with output artifact checks</li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/train/ablation.py apex_x/train/__init__.py apex_x/train/trainer.py apex_x/cli.py tests/test_ablation.py tests/test_cli.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/train/ablation.py apex_x/train/__init__.py apex_x/train/trainer.py apex_x/cli.py tests/test_ablation.py tests/test_cli.py</code></li> <li><code>python -m pytest -q tests/test_ablation.py tests/test_cli.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>INT8 QAT + PTQ fallback path implemented with FP16 router/gating policy:</li> <li>Added <code>apex_x/train/qat.py</code>:<ul> <li>activation observer + activation fake quant:</li> <li><code>ActivationObserver</code></li> <li><code>ActivationFakeQuant</code></li> <li>per-channel INT8 weight fake quant:</li> <li><code>WeightPerChannelFakeQuant</code></li> <li>wrapped train-time fake quant modules:</li> <li><code>FakeQuantConv2d</code></li> <li><code>FakeQuantLinear</code></li> <li>QAT/PTQ entrypoints:</li> <li><code>prepare_int8_qat(...)</code></li> <li><code>prepare_int8_ptq(...)</code></li> <li><code>calibrate_ptq(...)</code></li> <li>explicit wrapper traversal/state controls:</li> <li><code>iter_qat_wrappers(...)</code></li> <li><code>set_qat_state(...)</code></li> </ul> </li> <li>Updated <code>apex_x/train/trainer.py</code>:<ul> <li>quantization preparation step added before staged training:</li> <li>uses QAT when <code>train.qat_enable &amp;&amp; train.qat_int8</code></li> <li>uses PTQ calibration fallback when <code>runtime.precision_profile=edge</code> and QAT is off</li> <li>added deterministic calibration batch builder for PTQ fallback</li> <li>added quantization diagnostics to <code>train_summary[\"quantization\"]</code>:</li> <li><code>mode</code>, <code>wrapped_modules</code>, <code>calibration_batches</code>, <code>router_gating_fp16</code></li> <li>stage-3 routing gate path now keeps FP16 utility gating math and uses FP32 expected-cost accumulation</li> </ul> </li> <li>Updated <code>apex_x/train/__init__.py</code>:<ul> <li>exported QAT module types/functions for public train API surface</li> </ul> </li> <li>Added tests in <code>tests/test_qat.py</code>:<ul> <li>QAT wrapper conversion with router/gating skip policy</li> <li>PTQ calibration state transitions (observer off + fake quant on after calibration)</li> <li>trainer-level QAT/PTQ toggle smoke with finite loss/output checks</li> </ul> </li> <li>Added documentation:<ul> <li><code>docs/QAT.md</code> with INT8 policy, module behavior, trainer integration, and validation scope</li> <li>linked in <code>docs/index.md</code> and <code>mkdocs.yml</code></li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/train/qat.py apex_x/train/trainer.py apex_x/train/__init__.py tests/test_qat.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/train/qat.py apex_x/train/trainer.py apex_x/train/__init__.py</code></li> <li><code>python -m pytest -q tests/test_qat.py tests/test_trainer_stages.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>FP8-ready precision policy implemented with safe FP16 fallback:</li> <li>Added <code>apex_x/runtime/precision.py</code>:<ul> <li>precision policy dataclass:</li> <li><code>PrecisionPolicy</code></li> <li>runtime detection + resolution:</li> <li><code>resolve_precision_policy(...)</code></li> <li>conservative CUDA FP8 support gate (<code>sm90+</code> + torch FP8 dtype presence)</li> <li>dtype helpers:</li> <li><code>dtype_name(...)</code></li> <li>execution context helper:</li> <li><code>heavy_ops_autocast_context(...)</code><ul> <li>FP16 autocast path on CPU/CUDA</li> <li>FP8-ready no-op context pending specialized kernels/plugins</li> </ul> </li> </ul> </li> <li>Updated <code>apex_x/runtime/__init__.py</code> exports:<ul> <li><code>PrecisionPolicy</code>, <code>resolve_precision_policy</code>, <code>dtype_name</code>, <code>heavy_ops_autocast_context</code></li> </ul> </li> <li>Updated <code>apex_x/train/trainer.py</code>:<ul> <li>resolves precision policy at trainer init</li> <li>applies heavy-op autocast context during stage-1 teacher forward</li> <li>adds stage metrics:</li> <li><code>heavy_ops_dtype</code>, <code>fp8_enabled</code></li> <li>adds precision diagnostics in <code>train_summary[\"precision\"]</code>:</li> <li><code>profile</code>, <code>device</code>, <code>heavy_ops_dtype</code></li> <li><code>fp8_requested</code>, <code>fp8_enabled</code>, <code>fallback_reason</code></li> <li><code>router_dtype</code>, <code>kan_dtype</code></li> </ul> </li> <li>Updated <code>apex_x/train/qat.py</code>:<ul> <li>expanded INT8 wrapper skip policy to preserve FP16 for KAN-like modules:</li> <li>default skip tokens now include <code>\"kan\"</code> in addition to router/gating names</li> </ul> </li> <li>Added tests in <code>tests/test_precision_policy.py</code>:<ul> <li>CPU fallback smoke (<code>balanced</code> -&gt; FP16 fallback with explicit reason)</li> <li>mocked supported CUDA path enabling FP8 for heavy ops</li> <li>trainer summary smoke verifying fallback diagnostics</li> </ul> </li> <li>Added docs:<ul> <li><code>docs/FP8.md</code> documenting FP8 request rules, support detection, fallback contract, and smoke coverage</li> <li>linked from <code>docs/index.md</code> and <code>mkdocs.yml</code></li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/runtime/precision.py apex_x/runtime/__init__.py apex_x/train/trainer.py apex_x/train/qat.py tests/test_precision_policy.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/runtime/precision.py apex_x/runtime/__init__.py apex_x/train/trainer.py</code></li> <li><code>python -m pytest -q tests/test_precision_policy.py tests/test_trainer_stages.py tests/test_qat.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>Triton fused gather+gate+scatter path scaffolded with clean fallback to reference:</li> <li>Environment check result for this workspace:<ul> <li><code>torch.cuda.is_available() == False</code></li> <li>Triton package not installed</li> <li>therefore Triton kernel implementation path is unavailable in this run</li> </ul> </li> <li>Added <code>apex_x/runtime/triton_fused.py</code>:<ul> <li>availability model:</li> <li><code>TritonAvailability</code></li> <li><code>get_triton_availability()</code></li> <li>fused-result contract:</li> <li><code>FusedTileScatterResult</code></li> <li>reference fused pipeline:</li> <li><code>gather_gate_scatter_reference(...)</code></li> <li>implements:<ul> <li>gather selected heavy/base/proxy tiles</li> <li>per-pixel fusion gate application</li> <li>scatter with overlap priority semantics via <code>TileUnpackTorch</code></li> </ul> </li> <li>dispatch API:</li> <li><code>gather_gate_scatter(...)</code></li> <li>attempts Triton path when requested and available</li> <li>cleanly falls back to reference path when unavailable or stubbed</li> <li>explicit Triton kernel stub:</li> <li><code>_triton_fused_kernel_stub(...)</code> raises <code>NotImplementedError</code> (by design in no-Triton env)</li> </ul> </li> <li>Updated <code>apex_x/runtime/__init__.py</code> exports:<ul> <li><code>get_triton_availability</code></li> <li><code>gather_gate_scatter_reference</code></li> <li><code>gather_gate_scatter</code></li> <li>availability/result/backend dataclasses/types</li> </ul> </li> <li>Added microbenchmark script:<ul> <li><code>scripts/triton_fused_bench.py</code></li> <li>compares reference path vs dispatched fused path</li> <li>reports backend, fallback reason, and speed ratio</li> <li>works on CPU fallback path</li> </ul> </li> <li>Added tests:<ul> <li><code>tests/test_triton_fused.py</code>:</li> <li>reference fused path parity vs explicit reference composition</li> <li>dispatch fallback behavior in no-Triton/no-CUDA case</li> <li>forced Triton/no-fallback path raises stub error</li> <li><code>tests/test_triton_bench.py</code>:</li> <li>CPU smoke for benchmark utility</li> </ul> </li> <li>Added runtime docs:<ul> <li><code>docs/runtime/TRITON.md</code> describing dispatch contracts, fallback behavior, and benchmark usage</li> <li>linked in <code>docs/index.md</code> and <code>mkdocs.yml</code></li> </ul> </li> <li>Verification status:<ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/runtime/triton_fused.py apex_x/runtime/__init__.py tests/test_triton_fused.py tests/test_triton_bench.py scripts/triton_fused_bench.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/runtime/triton_fused.py apex_x/runtime/__init__.py</code></li> <li><code>python -m pytest -q tests/test_triton_fused.py tests/test_triton_bench.py tests/test_tile_pack_torch.py tests/test_tile_unpack_torch.py</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li>CPU performance regression suite implemented with baseline comparison gates:</li> <li>Added reusable perf suite module:<ul> <li><code>apex_x/bench/perf.py</code></li> <li>fixed-size infer benchmark (<code>ApexXModel.forward</code> on <code>[1,3,128,128]</code>)</li> <li>microbenchmarks:</li> <li><code>TilePackTorch</code></li> <li><code>TileUnpackTorch</code></li> <li><code>FusionGate</code></li> <li>report + compare helpers:</li> <li><code>run_cpu_perf_suite(...)</code></li> <li><code>compare_against_baseline(...)</code></li> <li>JSON read/write helpers</li> </ul> </li> <li>Updated <code>apex_x/bench/__init__.py</code> exports:<ul> <li>perf suite and compare utilities exposed</li> </ul> </li> <li>Replaced <code>scripts/perf_regression.py</code>:<ul> <li>runs suite and writes current JSON report</li> <li>optional baseline-template emit</li> <li>compare mode with pass/fail exit status for CI gating</li> <li>artifacts:</li> <li>current report JSON</li> <li>comparison summary JSON</li> </ul> </li> <li>Added committed CPU baseline:<ul> <li><code>scripts/perf_baseline_cpu.json</code></li> <li>per-metric tolerances via:</li> <li><code>max_regression_ratio</code></li> <li><code>max_regression_abs_ms</code></li> </ul> </li> <li>Added tests:<ul> <li><code>tests/test_perf_regression.py</code></li> <li>suite smoke coverage</li> <li>baseline compare pass/fail behavior</li> </ul> </li> <li>Added documentation:<ul> <li><code>docs/PERF.md</code></li> <li>linked from <code>docs/index.md</code> and <code>mkdocs.yml</code></li> </ul> </li> <li>Updated CI workflow:<ul> <li><code>.github/workflows/ci.yml</code> now includes <code>perf-regression</code> job on <code>ubuntu-latest</code> (CPU-only)</li> <li>job executes <code>scripts/perf_regression.py --compare ...</code></li> <li>job uploads perf artifacts (<code>perf_current_ci.json</code>, <code>perf_compare_ci.json</code>)</li> </ul> </li> <li> <p>Verification status:</p> <ul> <li>targeted checks passed:</li> <li><code>python -m ruff check apex_x/bench/perf.py apex_x/bench/__init__.py scripts/perf_regression.py tests/test_perf_regression.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/bench/perf.py apex_x/bench/__init__.py scripts/perf_regression.py</code></li> <li><code>python -m pytest -q tests/test_perf_regression.py</code></li> <li><code>python scripts/perf_regression.py --compare --baseline scripts/perf_baseline_cpu.json --output artifacts/perf_current_test.json --summary artifacts/perf_compare_test.json --infer-iters 15 --micro-iters 25 --infer-warmup 3 --micro-warmup 3</code></li> <li>full checks passed:</li> <li><code>python -m ruff check .</code></li> <li><code>python -m mypy --cache-dir=/dev/null</code></li> <li><code>python -m pytest -q</code></li> </ul> </li> <li> <p>TensorRT + Go runtime scaffolding added (Task A/B):</p> </li> <li>TensorRT C++ scaffold created under <code>runtime/tensorrt/</code>:<ul> <li><code>CMakeLists.txt</code> with optional feature probes:</li> <li><code>APEXX_ENABLE_TENSORRT</code> only when <code>NvInfer.h</code> is found</li> <li><code>APEXX_ENABLE_CUDA</code> only when CUDA compiler is available</li> <li>stub plugin interfaces/sources:</li> <li><code>TilePack</code></li> <li><code>TileSSMScan</code></li> <li><code>TileUnpackFusion</code></li> <li>optional <code>DecodeNMS</code></li> <li>utility binary:</li> <li><code>apexx_trt_plugin_info</code> (prints build summary and plugin flags)</li> </ul> </li> <li>TensorRT docs added:<ul> <li><code>docs/runtime/TENSORRT.md</code></li> <li>contract mapping from plugin spec to scaffold classes</li> <li>guarded build instructions:</li> <li><code>cd runtime/tensorrt &amp;&amp; cmake -S . -B build &amp;&amp; cmake --build build -j</code></li> </ul> </li> <li>Go microservice scaffold created under <code>runtime/go/</code>:<ul> <li>service entrypoint:</li> <li><code>runtime/go/cmd/apexx-runtime/main.go</code></li> <li>endpoints:</li> <li><code>POST /predict</code></li> <li><code>GET /health</code></li> <li><code>GET /metrics</code></li> <li>short-window batching queue with per-request budget profile support (<code>quality|balanced|edge</code>)</li> <li>adapters:</li> <li>ONNX Runtime CPU baseline scaffold (<code>ORTAdapter</code>)</li> <li>TensorRT CGO scaffold with build tags:<ul> <li><code>//go:build tensorrt &amp;&amp; cgo</code></li> <li>default fallback returns clear unavailable error</li> </ul> </li> <li>containerization:</li> <li><code>runtime/go/Dockerfile</code></li> <li><code>runtime/go/docker-compose.yml</code></li> <li>tests:</li> <li><code>runtime/go/internal/service/batcher_test.go</code></li> <li><code>runtime/go/internal/service/http_test.go</code></li> </ul> </li> <li>CI/docs integration updates:<ul> <li><code>.github/workflows/ci.yml</code> now includes <code>go-runtime</code> job (<code>go test ./...</code> in <code>runtime/go</code>)</li> <li><code>mkdocs.yml</code> + <code>docs/index.md</code> now link <code>docs/runtime/TENSORRT.md</code></li> <li><code>runtime/README.md</code> and root <code>README.md</code> updated with runtime scaffold usage</li> </ul> </li> <li>Build/run commands verified for scaffolds:<ul> <li>Go tests:</li> <li><code>cd runtime/go &amp;&amp; go test ./...</code></li> <li>Go service:</li> <li><code>cd runtime/go &amp;&amp; go run ./cmd/apexx-runtime -addr :8080 -adapter onnxruntime</code></li> <li>TensorRT scaffold build commands documented (not executed in this environment due missing <code>cmake</code>):</li> <li><code>cd runtime/tensorrt &amp;&amp; cmake -S . -B build &amp;&amp; cmake --build build -j</code></li> </ul> </li> <li> <p>Remaining work from this milestone:</p> <ul> <li>implement real TensorRT plugin classes (<code>IPluginV2DynamicExt</code>) + serialization</li> <li>replace ORT stub adapter with true ONNX Runtime session execution</li> <li>implement TensorRT CGO adapter bridge to compiled plugin/runtime binaries</li> <li>add optional gRPC server for the Go service (HTTP baseline is complete)</li> </ul> </li> <li> <p>Runtime capability detection module implemented:</p> </li> <li>Added <code>apex_x/runtime/caps.py</code> with unified runtime probe object:<ul> <li><code>RuntimeCaps</code></li> <li><code>cuda: CudaCaps</code></li> <li><code>triton: TritonCaps</code></li> <li><code>tensorrt: TensorRTCaps</code></li> <li><code>fp8: FP8Caps</code></li> <li>exported from <code>apex_x/runtime/__init__.py</code></li> </ul> </li> <li>Detection coverage:<ul> <li>CUDA availability + device name + compute capability</li> <li>Triton availability + version</li> <li>TensorRT:</li> <li>Python package/module availability</li> <li>header availability (<code>NvInfer.h</code>/<code>NvInferRuntime.h</code>) via:<ul> <li>explicit <code>header_search_paths</code></li> <li>env hints (<code>TENSORRT_INCLUDE_DIR</code>, <code>TRT_INCLUDE_DIR</code>, <code>TENSORRT_ROOT</code>, <code>TRT_ROOT</code>, <code>CUDA_HOME</code>, <code>CUDA_PATH</code>)</li> <li>common include directories</li> </ul> </li> <li>INT8 availability gate for TRT usage (<code>BuilderFlag.INT8</code> + CUDA)</li> <li>FP8 availability gate:</li> <li>torch FP8 dtype support</li> <li>CUDA presence</li> <li>compute capability <code>sm90+</code></li> </ul> </li> <li>Added tests (CPU-safe, mock-driven):<ul> <li><code>tests/test_caps_runtime.py</code></li> <li><code>tests/test_caps_tensorrt_fp8.py</code></li> </ul> </li> <li>Added docs:<ul> <li><code>docs/runtime/CAPS.md</code></li> <li>linked in <code>docs/index.md</code> and <code>mkdocs.yml</code></li> </ul> </li> <li>Usage instructions:<ul> <li>basic:</li> <li><code>from apex_x.runtime import detect_runtime_caps</code></li> <li><code>caps = detect_runtime_caps()</code></li> <li><code>caps.to_dict()</code></li> <li>explicit TRT header path:</li> <li><code>detect_runtime_caps(header_search_paths=[\"/usr/local/TensorRT/include\"])</code></li> </ul> </li> <li> <p>Validation status:</p> <ul> <li><code>python -m pytest -q tests/test_caps_runtime.py tests/test_caps_tensorrt_fp8.py</code></li> <li><code>python -m ruff check apex_x/runtime/caps.py tests/test_caps_runtime.py tests/test_caps_tensorrt_fp8.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/runtime/caps.py apex_x/runtime/__init__.py</code></li> </ul> </li> <li> <p>Runtime parity framework implemented:</p> </li> <li>Added <code>apex_x/runtime/parity.py</code> with backend-agnostic parity APIs:<ul> <li><code>ParityCase</code>, <code>run_parity_case(...)</code>, <code>evaluate_parity_outputs(...)</code></li> <li>tolerance controls:</li> <li><code>NumericTolerance</code></li> <li><code>ToleranceConfig</code> (<code>default</code>, <code>fp16</code>, <code>bf16</code>, <code>int8</code>)</li> <li>reporting objects:</li> <li><code>TensorParityStats</code></li> <li><code>ParityReport</code></li> <li><code>format_parity_report(...)</code></li> </ul> </li> <li>Determinism contract:<ul> <li><code>run_parity_case(...)</code> calls <code>seed_all(seed, deterministic=...)</code> before input generation</li> </ul> </li> <li>Metrics emitted per compared tensor:<ul> <li><code>max_abs_err</code>, <code>mean_abs_err</code></li> <li><code>max_rel_err</code>, <code>mean_rel_err</code></li> <li><code>mismatch_count</code>, <code>total_count</code>, <code>mismatch_ratio</code></li> <li>pass/fail against configured tolerance + mismatch-ratio limit</li> </ul> </li> <li>Exported from <code>apex_x/runtime/__init__.py</code> for direct runtime use</li> <li>Added tests:<ul> <li><code>tests/test_parity_framework_core.py</code></li> <li><code>tests/test_parity_framework_tolerances.py</code></li> <li>tests are CPU-safe and use small shapes for CI speed</li> </ul> </li> <li>Added documentation:<ul> <li><code>docs/runtime/PARITY.md</code></li> <li>linked in <code>docs/index.md</code> and <code>mkdocs.yml</code></li> </ul> </li> <li>Usage instructions:<ul> <li>create a <code>ParityCase</code> with <code>input_factory</code>, <code>reference_fn</code>, and <code>candidate_fn</code></li> <li>run <code>run_parity_case(case, seed=..., deterministic=True)</code></li> <li>serialize/report with <code>report.to_dict()</code> or <code>format_parity_report(report)</code></li> </ul> </li> <li> <p>Validation status:</p> <ul> <li><code>python -m ruff check apex_x/runtime/parity.py apex_x/runtime/__init__.py tests/test_parity_framework_core.py tests/test_parity_framework_tolerances.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/runtime/parity.py apex_x/runtime/__init__.py</code></li> <li><code>python -m pytest -q tests/test_parity_framework_core.py tests/test_parity_framework_tolerances.py</code></li> <li><code>.venv/bin/mkdocs build --strict</code></li> </ul> </li> <li> <p>Triton TilePack gather kernel implemented with fallback dispatch:</p> </li> <li>Added new kernel module:<ul> <li><code>apex_x/kernels/triton/tilepack.py</code></li> </ul> </li> <li>Added package exports:<ul> <li><code>apex_x/kernels/__init__.py</code></li> <li><code>apex_x/kernels/triton/__init__.py</code></li> </ul> </li> <li>Implemented Triton kernel contract:<ul> <li>Input: <code>F[B,C,H,W]</code> contiguous <code>NCHW</code></li> <li>Input indices: <code>idx[B,K]</code> integer (<code>int32</code> kernel path; <code>int64</code> accepted and cast)</li> <li>Output: <code>P[B,K,C,t,t]</code> contiguous</li> </ul> </li> <li>Kernel path support:<ul> <li><code>fp16</code>, <code>bf16</code> on CUDA</li> <li>no Python tile loops in kernel gather path</li> </ul> </li> <li>Added vectorized reference fallback:<ul> <li><code>tilepack_reference(...)</code> uses tensor gather (no per-tile Python loops)</li> </ul> </li> <li>Added dispatch behavior:<ul> <li><code>tilepack_dispatch(...)</code></li> <li>falls back when Triton/CUDA unavailable</li> <li>falls back when <code>requires_grad</code> and <code>inference_only=True</code></li> <li>reason: Triton path is inference-oriented without custom backward registration</li> </ul> </li> <li>Added parity tests:<ul> <li><code>tests/test_triton_tilepack_parity_dispatch.py</code></li> <li><code>tests/test_triton_tilepack_parity_gpu.py</code></li> <li>GPU parity auto-skips when Triton/CUDA unavailable</li> </ul> </li> <li>Added benchmark:<ul> <li><code>apex_x/bench/triton_tilepack_bench.py</code></li> </ul> </li> <li>Added docs:<ul> <li><code>docs/runtime/TRITON_TILEPACK.md</code></li> <li><code>docs/runtime/TRITON.md</code> updated with TilePack status</li> <li>docs navigation updated in <code>docs/index.md</code> and <code>mkdocs.yml</code></li> </ul> </li> <li>Run commands:<ul> <li>tests:</li> <li><code>python -m pytest -q tests/test_triton_tilepack_parity_dispatch.py tests/test_triton_tilepack_parity_gpu.py</code></li> <li>bench (module):</li> <li><code>python -m apex_x.bench.triton_tilepack_bench --iters 50 --warmup 10 --batch 1 --channels 128 --height 128 --width 128 --tile-size 8 --kmax 32 --dtype fp16</code></li> <li>lint/type:</li> <li><code>python -m ruff check apex_x/kernels/triton/tilepack.py apex_x/bench/triton_tilepack_bench.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tilepack.py apex_x/bench/triton_tilepack_bench.py</code></li> </ul> </li> <li> <p>Validation status:</p> <ul> <li><code>python -m ruff check ...</code> passed</li> <li><code>python -m mypy --cache-dir=/dev/null ...</code> passed</li> <li><code>python -m pytest -q tests/test_triton_tilepack_parity_dispatch.py tests/test_triton_tilepack_parity_gpu.py</code> passed (GPU tests skipped on CPU-only env)</li> <li><code>.venv/bin/mkdocs build --strict</code> passed</li> </ul> </li> <li> <p>Triton TileUnpack scatter kernel extended to overlap + priority semantics:</p> </li> <li>Added kernel module:<ul> <li><code>apex_x/kernels/triton/tileunpack.py</code></li> </ul> </li> <li>Added Triton kernel dispatch/availability API:<ul> <li><code>get_triton_tileunpack_availability()</code></li> <li><code>tileunpack_reference(...)</code></li> <li><code>tileunpack_triton(...)</code></li> <li><code>tileunpack_dispatch(...)</code></li> </ul> </li> <li>Added exports:<ul> <li><code>apex_x/kernels/triton/__init__.py</code></li> </ul> </li> <li>Implemented semantics:<ul> <li>Inputs: <code>F_base[B,C,H,W]</code>, <code>P_out[B,K,C,t,t]</code>, and <code>idx/meta</code></li> <li>Output: <code>F_merged[B,C,H,W]</code></li> <li>deterministic overlap overwrite with priorities:</li> <li>per-tile <code>levels[B,K]</code> (higher level wins)</li> <li>or pre-sorted K-order (<code>assume_priority_sorted=True</code>) as implicit priority</li> <li>default mode: <code>overlap_mode=\\\"override\\\"</code> (priority overwrite)</li> <li>optional mode: <code>overlap_mode=\\\"blend\\\"</code> (currently reference fallback)</li> </ul> </li> <li>Updated tests:<ul> <li><code>tests/test_triton_tileunpack_parity_dispatch.py</code></li> <li><code>tests/test_triton_tileunpack_parity_gpu.py</code></li> <li><code>tests/test_triton_tileunpack_overlap_dispatch.py</code></li> <li><code>tests/test_triton_tileunpack_overlap_gpu.py</code></li> <li>includes synthetic overlap fixtures and parity against reference behavior</li> </ul> </li> <li>Updated microbenchmark:<ul> <li><code>apex_x/bench/triton_tileunpack_bench.py</code></li> <li>supports overlap stress via <code>--overlap-shift</code></li> <li>supports level-aware runs via default levels (<code>--no-levels</code> to disable)</li> </ul> </li> <li>Added docs:<ul> <li><code>docs/runtime/TRITON_TILEUNPACK.md</code></li> <li>updated <code>docs/runtime/TRITON.md</code></li> <li>updated docs nav (<code>docs/index.md</code>, <code>mkdocs.yml</code>)</li> </ul> </li> <li>Run commands:<ul> <li>parity tests:</li> <li><code>python -m pytest -q tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py</code></li> <li>microbench:</li> <li><code>python -m apex_x.bench.triton_tileunpack_bench --batch 1 --channels 128 --height 128 --width 128 --tile-size 8 --kmax 32 --overlap-shift 4 --warmup 10 --iters 50 --dtype fp16</code></li> <li>lint/type:</li> <li><code>python -m ruff check apex_x/kernels/triton/tileunpack.py apex_x/bench/triton_tileunpack_bench.py tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tileunpack.py apex_x/bench/triton_tileunpack_bench.py</code></li> </ul> </li> <li> <p>Validation status:</p> <ul> <li><code>python -m ruff check ...</code> passed</li> <li><code>python -m mypy --cache-dir=/dev/null ...</code> passed</li> <li><code>python -m pytest -q tests/test_triton_tileunpack_parity_dispatch.py tests/test_triton_tileunpack_parity_gpu.py tests/test_triton_tileunpack_overlap_dispatch.py tests/test_triton_tileunpack_overlap_gpu.py</code> passed (GPU tests skipped on CPU-only env)</li> <li><code>python -m apex_x.bench.triton_tileunpack_bench --iters 3 --warmup 1 --batch 1 --channels 8 --height 32 --width 32 --tile-size 4 --kmax 4 --overlap-shift 2 --dtype fp16</code> executed successfully (reference backend on CPU)</li> <li><code>.venv/bin/mkdocs build --strict</code> passed</li> </ul> </li> <li> <p>Triton FusionGate alpha/fusion kernels implemented with fallback dispatch:</p> </li> <li>Added kernel module:<ul> <li><code>apex_x/kernels/triton/fusiongate.py</code></li> </ul> </li> <li>Added exports:<ul> <li><code>apex_x/kernels/triton/__init__.py</code></li> </ul> </li> <li>Implemented kernels and dispatch:<ul> <li>alpha kernel:</li> <li>inputs: boundary/uncertainty proxies (<code>[B,1,H,W]</code> or <code>[B,H,W]</code>)</li> <li>output: <code>alpha[B,1,H,W]</code></li> <li>formula: <code>alpha = sigmoid(softplus(w_b) * boundary + softplus(w_u) * uncertainty + bias)</code></li> <li>optional fusion kernel:</li> <li><code>fused = base + alpha * (detail - base)</code></li> <li>supports optional in-place output in dispatch API</li> <li>fallback semantics:</li> <li>falls back to reference when Triton/CUDA unavailable</li> <li>falls back when autograd is requested and <code>inference_only=True</code></li> </ul> </li> <li>Added tests:<ul> <li><code>tests/test_triton_fusiongate_parity_dispatch.py</code></li> <li><code>tests/test_triton_fusiongate_parity_gpu.py</code></li> <li>coverage:</li> <li>parity vs <code>apex_x.model.FusionGate.compute_alpha</code> (simplified alpha path)</li> <li>alpha range checks (<code>[0,1]</code>)</li> <li>optional fusion parity</li> <li>GPU parity auto-skip without CUDA+Triton</li> </ul> </li> <li>Added microbenchmark:<ul> <li><code>apex_x/bench/triton_fusiongate_bench.py</code></li> <li>measures:</li> <li>alpha reference vs dispatch</li> <li>alpha+fusion reference vs dispatch</li> </ul> </li> <li>Added docs:<ul> <li><code>docs/runtime/TRITON_FUSION.md</code></li> <li>updated:</li> <li><code>docs/runtime/TRITON.md</code></li> <li><code>docs/index.md</code></li> <li><code>mkdocs.yml</code></li> </ul> </li> <li>Run commands:<ul> <li>tests:</li> <li><code>python -m pytest -q tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py</code></li> <li>benchmark:</li> <li><code>python -m apex_x.bench.triton_fusiongate_bench --batch 1 --channels 128 --height 128 --width 128 --warmup 10 --iters 50 --dtype fp16</code></li> <li>lint/type:</li> <li><code>python -m ruff check apex_x/kernels/triton/fusiongate.py apex_x/bench/triton_fusiongate_bench.py tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/fusiongate.py apex_x/bench/triton_fusiongate_bench.py</code></li> </ul> </li> <li>Validation status:<ul> <li><code>python -m ruff check ...</code> passed</li> <li><code>python -m mypy --cache-dir=/dev/null ...</code> passed</li> <li><code>python -m pytest -q tests/test_triton_fusiongate_parity_dispatch.py tests/test_triton_fusiongate_parity_gpu.py</code> passed (GPU tests skipped on CPU-only env)</li> <li><code>python -m apex_x.bench.triton_fusiongate_bench --iters 3 --warmup 1 --batch 1 --channels 8 --height 32 --width 32 --dtype fp16</code> executed successfully (reference backend on CPU)</li> <li><code>.venv/bin/mkdocs build --strict</code> passed</li> </ul> </li> </ul>"},{"location":"CONTEXT/#invariants-to-preserve","title":"Invariants to Preserve","text":"<ul> <li>Deterministic inference tile selection under fixed config</li> <li>Fixed <code>Kmax</code>-buffer shape contract for runtime compatibility</li> <li>No Python-side dynamic control flow in future export graph path</li> <li>CPU baseline must remain runnable at all times</li> </ul>"},{"location":"CONTEXT/#open-risks","title":"Open Risks","text":"<ul> <li>Tile-SSM is currently a placeholder scan, not final kernel-equivalent behavior</li> <li>Detection/segmentation heads are minimal baseline stubs</li> <li>Runtime plugins are currently specification-only, not implemented</li> </ul>"},{"location":"CONTEXT/#immediate-next-steps","title":"Immediate Next Steps","text":"<ol> <li>Expand baseline heads to full DET + INST-SEG proto path per spec.</li> <li>Add explicit continuous-budget training loop example with dual <code>mu</code> update.</li> <li>Add deterministic quadtree <code>L1/L2</code> split implementation and tests.</li> <li>Add export smoke tests (ONNX contract checks with fixed <code>Kmax</code>).</li> <li>Add perf threshold config file and CI perf guard for CPU baseline.</li> <li>Add CLI entrypoints (Typer + Rich) for <code>run</code>, <code>test</code>, and <code>perf</code> commands.</li> <li>Add initial concrete <code>losses/</code>, <code>train/</code>, <code>infer/</code>, and <code>export/</code> implementations beyond placeholders.</li> <li>Add a typed Typer command for <code>config validate --config path.yaml --set key=value</code> using new loader/override utilities.</li> <li>Add logging configuration knobs into <code>RuntimeConfig</code> (log level/format) and route through <code>configure_logging()</code>.</li> <li>Add <code>apex-x config validate</code> and <code>apex-x config dump</code> subcommands for explicit config workflows.</li> <li>Add docs deployment workflow (e.g., GitHub Pages) after docs structure stabilizes.</li> <li>Add real TRT/ORT <code>RuntimeAdapterProtocol</code> implementations behind feature flags.</li> <li>Add smoke example invocation to README and optionally CI as a dedicated quick check.</li> <li>Add ADR template/checklist for future decisions to keep decision records uniform.</li> <li>Integrate L0 mapping helpers directly inside pack/unpack metadata paths (store tile <code>(ty,tx)</code> alongside pixel origins) for easier runtime plugin parity checks.</li> <li>Add non-square grid Hilbert fixture coverage (e.g., <code>3x5</code>, <code>5x3</code>) to lock padded-power-of-two traversal behavior.</li> <li>Add explicit fixture snapshots for scan modes (<code>l2r/r2l/u2d/d2u</code>) on representative non-square grids and enforce them in CI.</li> <li>Connect split-budget selection (<code>B2/B3</code>) in inference path to quadtree depth-2 mappings/metadata and add end-to-end selection tests.</li> <li>Integrate <code>TileSelection</code>/<code>TileSelectionTrace</code> emission into model inference outputs and add CLI flag to dump selection traces for ablations.</li> <li>Add optional CLI command to generate overlay images from stored <code>TileSelectionTrace</code> JSON for quick qualitative routing inspection.</li> <li>Wire <code>StaticCostModel</code> into routing/inference selection path so budgeting uses per-level <code>C_c/C_h</code> + pack/unpack/split overhead directly instead of scalar placeholders.</li> <li>Integrate <code>sample_oracle_set(...)</code> into training loops so oracle subset <code>S</code> is produced from random + uncertainty-biased policies directly from PV <code>u_hat</code>.</li> <li>Add budget-selection debug artifact that logs per-tile score/rank and final stable tie-break order for exact replay in ablation runs.</li> <li>Integrate PV aggregation output <code>x_i</code> into router training/inference path so utility heads consume pooled <code>mean/max/var</code> vectors instead of placeholder signals.</li> <li>Wire <code>RouterTinyMLP</code> into model/routing execution path as the default trainable router backend (with config-selectable fallback to <code>IdentityRouter</code>).</li> <li>Add config switch for router backend (<code>identity</code> / <code>tiny_mlp</code> / <code>kan_like</code>) and wire <code>RouterKANLike</code> into inference/training stubs.</li> <li>Integrate <code>ste_gate_from_utilities(...)</code> into training stubs so router utilities produce <code>p_i</code>/<code>g_i</code> directly in continuous-budget examples.</li> <li>Wire <code>BudgetDualController</code> into training stubs so <code>mu</code>, <code>E[C]</code>, and budget term are tracked/updated per step with debug logs.</li> <li>Use <code>GreedySelectionResult.kmax_buffer</code> + <code>valid_count</code> directly in model inference outputs to mirror runtime plugin shape contracts.</li> <li>Integrate <code>deterministic_two_stage_selection(...)</code> into model inference path so <code>B1/B2</code> and <code>L1</code> routing are exercised end-to-end in CPU baseline outputs.</li> <li>Use <code>hysteresis_rollout(...)</code> in temporal/video inference stubs and log <code>count_mask_toggles(...)</code> as an anti-flicker metric.</li> <li>Extend routing diagnostics to include L1/L2 selection once two-stage routing is wired into the model forward path.</li> <li>Add optional artifact export for diagnostics snapshots (JSON + histogram plots) from CLI train/predict commands for ablation workflows.</li> <li>Wire toggle states into YAML examples/README config snippets so users can reproduce dense/no-SSM/no-nesting baselines quickly.</li> <li>Integrate torch tile pack/unpack path into runtime adapter abstractions and add a CPU fallback selection path for adapter-level smoke tests.</li> <li>Integrate <code>FusionGate</code> into the model forward path (or runtime adapter path) to replace direct heavy overwrite with proxy-conditioned fusion in CPU baseline experiments.</li> <li>Integrate <code>CheapBlock</code> into PV/FF cheap path stubs and add micro-benchmarks for block latency under CPU profiles.</li> <li>Integrate <code>TileRefineBlock</code> after Tile-SSM in the model forward path so packed-tile local refinement is exercised end-to-end in baseline outputs.</li> <li>Wire <code>PVBackbone</code> into model execution path as the canonical PV stream source (<code>P3/P4/P5</code>) and align routing signals to these outputs.</li> <li>Integrate <code>PVModule</code> into <code>ApexXModel.forward</code> so routing and diagnostics consume PV coarse proxies instead of handcrafted tile-signal placeholders.</li> <li>Replace/augment NumPy <code>tile_ssm_scan</code> usage in <code>ApexXModel.forward</code> with <code>StableStateSpaceScan</code> in torch execution paths and add parity checks for inference outputs.</li> <li>Add <code>ApexXModel</code> config switch for scan direction mode (<code>forward</code> vs <code>bidirectional</code>) and wire merge-gated bidirectional scan into packed-tile path.</li> <li>Wire <code>decode_and_nms(...)</code> into model/inference outputs so DET head predictions use the new deterministic decode/NMS path in end-to-end CPU runs.</li> <li>Wire <code>PrototypeInstanceSegHead</code> into end-to-end model/infer path (using DET-selected instances) and expose assembled masks in CLI <code>predict</code> outputs.</li> <li>Integrate <code>instance_segmentation_losses(...)</code> into training stubs with mask/box matching targets and log BCE/Dice/boundary components in trainer diagnostics.</li> <li>Wire <code>FFTileRefinementHook</code> active-tile indices from routing outputs in model/infer path so refinement uses real FF-selected tiles end-to-end.</li> <li>Wire <code>generate_panoptic_output(...)</code> into inference/CLI outputs so panoptic maps and <code>segments_info</code> are emitted from DET + INST-SEG + SEM-SEG predictions end-to-end.</li> <li>Wire <code>evaluate_panoptic_quality(...)</code> into dataset evaluation loops so <code>apex-x eval</code> can consume real predicted/GT panoptic artifacts and report dataset-level PQ metrics.</li> <li>Integrate <code>TrackEmbeddingHead</code> into model/infer outputs and add config-controlled tracking head enable/disable behavior.</li> <li>Add a basic association loop wrapper in <code>apex_x/infer</code> that keeps <code>TrackState</code> across frames and emits stable track IDs in CLI <code>predict</code>.</li> <li>Add optional motion gating term into Hungarian cost/gate path (for video mode) and verify flicker reduction with temporal fixtures.</li> <li>Integrate <code>apply_pcgradpp(...)</code> into concrete training step codepath so DET/SEG grouped losses are projected on shared trunk params during optimization and surfaced in trainer diagnostics.</li> <li>Integrate <code>distillation_losses(...)</code> into the concrete training path with config-driven weights/temperature/feature-layer selection and expose per-component values in trainer diagnostics.</li> <li>Integrate <code>compute_oracle_delta_targets(...)</code> + <code>utility_oracle_loss(...)</code> into router training loops so sampled set <code>S</code> drives utility regression/ranking with detached oracle targets.</li> <li>Wire <code>TeacherModel</code> into train/eval loops so EMA updates, distill outputs, and student-teacher loss plumbing are exercised end-to-end with config toggles.</li> <li>Expand <code>ApexXTrainer</code> stage loop from smoke-level synthetic batches to dataset-backed dataloaders with checkpointing/resume support.</li> <li>Add stage-aware CLI logging/artifacts (<code>stage_metrics.json</code>, <code>mu_history.json</code>) for ablation reproducibility.</li> <li>Add CI smoke command for staged training CLI (<code>apex-x train --steps-per-stage 1</code>) to guard regressions in train wiring.</li> <li>Wire <code>TransformPipeline</code> and <code>MosaicV2</code> into an actual dataset/dataloader path controlled by <code>DataConfig</code> knobs (<code>flip_prob</code>, <code>mosaic_prob</code>, scale range).</li> <li>Add serialization/debug helpers to visualize transformed boxes/masks and mosaic split/crop decisions for reproducible augmentation ablations.</li> <li>Add dataset-wide evaluation loops that consume real model predictions and emit the new eval report (JSON/MD) directly from inference artifacts, beyond tiny fixture mode.</li> <li>Extend ablation runner to ingest real dataset eval outputs (instead of tiny fixture metrics) and add per-toggle significance summaries across seeds.</li> <li>Expand QAT coverage beyond Conv/Linear wrappers to selected normalization-sensitive blocks with explicit parity gates versus FP16 baseline.</li> <li>Add runtime-backed FP8 kernel probe path (beyond capability check) and enforce parity/perf gates before enabling FP8-by-default on compatible GPUs.</li> <li>Implement real Triton fused gather+gate+scatter kernel and wire it under <code>gather_gate_scatter(...)</code> dispatch when CUDA+Triton are available; add parity + perf thresholds against reference path.</li> <li>Add dataset/profile-specific perf baselines (e.g., quality/balanced/edge configs) and split tolerances by CPU model class for stricter regression gates.</li> </ol>"},{"location":"CONTEXT/#latest-update-2026-02-08-triton-fused-stage-1-pipeline","title":"Latest Update (2026-02-08): Triton Fused Stage-1 Pipeline","text":"<ul> <li>Added a new practical fused Triton fast path module:</li> <li><code>apex_x/kernels/triton/fused_pack_op_unpack.py</code></li> <li>Implements <code>gather -&gt; pointwise affine + ReGLU-like gate -&gt; scatter</code> in one Triton kernel launch sequence.</li> <li>Added dispatch + fallback API:<ul> <li><code>get_triton_fused_stage1_availability()</code></li> <li><code>fused_pack_op_unpack_reference(...)</code></li> <li><code>fused_pack_op_unpack_triton(...)</code></li> <li><code>fused_pack_op_unpack_dispatch(...)</code></li> </ul> </li> <li>Determinism rule for Stage-1 path:<ul> <li>requires unique tile indices per batch row to avoid overlap write races.</li> </ul> </li> <li>Added exports:</li> <li><code>apex_x/kernels/triton/__init__.py</code> now exports fused Stage-1 APIs.</li> <li>Added parity tests:</li> <li><code>tests/test_triton_fused_stage1_dispatch.py</code></li> <li><code>tests/test_triton_fused_stage1_gpu.py</code></li> <li>Added microbenchmark:</li> <li><code>apex_x/bench/triton_fused_stage1_bench.py</code></li> <li>compares:<ul> <li>explicit reference composition (<code>pack -&gt; op -&gt; unpack</code>)</li> <li>separate dispatch composition (<code>TilePack dispatch -&gt; op -&gt; TileUnpack dispatch</code>)</li> <li>fused Stage-1 dispatch</li> </ul> </li> <li>Added docs:</li> <li><code>docs/runtime/TRITON_FUSED_STAGE1.md</code></li> <li>updated:<ul> <li><code>docs/runtime/TRITON.md</code></li> <li><code>docs/index.md</code></li> <li><code>mkdocs.yml</code></li> </ul> </li> </ul>"},{"location":"CONTEXT/#run-commands","title":"Run Commands","text":"<ul> <li>Tests:</li> <li><code>python -m pytest -q tests/test_triton_fused_stage1_dispatch.py tests/test_triton_fused_stage1_gpu.py</code></li> <li>Microbenchmark:</li> <li><code>python -m apex_x.bench.triton_fused_stage1_bench --batch 1 --channels 128 --height 128 --width 128 --tile-size 8 --kmax 32 --warmup 10 --iters 50 --dtype fp16</code></li> <li>Lint/type quick checks:</li> <li><code>python -m ruff check apex_x/kernels/triton/fused_pack_op_unpack.py tests/test_triton_fused_stage1_dispatch.py tests/test_triton_fused_stage1_gpu.py apex_x/bench/triton_fused_stage1_bench.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/fused_pack_op_unpack.py apex_x/bench/triton_fused_stage1_bench.py</code></li> </ul>"},{"location":"CONTEXT/#remaining-work","title":"Remaining Work","text":"<ul> <li>Wire Stage-1 fused kernel into legacy runtime entrypoint:</li> <li><code>apex_x/runtime/triton_fused.py::gather_gate_scatter(...)</code></li> <li>Extend fused kernel beyond Stage-1 local transform:</li> <li>add overlap-priority semantics in-kernel where needed</li> <li>integrate Tile-SSM-related fused blocks (next stages)</li> <li>Add GPU CI perf threshold gates for <code>speedup_separate_over_fused</code>.</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-triton-tilessm-scan-baseline","title":"Latest Update (2026-02-08): Triton TileSSM Scan Baseline","text":"<ul> <li>Added Triton TileSSM scan module:</li> <li><code>apex_x/kernels/triton/tilessm_scan.py</code></li> <li>Forward-only recurrence scan over tokens <code>tokens[B,K,C]</code> with stable sanitization/clamping.</li> <li>Outputs:<ul> <li><code>y[B,K,C]</code></li> <li><code>final_state[B,C]</code></li> </ul> </li> <li>Added availability + dispatch API:<ul> <li><code>get_triton_tilessm_availability()</code></li> <li><code>tilessm_scan_reference(...)</code></li> <li><code>tilessm_scan_triton(...)</code></li> <li><code>tilessm_scan_dispatch(...)</code></li> </ul> </li> <li>Dispatch keeps training-safe behavior:<ul> <li><code>inference_only=True</code> falls back to reference when autograd is active.</li> </ul> </li> <li>Exported TileSSM API:</li> <li><code>apex_x/kernels/triton/__init__.py</code></li> <li>Integrated inference path into model heavy FF scan:</li> <li>updated <code>apex_x/model/ff_heavy_path.py</code><ul> <li>new <code>use_triton_inference_scan</code> toggle</li> <li>eval mode uses <code>tilessm_scan_dispatch(...)</code></li> <li>train mode keeps torch scan path (<code>StableStateSpaceScan</code> / <code>StableBidirectionalStateSpaceScan</code>)</li> </ul> </li> <li>updated <code>apex_x/model/ff_module.py</code><ul> <li>routes <code>RuntimeConfig.enable_runtime_plugins</code> to <code>FFHeavyPath(..., use_triton_inference_scan=...)</code></li> </ul> </li> <li>Added tests:</li> <li><code>tests/test_triton_tilessm_parity_dispatch.py</code></li> <li><code>tests/test_triton_tilessm_parity_gpu.py</code></li> <li><code>tests/test_ff_heavy_path_tilessm_dispatch.py</code></li> <li>Added throughput benchmark:</li> <li><code>apex_x/bench/triton_tilessm_bench.py</code></li> <li>Added docs:</li> <li><code>docs/runtime/TRITON_SSM.md</code></li> <li>updated:<ul> <li><code>docs/runtime/TRITON.md</code></li> <li><code>docs/index.md</code></li> <li><code>mkdocs.yml</code></li> </ul> </li> </ul>"},{"location":"CONTEXT/#run-commands_1","title":"Run Commands","text":"<ul> <li>Tests:</li> <li><code>python -m pytest -q tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py tests/test_ff_heavy_path.py</code></li> <li>Benchmark:</li> <li><code>python -m apex_x.bench.triton_tilessm_bench --batch 2 --steps 256 --channels 128 --warmup 10 --iters 50 --dtype fp16</code></li> <li>Lint/type checks:</li> <li><code>python -m ruff check apex_x/kernels/triton/tilessm_scan.py apex_x/model/ff_heavy_path.py apex_x/model/ff_module.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tilessm_scan.py apex_x/model/ff_heavy_path.py apex_x/model/ff_module.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py</code></li> <li>Docs:</li> <li><code>.venv/bin/mkdocs build --strict</code></li> </ul>"},{"location":"CONTEXT/#validation-status","title":"Validation Status","text":"<ul> <li><code>ruff</code>: passed on changed TileSSM files.</li> <li><code>mypy</code>: passed on changed TileSSM files.</li> <li><code>pytest</code>: passed for new parity/integration tests (GPU tests auto-skipped on CPU-only environment).</li> <li>benchmark smoke run: completed on CPU fallback path.</li> <li>docs build: passed with strict mode.</li> </ul>"},{"location":"CONTEXT/#remaining-work_1","title":"Remaining Work","text":"<ul> <li>Add multi-direction scan execution mode in Triton TileSSM path (current kernel is forward-only baseline).</li> <li>Add a fused TileSSM + tile-local refine path after this baseline.</li> <li>Add GPU CI lane for TileSSM parity/perf thresholds when CUDA runners are available.</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-triton-tilessm-multi-direction","title":"Latest Update (2026-02-08): Triton TileSSM Multi-Direction","text":"<ul> <li>Extended <code>apex_x/kernels/triton/tilessm_scan.py</code> to support directional scanning:</li> <li><code>direction</code>: <code>forward</code>, <code>backward</code>, <code>bidirectional</code></li> <li><code>merge_mode</code> for bidirectional: <code>sum</code>, <code>avg</code>, <code>gated</code></li> <li>optional torch-computed <code>merge_gate</code> for gated merge (<code>[C]</code> or <code>[B,1,C]</code>)</li> <li>Added clean directional API:</li> <li><code>scan(tokens, direction=...) -&gt; y</code></li> <li>routes through dispatch with fallback behavior</li> <li>Kept training/inference separation:</li> <li>training/backward still uses torch scan path</li> <li>inference dispatch can use Triton path (<code>inference_only=True</code>)</li> <li>Updated FF inference integration:</li> <li><code>apex_x/model/ff_heavy_path.py</code></li> <li>Triton inference path now uses directional dispatch (<code>forward</code> and <code>backward</code>) and applies learned torch gate for merge in bidirectional mode.</li> <li>Updated exports:</li> <li><code>apex_x/kernels/triton/__init__.py</code> now exports:<ul> <li><code>ScanDirection</code></li> <li><code>BidirectionalMergeMode</code></li> <li><code>scan</code></li> </ul> </li> <li>Updated benchmark for multi-direction overhead:</li> <li><code>apex_x/bench/triton_tilessm_bench.py</code></li> <li>now reports forward/backward/bidirectional timings and overhead ratios vs forward.</li> <li>Updated tests:</li> <li><code>tests/test_triton_tilessm_parity_dispatch.py</code><ul> <li>added backward parity vs torch manual recurrence</li> <li>added bidirectional parity for <code>sum/avg/gated</code></li> <li>added clean API <code>scan(...)</code> test</li> </ul> </li> <li><code>tests/test_triton_tilessm_parity_gpu.py</code><ul> <li>added bidirectional avg parity test (GPU)</li> </ul> </li> <li>Updated docs:</li> <li><code>docs/runtime/TRITON_SSM.md</code></li> <li><code>docs/runtime/TRITON.md</code></li> </ul>"},{"location":"CONTEXT/#run-commands_2","title":"Run Commands","text":"<ul> <li>Tests:</li> <li><code>python -m pytest -q tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py tests/test_ff_heavy_path_tilessm_dispatch.py tests/test_ff_heavy_path.py</code></li> <li>Benchmark:</li> <li><code>python -m apex_x.bench.triton_tilessm_bench --batch 2 --steps 256 --channels 128 --warmup 10 --iters 50 --dtype fp16</code></li> <li>Lint/type:</li> <li><code>python -m ruff check apex_x/kernels/triton/tilessm_scan.py apex_x/kernels/triton/__init__.py apex_x/model/ff_heavy_path.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py</code></li> <li><code>python -m mypy --cache-dir=/dev/null apex_x/kernels/triton/tilessm_scan.py apex_x/kernels/triton/__init__.py apex_x/model/ff_heavy_path.py apex_x/bench/triton_tilessm_bench.py tests/test_triton_tilessm_parity_dispatch.py tests/test_triton_tilessm_parity_gpu.py</code></li> <li>Docs:</li> <li><code>.venv/bin/mkdocs build --strict</code></li> </ul>"},{"location":"CONTEXT/#validation-status_1","title":"Validation Status","text":"<ul> <li><code>ruff</code>: passed</li> <li><code>mypy</code>: passed</li> <li><code>pytest</code>: passed (GPU tests skipped on CPU-only environment)</li> <li>benchmark smoke run: passed (reference fallback on CPU)</li> <li>docs build (<code>mkdocs --strict</code>): passed</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-tensorrt-build-hardening-harness","title":"Latest Update (2026-02-08): TensorRT Build Hardening + Harness","text":"<ul> <li>Read runtime specs from:</li> <li><code>docs/runtime/PLUGIN_SPEC.md</code> (canonical)</li> <li><code>docs/runtime/TENSORRT.md</code></li> <li>Added alias page: <code>docs/runtime/PLUGIN_SPECS.md</code> -&gt; points to canonical spec</li> <li>Hardened TensorRT CMake in <code>runtime/tensorrt/CMakeLists.txt</code>:</li> <li>shared plugin library support:<ul> <li><code>apexx_trt_plugins</code> (SHARED)</li> </ul> </li> <li>added always-build static core:<ul> <li><code>apexx_trt_plugin_core</code> (STATIC, PIC)</li> </ul> </li> <li>compile-guard behavior:<ul> <li>shared plugin library builds only when TensorRT headers and CUDA compiler are found</li> <li>if TRT/CUDA unavailable, shared build is skipped cleanly and repo remains buildable</li> </ul> </li> <li>plugin info target kept available:<ul> <li><code>apexx_trt_plugin_info</code></li> </ul> </li> <li>harness target added conditionally (only with shared build):<ul> <li><code>apexx_trt_plugin_harness</code></li> </ul> </li> <li>Added minimal plugin enqueue-like path for stubs:</li> <li><code>runtime/tensorrt/include/apexx_trt/plugin_stub.hpp</code><ul> <li><code>DummyTensor</code>, <code>PluginEnqueueInputs</code>, <code>PluginEnqueueOutputs</code>, <code>PluginStub::enqueue(...)</code></li> </ul> </li> <li>implemented enqueue methods in:<ul> <li><code>runtime/tensorrt/src/tile_pack_plugin.cpp</code></li> <li><code>runtime/tensorrt/src/tile_ssm_scan_plugin.cpp</code></li> <li><code>runtime/tensorrt/src/tile_unpack_fusion_plugin.cpp</code></li> <li><code>runtime/tensorrt/src/decode_nms_plugin.cpp</code></li> </ul> </li> <li>Added shared-library C ABI entrypoints in:</li> <li><code>runtime/tensorrt/include/apexx_trt/common.hpp</code></li> <li><code>runtime/tensorrt/src/common.cpp</code></li> <li>symbols:<ul> <li><code>apexx_trt_abi_version()</code></li> <li><code>apexx_trt_build_summary_cstr()</code></li> <li><code>apexx_trt_invoke_minimal(...)</code></li> </ul> </li> <li>Added minimal runtime harness executable source:</li> <li><code>runtime/tensorrt/tests/plugin_harness_main.cpp</code></li> <li>harness behavior:<ul> <li>loads plugin shared library via <code>dlopen</code>/<code>LoadLibrary</code></li> <li>resolves C ABI symbols</li> <li>creates dummy tensors</li> <li>invokes minimal plugin call path for TilePack/TileSSMScan/TileUnpackFusion/DecodeNMS</li> </ul> </li> <li>Added build doc:</li> <li><code>docs/runtime/TENSORRT_BUILD.md</code></li> <li>Updated docs navigation and runtime note cross-links:</li> <li><code>mkdocs.yml</code></li> <li><code>docs/index.md</code></li> <li><code>docs/runtime/TENSORRT.md</code></li> </ul>"},{"location":"CONTEXT/#exact-build-commands","title":"Exact Build Commands","text":"<ul> <li>Auto-detect build:</li> <li><code>cd runtime/tensorrt</code></li> <li><code>cmake -S . -B build</code></li> <li><code>cmake --build build -j</code></li> <li><code>./build/apexx_trt_plugin_info</code></li> <li>Explicit TRT/CUDA paths:</li> <li><code>cd runtime/tensorrt</code></li> <li><code>cmake -S . -B build -DTENSORRT_INCLUDE_DIR=\\\"${TENSORRT_ROOT}/include\\\" -DCMAKE_CUDA_COMPILER=\\\"${CUDA_HOME}/bin/nvcc\\\"</code></li> <li><code>cmake --build build -j</code></li> <li>Force skip shared plugin build (portable/CI machines):</li> <li><code>cd runtime/tensorrt</code></li> <li><code>cmake -S . -B build -DAPEXX_ENABLE_TENSORRT=OFF -DAPEXX_ENABLE_CUDA=OFF -DAPEXX_BUILD_PLUGIN_TEST_HARNESS=OFF</code></li> <li><code>cmake --build build -j</code></li> <li>Harness run (when shared plugin target is built):</li> <li><code>./build/apexx_trt_plugin_harness ./build/libapexx_trt_plugins.so</code></li> <li>or:</li> <li><code>export APEXX_TRT_PLUGIN_LIB=./build/libapexx_trt_plugins.so</code></li> <li><code>./build/apexx_trt_plugin_harness</code></li> </ul>"},{"location":"CONTEXT/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>TENSORRT_ROOT</code>: TensorRT install root (optional)</li> <li><code>CUDA_HOME</code>: CUDA root (optional)</li> <li><code>CMAKE_PREFIX_PATH</code>: dependency discovery override (optional)</li> <li><code>APEXX_TRT_PLUGIN_LIB</code>: path to shared plugin library for harness runtime loading</li> </ul>"},{"location":"CONTEXT/#validation-status_2","title":"Validation Status","text":"<ul> <li><code>mkdocs build --strict</code>: passed</li> <li><code>pytest tests/test_import_smoke.py</code>: passed</li> <li>Local CMake configure/build execution could not be run in this environment because <code>cmake</code> binary is not installed (<code>command not found</code>).</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-tensorrt-tilepack-plugin-real-implementation","title":"Latest Update (2026-02-08): TensorRT TilePack Plugin (Real Implementation)","text":"<ul> <li>Read and aligned implementation to:</li> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> <li><code>docs/runtime/PLUGIN_SPECS.md</code> (alias page)</li> <li><code>docs/runtime/TENSORRT.md</code></li> <li>Implemented real TensorRT TilePack plugin under:</li> <li><code>runtime/tensorrt/plugins/tilepack.h</code></li> <li><code>runtime/tensorrt/plugins/tilepack.cpp</code></li> <li><code>runtime/tensorrt/plugins/tilepack.cu</code></li> <li>Implemented plugin contract and behavior:</li> <li>plugin type/version/namespace:<ul> <li><code>TilePack</code> / <code>1</code> / <code>apexx</code></li> </ul> </li> <li>tensor contract:<ul> <li>input0: <code>F[B,C,H,W]</code> FP16</li> <li>input1: <code>idx[B,K]</code> INT32</li> <li>output0: <code>P[B,K,C,t,t]</code> FP16</li> </ul> </li> <li><code>getOutputDimensions(...)</code> shape inference for dynamic dims.</li> <li><code>supportsFormatCombination(...)</code> with linear format + required dtypes.</li> <li>serialization/deserialization of <code>tile_size</code>.</li> <li><code>enqueue(...)</code> calls CUDA helper <code>launch_tilepack_fp16(...)</code>.</li> <li>Implemented CUDA gather kernel:</li> <li>launches over total elements in <code>P</code></li> <li>maps each output element to source <code>F</code> via tile index and local <code>(dy, dx)</code></li> <li>zero-fills invalid tile indices / out-of-bounds accesses</li> <li>contiguous output layout <code>[B,K,C,t,t]</code>.</li> <li>Added C++ integration/parity test harness:</li> <li><code>runtime/tensorrt/tests/tilepack_plugin_test.cpp</code></li> <li>test covers:<ul> <li>creator-based plugin construction</li> <li>plugin serialization/deserialization</li> <li><code>enqueue(...)</code> execution on CUDA buffers</li> <li>parity vs host reference implementation.</li> </ul> </li> <li>Added Python parity test for engine-level integration:</li> <li><code>tests/test_tensorrt_tilepack_parity.py</code></li> <li>builds TRT engine with TilePack plugin (when TRT Python + CUDA available)</li> <li>compares output to PyTorch reference (<code>tilepack_reference</code>).</li> <li>Hardened CMake wiring for real plugin path in <code>runtime/tensorrt/CMakeLists.txt</code>:</li> <li>fixed runtime library detection guard so <code>nvinfer/cudart</code> discovery actually runs.</li> <li>gated shared plugin target creation on runtime library availability to avoid configure/link failures on partial installs.</li> <li>real plugin build remains optional and guarded.</li> <li><code>apexx_trt_tilepack_test</code> only builds when real plugin path is enabled.</li> <li>Updated docs:</li> <li><code>docs/runtime/TENSORRT_BUILD.md</code></li> <li><code>docs/runtime/TENSORRT.md</code></li> </ul>"},{"location":"CONTEXT/#run-commands_3","title":"Run Commands","text":"<ul> <li>Python-level checks:</li> <li><code>python -m pytest -q tests/test_tensorrt_tilepack_parity.py tests/test_import_smoke.py</code></li> <li>TensorRT C++ build/test (when <code>cmake</code>, TensorRT, CUDA available):</li> <li><code>cd runtime/tensorrt</code></li> <li><code>cmake -S . -B build -DAPEXX_ENABLE_REAL_TILEPACK_PLUGIN=ON</code></li> <li><code>cmake --build build -j</code></li> <li><code>./build/apexx_trt_tilepack_test</code></li> <li>Python TRT parity (optional):</li> <li><code>export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so</code></li> <li><code>python -m pytest -q tests/test_tensorrt_tilepack_parity.py</code></li> </ul>"},{"location":"CONTEXT/#validation-status_3","title":"Validation Status","text":"<ul> <li><code>python -m pytest -q tests/test_tensorrt_tilepack_parity.py tests/test_import_smoke.py</code>:</li> <li>passed</li> <li>TRT parity test skipped on CPU-only environment (expected).</li> <li>Local CMake build/test for TensorRT plugin could not be executed in this environment because <code>cmake</code> is unavailable.</li> </ul>"},{"location":"CONTEXT/#remaining-work_2","title":"Remaining Work","text":"<ul> <li>Add INT8 support path for TilePack plugin (currently FP16-only).</li> <li>Add stronger shape/stride guard coverage for dynamic-shape edge cases in C++ tests.</li> <li>Add CI GPU lane for TensorRT plugin parity/perf once CUDA runners are available.</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-tensorrt-tileunpackfusion-plugin-priority-alpha","title":"Latest Update (2026-02-08): TensorRT TileUnpackFusion Plugin (Priority + Alpha)","text":"<ul> <li>Read and aligned implementation to:</li> <li><code>docs/CONTEXT.md</code></li> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> <li><code>docs/runtime/PLUGIN_SPECS.md</code></li> <li>Implemented real TensorRT <code>TileUnpackFusion</code> plugin:</li> <li><code>runtime/tensorrt/plugins/tileunpackfusion.h</code></li> <li><code>runtime/tensorrt/plugins/tileunpackfusion.cpp</code></li> <li><code>runtime/tensorrt/plugins/tileunpackfusion.cu</code></li> <li>Plugin contract implemented (TensorRT dynamic ext):</li> <li>inputs:<ul> <li><code>F_base[B,C,H,W]</code> FP16</li> <li><code>P_out[B,K,C,t,t]</code> FP16</li> <li><code>idx[B,K]</code> INT32</li> <li><code>levels[B,K]</code> INT32 (priority source for deterministic nesting semantics)</li> <li>optional <code>alpha[B,1,H,W]</code> FP16</li> </ul> </li> <li>output:<ul> <li><code>F_merged[B,C,H,W]</code> FP16</li> </ul> </li> <li>Runtime semantics implemented:</li> <li>deterministic overwrite priority per pixel:<ul> <li>higher <code>levels</code> wins (e.g. <code>L2 &gt; L1 &gt; L0</code>)</li> <li>tie-break inside same level by tile order <code>k</code> (later <code>k</code> wins)</li> </ul> </li> <li>two-pass CUDA path:<ul> <li>pass 1: atomic winner-key map over <code>[B,H,W]</code></li> <li>pass 2: scatter only winner tile pixels</li> </ul> </li> <li>fusion behavior:<ul> <li>without alpha: overwrite winner pixel value</li> <li>with alpha: <code>F = F_base + alpha * (F_winner - F_base)</code> (per pixel/channel)</li> </ul> </li> <li>output map starts from dense base copy, so non-selected pixels stay unchanged</li> <li>CMake/runtime wiring:</li> <li>updated <code>runtime/tensorrt/CMakeLists.txt</code></li> <li>added build option:<ul> <li><code>APEXX_ENABLE_REAL_TILEUNPACKFUSION_PLUGIN</code> (default <code>ON</code>)</li> </ul> </li> <li>real plugin sources are linked into shared plugin library when TRT/CUDA libs are available</li> <li>added C++ test target:<ul> <li><code>apexx_trt_tileunpackfusion_test</code></li> </ul> </li> <li>Added C++ priority correctness harness:</li> <li><code>runtime/tensorrt/tests/tileunpackfusion_plugin_test.cpp</code></li> <li>crafted overlap case validates <code>L2</code> overwrite over <code>L1/L0</code> on same region</li> <li>Added Python TensorRT parity test vs PyTorch reference path:</li> <li><code>tests/test_tensorrt_tileunpackfusion_parity.py</code></li> <li>compares TRT plugin output to <code>TileUnpackTorch + FusionGate</code> composed reference</li> <li>Updated docs:</li> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> <li><code>docs/runtime/TENSORRT.md</code></li> <li><code>docs/runtime/TENSORRT_BUILD.md</code></li> </ul>"},{"location":"CONTEXT/#run-commands_4","title":"Run Commands","text":"<ul> <li>Python checks (CPU env will skip CUDA-dependent TRT tests):</li> <li><code>python -m ruff check tests/test_tensorrt_tileunpackfusion_parity.py</code></li> <li><code>python -m pytest -q tests/test_tensorrt_tileunpackfusion_parity.py tests/test_tensorrt_tilepack_parity.py tests/test_import_smoke.py</code></li> <li>C++ build/test (when <code>cmake</code> + TensorRT + CUDA available):</li> <li><code>cd runtime/tensorrt</code></li> <li><code>cmake -S . -B build -DAPEXX_ENABLE_REAL_TILEUNPACKFUSION_PLUGIN=ON</code></li> <li><code>cmake --build build -j</code></li> <li><code>./build/apexx_trt_tileunpackfusion_test</code></li> <li>Optional Python parity run with plugin library:</li> <li><code>export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so</code></li> <li><code>python -m pytest -q tests/test_tensorrt_tileunpackfusion_parity.py</code></li> </ul>"},{"location":"CONTEXT/#validation-status_4","title":"Validation Status","text":"<ul> <li><code>ruff</code> on new parity test: passed.</li> <li><code>pytest</code> targeted run:</li> <li>passed for non-TRT tests</li> <li>TRT parity tests skipped in this CPU-only environment (expected).</li> <li>Local TensorRT C++ compile/run for the new plugin was not executed here because <code>cmake</code> is unavailable in this environment.</li> </ul>"},{"location":"CONTEXT/#remaining-work_3","title":"Remaining Work","text":"<ul> <li>Add native blend-mode merge path inside TensorRT plugin (currently overwrite + optional alpha fusion path).</li> <li>Add stricter input-range safeguards for very large/negative <code>levels</code> to prevent key-overflow edge cases.</li> <li>Add GPU CI lane that builds <code>apexx_trt_tileunpackfusion_test</code> and runs Python TRT parity.</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-tensorrt-tilessmscan-plugin-forward-backward-flag","title":"Latest Update (2026-02-08): TensorRT TileSSMScan Plugin (Forward + Backward Flag)","text":"<ul> <li>Read and aligned implementation to:</li> <li><code>docs/CONTEXT.md</code></li> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> <li><code>docs/runtime/PLUGIN_SPECS.md</code></li> <li>Implemented real TensorRT <code>TileSSMScan</code> plugin:</li> <li><code>runtime/tensorrt/plugins/tilessm_scan.h</code></li> <li><code>runtime/tensorrt/plugins/tilessm_scan.cpp</code></li> <li><code>runtime/tensorrt/plugins/tilessm_scan.cu</code></li> <li>Plugin contract implemented (TensorRT dynamic ext):</li> <li>inputs:<ul> <li><code>tokens[B,K,C]</code> FP16</li> <li><code>decay[C]</code>, <code>input_gain[C]</code>, <code>output_gain[C]</code>, <code>state_bias[C]</code> FP16</li> <li>optional <code>init_state[B,C]</code> FP16</li> </ul> </li> <li>outputs:<ul> <li><code>y[B,K,C]</code> FP16</li> <li><code>final_state[B,C]</code> FP16</li> </ul> </li> <li>plugin fields:<ul> <li><code>direction</code> (<code>0=forward</code>, <code>1=backward</code>)</li> <li><code>clamp_value</code> (default <code>1e4</code>)</li> </ul> </li> <li>Runtime recurrence semantics implemented to match torch placeholder:</li> <li>per-channel recurrence across <code>K</code>:<ul> <li><code>state = decay * state + (1 - decay) * (input_gain * token + state_bias)</code></li> <li><code>y = output_gain * state</code></li> </ul> </li> <li>stability constraints:<ul> <li><code>decay</code> clamped to <code>(1e-6, 1-1e-6)</code></li> <li><code>token</code> NaN sanitization + clamp to <code>[-clamp_value, clamp_value]</code></li> </ul> </li> <li>backward direction scans from <code>K-1</code> to <code>0</code> while writing outputs in original index positions.</li> <li>CMake/runtime wiring:</li> <li>updated <code>runtime/tensorrt/CMakeLists.txt</code></li> <li>added build option:<ul> <li><code>APEXX_ENABLE_REAL_TILESSM_PLUGIN</code> (default <code>ON</code>)</li> </ul> </li> <li>real plugin sources are linked into shared plugin library when TRT/CUDA libs are available</li> <li>added C++ test/benchmark target:<ul> <li><code>apexx_trt_tilessm_test</code></li> </ul> </li> <li>Added C++ plugin test + microbenchmark harness:</li> <li><code>runtime/tensorrt/tests/tilessm_scan_plugin_test.cpp</code></li> <li>validates plugin output vs host reference for:<ul> <li>forward direction</li> <li>backward direction</li> </ul> </li> <li>prints lightweight performance metrics from CUDA-event timing:<ul> <li>average latency (ms)</li> <li>token throughput (tok/s)</li> </ul> </li> <li>Added Python TensorRT parity tests:</li> <li><code>tests/test_tensorrt_tilessm_parity.py</code></li> <li>compares TensorRT engine outputs to <code>tilessm_scan_reference(...)</code> for forward/backward on small shapes.</li> <li>Updated docs:</li> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> <li><code>docs/runtime/TENSORRT.md</code></li> <li><code>docs/runtime/TENSORRT_BUILD.md</code></li> </ul>"},{"location":"CONTEXT/#run-commands_5","title":"Run Commands","text":"<ul> <li>Python checks (CPU env will skip CUDA-dependent TRT tests):</li> <li><code>python -m ruff check tests/test_tensorrt_tilessm_parity.py</code></li> <li><code>python -m pytest -q tests/test_tensorrt_tilessm_parity.py tests/test_tensorrt_tilepack_parity.py tests/test_tensorrt_tileunpackfusion_parity.py tests/test_import_smoke.py</code></li> <li>C++ build/test/bench (when <code>cmake</code> + TensorRT + CUDA available):</li> <li><code>cd runtime/tensorrt</code></li> <li><code>cmake -S . -B build -DAPEXX_ENABLE_REAL_TILESSM_PLUGIN=ON</code></li> <li><code>cmake --build build -j</code></li> <li><code>./build/apexx_trt_tilessm_test</code></li> <li>Optional Python parity run with plugin library:</li> <li><code>export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so</code></li> <li><code>python -m pytest -q tests/test_tensorrt_tilessm_parity.py</code></li> </ul>"},{"location":"CONTEXT/#validation-status_5","title":"Validation Status","text":"<ul> <li><code>ruff</code> on new TRT TileSSM parity test: passed.</li> <li><code>pytest</code> targeted run:</li> <li>passed for non-TRT tests</li> <li>TRT parity tests skipped in this CPU-only environment (expected).</li> <li>Local TensorRT C++ compile/run for new TileSSM plugin was not executed here because <code>cmake</code> is unavailable in this environment.</li> </ul>"},{"location":"CONTEXT/#remaining-work_4","title":"Remaining Work","text":"<ul> <li>Add TensorRT-side multi-direction merge modes (<code>sum/avg/gated</code>) beyond single-direction plugin field.</li> <li>Add broader dtype support (<code>fp32</code>/<code>bf16</code>) if needed for debugging and parity triage.</li> <li>Add GPU CI lane that builds and runs <code>apexx_trt_tilessm_test</code> and Python TRT parity tests.</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-tensorrt-decodenms-plugin-det-postprocessing-in-engine","title":"Latest Update (2026-02-08): TensorRT Decode+NMS Plugin (DET Postprocessing In-Engine)","text":"<ul> <li>Read and aligned implementation to:</li> <li><code>docs/CONTEXT.md</code></li> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> <li><code>docs/ENGINEERING_SPEC.md</code> (DET decode/NMS behavior)</li> <li>Implemented real TensorRT Decode+NMS plugin:</li> <li><code>runtime/tensorrt/plugins/nms_decode.h</code></li> <li><code>runtime/tensorrt/plugins/nms_decode.cpp</code></li> <li><code>runtime/tensorrt/plugins/nms_decode.cu</code></li> <li>Plugin contract implemented (TensorRT dynamic ext):</li> <li>inputs:<ul> <li><code>cls_logits[B,N,C]</code> FP16</li> <li><code>box_reg[B,N,4]</code> FP16 (<code>l,t,r,b</code> logits)</li> <li><code>quality[B,N]</code> FP16</li> <li><code>centers[N,2]</code> FP16</li> <li><code>strides[N]</code> FP16</li> </ul> </li> <li>outputs:<ul> <li><code>boxes[B,max_det,4]</code> FP16</li> <li><code>scores[B,max_det]</code> FP16</li> <li><code>class_ids[B,max_det]</code> INT32</li> <li><code>valid_counts[B]</code> INT32</li> </ul> </li> <li>plugin fields:<ul> <li><code>max_detections</code></li> <li><code>pre_nms_topk</code></li> <li><code>score_threshold</code></li> <li><code>iou_threshold</code></li> </ul> </li> <li>Runtime semantics implemented to match project PyTorch path:</li> <li>decode:<ul> <li><code>dist = softplus(clamp(box_reg,-20,20)) * stride</code></li> <li><code>xyxy</code> from <code>(cx,cy)</code> and decoded distances</li> </ul> </li> <li>score:<ul> <li><code>sigmoid(clamp(cls,-60,60)) * sigmoid(clamp(quality,-60,60))</code></li> </ul> </li> <li>candidate selection:<ul> <li>threshold by <code>score_threshold</code></li> <li>top-k by <code>pre_nms_topk</code> with deterministic tie-break (<code>pair_id = anchor*C + class</code>)</li> </ul> </li> <li>class-wise deterministic NMS:<ul> <li>IoU suppression per class</li> <li>final ordering by score descending, tie-break by pair-id ascending</li> </ul> </li> <li>padding semantics:<ul> <li>invalid rows use <code>class_ids=-1</code>, <code>scores=0</code>, <code>boxes=0</code></li> </ul> </li> <li>CMake/runtime wiring:</li> <li>updated <code>runtime/tensorrt/CMakeLists.txt</code></li> <li>added build option:<ul> <li><code>APEXX_ENABLE_REAL_NMS_DECODE_PLUGIN</code> (default <code>ON</code>)</li> </ul> </li> <li>added C++ test target:<ul> <li><code>apexx_trt_nms_decode_test</code></li> </ul> </li> <li>Added C++ plugin parity/benchmark harness:</li> <li><code>runtime/tensorrt/tests/nms_decode_plugin_test.cpp</code></li> <li>compares plugin outputs to host reference</li> <li>includes corner case checks:<ul> <li>no boxes (high threshold)</li> <li>many candidates</li> </ul> </li> <li>includes lightweight latency metric printout</li> <li>Added Python parity tests vs PyTorch decode+nms reference:</li> <li><code>tests/test_tensorrt_nms_decode_parity.py</code></li> <li>covers:<ul> <li>fixed-seed parity</li> <li>no boxes corner case</li> <li>many boxes corner case</li> </ul> </li> <li>Updated docs:</li> <li>new:<ul> <li><code>docs/runtime/TENSORRT_POST.md</code></li> </ul> </li> <li>updated:<ul> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> <li><code>docs/runtime/TENSORRT.md</code></li> <li><code>docs/runtime/TENSORRT_BUILD.md</code></li> <li><code>docs/index.md</code></li> <li><code>mkdocs.yml</code></li> </ul> </li> </ul>"},{"location":"CONTEXT/#run-commands_6","title":"Run Commands","text":"<ul> <li>Python checks (CPU env will skip CUDA/TRT tests):</li> <li><code>python -m ruff check tests/test_tensorrt_nms_decode_parity.py tests/test_tensorrt_tilepack_parity.py</code></li> <li><code>python -m pytest -q tests/test_tensorrt_nms_decode_parity.py tests/test_tensorrt_tilepack_parity.py tests/test_tensorrt_tilessm_parity.py tests/test_tensorrt_tileunpackfusion_parity.py tests/test_det_decode_nms.py tests/test_import_smoke.py</code></li> <li>C++ build/test (when <code>cmake</code> + TensorRT + CUDA available):</li> <li><code>cd runtime/tensorrt</code></li> <li><code>cmake -S . -B build -DAPEXX_ENABLE_REAL_NMS_DECODE_PLUGIN=ON</code></li> <li><code>cmake --build build -j</code></li> <li><code>./build/apexx_trt_nms_decode_test</code></li> <li>Optional Python TRT parity:</li> <li><code>export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so</code></li> <li><code>python -m pytest -q tests/test_tensorrt_nms_decode_parity.py</code></li> </ul>"},{"location":"CONTEXT/#validation-status_6","title":"Validation Status","text":"<ul> <li><code>ruff</code> on changed TensorRT parity tests: passed.</li> <li><code>pytest</code> targeted suite:</li> <li>passed for CPU-safe tests</li> <li>TRT tests skipped on CPU-only environment (expected).</li> <li><code>mkdocs build --strict</code> could not run in this environment because <code>mkdocs</code> is not installed.</li> <li>Local TensorRT C++ compile/run for new Decode+NMS plugin was not executed here because <code>cmake</code> is unavailable in this environment.</li> </ul>"},{"location":"CONTEXT/#remaining-work_5","title":"Remaining Work","text":"<ul> <li>Optional backend integration path to TensorRT EfficientNMS (when exact deterministic parity requirements are satisfied).</li> <li>Performance optimization of CUDA kernel (current implementation prioritizes correctness/determinism over throughput).</li> <li>GPU CI lane to compile/run <code>apexx_trt_nms_decode_test</code> and Python TRT decode+NMS parity tests.</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-tensorrt-python-engine-builder-int8-calibrator","title":"Latest Update (2026-02-08): TensorRT Python Engine Builder + INT8 Calibrator","text":"<ul> <li>Read and aligned implementation to:</li> <li><code>docs/CONTEXT.md</code></li> <li><code>docs/ENGINEERING_SPEC.md</code></li> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> <li><code>docs/runtime/TENSORRT.md</code></li> <li><code>docs/runtime/TENSORRT_BUILD.md</code></li> <li><code>docs/runtime/TENSORRT_POST.md</code></li> <li>Added Python TensorRT builder package:</li> <li><code>apex_x/runtime/tensorrt/__init__.py</code></li> <li><code>apex_x/runtime/tensorrt/builder.py</code></li> <li><code>apex_x/runtime/tensorrt/calibrator.py</code></li> <li>Added API exports:</li> <li>updated <code>apex_x/runtime/__init__.py</code></li> <li>runtime now exposes:<ul> <li><code>TensorRTEngineBuilder</code></li> <li><code>TensorRTEngineBuildConfig</code></li> <li><code>EngineBuildResult</code></li> <li><code>TensorRTEntropyCalibrator</code></li> <li>calibration typing/config objects</li> </ul> </li> <li>Builder capabilities implemented:</li> <li>build from ONNX:<ul> <li><code>TensorRTEngineBuilder.build_from_onnx(...)</code></li> </ul> </li> <li>build from direct TRT network factory:<ul> <li><code>TensorRTEngineBuilder.build_from_network(...)</code></li> </ul> </li> <li>optional plugin-library loading via <code>ctypes</code> with global symbol visibility</li> <li>custom plugin registry checks for:<ul> <li>required: <code>TilePack</code>, <code>TileSSMScan</code>, <code>TileUnpackFusion</code></li> <li>optional: <code>DecodeNMS</code></li> </ul> </li> <li>FP16 build support</li> <li>INT8 build support with calibrator attachment and cache path</li> <li>router/KAN FP16 constraints during INT8 builds via layer-name keywords</li> <li>Calibrator capabilities implemented:</li> <li><code>TensorRTEntropyCalibrator</code> (IInt8EntropyCalibrator2-backed when TRT is available)</li> <li>streams calibration batches from iterable loader</li> <li>supports batch format:<ul> <li>single-input <code>np.ndarray</code></li> <li>multi-input <code>dict[str, np.ndarray]</code></li> </ul> </li> <li>stable cache I/O:<ul> <li><code>read_calibration_cache()</code></li> <li><code>write_calibration_cache(...)</code></li> </ul> </li> <li>keeps device tensors alive across <code>get_batch(...)</code> calls</li> <li>Added docs:</li> <li>new:<ul> <li><code>docs/runtime/TENSORRT_INT8.md</code></li> </ul> </li> <li>updated:<ul> <li><code>docs/runtime/TENSORRT.md</code></li> <li><code>docs/index.md</code></li> <li><code>mkdocs.yml</code></li> </ul> </li> <li>Added tests:</li> <li><code>tests/test_tensorrt_builder_smoke.py</code></li> <li>coverage:<ul> <li>no-TRT failure path (deterministic error)</li> <li>FP16 tiny-network smoke build (skip with capability guards)</li> <li>INT8 tiny-network + calibration smoke build (skip with capability guards)</li> </ul> </li> </ul>"},{"location":"CONTEXT/#engine-build-commands","title":"Engine Build Commands","text":"<ul> <li>FP16 direct-network smoke:</li> <li><code>python -m pytest -q tests/test_tensorrt_builder_smoke.py -k fp16</code></li> <li>INT8 direct-network smoke:</li> <li><code>python -m pytest -q tests/test_tensorrt_builder_smoke.py -k int8</code></li> <li>Python build usage (programmatic):</li> <li>use <code>TensorRTEngineBuilder.build_from_network(...)</code> or <code>build_from_onnx(...)</code></li> <li>recommended artifact paths:<ul> <li>engines: <code>artifacts/trt/*.engine</code></li> <li>calibration caches: <code>artifacts/trt/*.cache</code></li> </ul> </li> </ul>"},{"location":"CONTEXT/#artifact-locations","title":"Artifact Locations","text":"<ul> <li>Engine outputs:</li> <li>path passed to <code>engine_path</code> (recommended: <code>artifacts/trt/</code>)</li> <li>INT8 calibration cache:</li> <li>path passed via <code>TensorRTEngineBuildConfig.calibration_cache_path</code></li> <li>recommended: <code>artifacts/trt/int8.cache</code></li> </ul>"},{"location":"CONTEXT/#validation-status_7","title":"Validation Status","text":"<ul> <li><code>ruff</code> on changed runtime TRT builder/calibrator files: passed.</li> <li><code>mypy</code> on changed runtime TRT builder/calibrator files: passed.</li> <li><code>pytest</code> targeted run:</li> <li>passed for CPU-safe tests</li> <li>CUDA/TRT-dependent tests skipped in this environment (expected).</li> <li><code>mkdocs build --strict</code> could not run in this environment because <code>mkdocs</code> is not installed.</li> </ul>"},{"location":"CONTEXT/#remaining-work_6","title":"Remaining Work","text":"<ul> <li>Add CLI wrapper command for Python TRT builder workflow (currently programmatic API only).</li> <li>Add ONNX custom-op placeholder integration example with concrete node/plugin mapping.</li> <li>Add GPU CI lane executing TRT builder smoke tests with plugin library loading.</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-unified-gpu-benchmark-suite-torchtritontrt","title":"Latest Update (2026-02-08): Unified GPU Benchmark Suite (Torch/Triton/TRT)","text":"<ul> <li>Read and aligned implementation to:</li> <li><code>docs/CONTEXT.md</code></li> <li><code>docs/PRD.md</code></li> <li><code>docs/ENGINEERING_SPEC.md</code></li> <li><code>docs/PERF.md</code></li> <li><code>docs/runtime/TRITON.md</code></li> <li>Added unified CUDA benchmark runner:</li> <li><code>apex_x/bench/gpu_bench.py</code></li> <li>Runner coverage implemented:</li> <li>Tile ops microbench:<ul> <li><code>TilePack</code> (torch reference vs Triton dispatch)</li> <li><code>TileUnpack</code> (torch reference vs Triton dispatch)</li> <li><code>FusionGate</code> (torch reference vs Triton dispatch)</li> </ul> </li> <li>TileSSM microbench:<ul> <li>torch reference vs Triton dispatch</li> <li>TensorRT plugin path benchmark for <code>TileSSMScan</code> (when TensorRT Python + plugin library are available)</li> </ul> </li> <li>End-to-end FF inference benchmark:<ul> <li>torch eager path</li> <li>torch+Triton fast-path (Triton requested for inference scan with fallback guard)</li> <li>optional TensorRT engine benchmark (<code>--trt-engine-path</code>)</li> </ul> </li> <li>Report outputs implemented:</li> <li>JSON: <code>artifacts/perf_gpu.json</code> (default)</li> <li>Markdown: <code>artifacts/perf_gpu.md</code> (default)</li> <li>Metrics include <code>p50/p95</code>, throughput (<code>tiles/tokens/elements/frames per second</code>), and CUDA peak memory (<code>max_memory_allocated</code>).</li> <li>Added GPU perf documentation:</li> <li><code>docs/PERF_GPU.md</code></li> <li>Updated docs navigation:</li> <li><code>docs/index.md</code></li> <li><code>mkdocs.yml</code></li> </ul>"},{"location":"CONTEXT/#run-commands_7","title":"Run Commands","text":"<ul> <li>Default fixed-profile GPU benchmark:</li> <li><code>python -m apex_x.bench.gpu_bench --output-json artifacts/perf_gpu.json --output-md artifacts/perf_gpu.md</code></li> <li>Faster smoke-like run:</li> <li><code>python -m apex_x.bench.gpu_bench --warmup 3 --iters 10 --output-json artifacts/perf_gpu_smoke.json --output-md artifacts/perf_gpu_smoke.md</code></li> <li>Enable TensorRT plugin TileSSM bench:</li> <li><code>export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so</code></li> <li><code>python -m apex_x.bench.gpu_bench</code></li> <li>Optional TensorRT engine benchmark:</li> <li><code>python -m apex_x.bench.gpu_bench --trt-engine-path artifacts/trt/apex_x.engine --trt-input-shape input=1x3x128x128</code></li> </ul>"},{"location":"CONTEXT/#artifact-locations_1","title":"Artifact Locations","text":"<ul> <li>JSON report:</li> <li><code>artifacts/perf_gpu.json</code> (or <code>--output-json</code>)</li> <li>Markdown summary:</li> <li><code>artifacts/perf_gpu.md</code> (or <code>--output-md</code>)</li> </ul>"},{"location":"CONTEXT/#validation-status_8","title":"Validation Status","text":"<ul> <li>Local run in this environment:</li> <li>suite executes with capability guards</li> <li>when CUDA is unavailable, output is <code>status=skipped</code> with explicit reason.</li> <li>Static checks and mypy/test status are tracked per current session commands.</li> </ul>"},{"location":"CONTEXT/#remaining-work_7","title":"Remaining Work","text":"<ul> <li>Add GPU CI lane to run <code>apex_x/bench/gpu_bench.py</code> on fixed CUDA runners and compare against a committed GPU baseline.</li> <li>Add optional CSV/time-series exporter for long-run perf trend tracking.</li> <li>Add explicit TRT engine profile presets for common deployed input signatures.</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-gpu-perf-regression-ci-workflow-baseline-compare","title":"Latest Update (2026-02-08): GPU Perf Regression CI Workflow + Baseline Compare","text":"<ul> <li>Read and aligned implementation to:</li> <li><code>docs/CONTEXT.md</code></li> <li><code>docs/PRD.md</code></li> <li><code>docs/ENGINEERING_SPEC.md</code></li> <li>Added GPU perf regression workflow:</li> <li><code>.github/workflows/perf_gpu.yml</code></li> <li>Workflow behavior:</li> <li>skipped by default on manual dispatch (<code>run_mode=skip</code>)</li> <li>executes only when:<ul> <li>manual dispatch sets <code>run_mode=self-hosted-gpu</code>, or</li> <li>nightly schedule runs with repository variable <code>APEXX_ENABLE_GPU_NIGHTLY=true</code></li> </ul> </li> <li>runs on <code>runs-on: [self-hosted, linux, x64, gpu]</code></li> <li>keeps CPU perf regression intact in <code>.github/workflows/ci.yml</code> (<code>perf-regression</code> job unchanged)</li> <li>Added GPU regression compare script:</li> <li><code>scripts/perf_regression_gpu.py</code></li> <li>runs <code>apex_x/bench/gpu_bench.py</code> via Python module API and compares against stored baseline thresholds</li> <li>Added stored GPU baseline JSON:</li> <li><code>scripts/perf_baseline_gpu.json</code></li> <li>Added documentation:</li> <li><code>docs/CI_GPU.md</code> (self-hosted setup, nightly switch, baseline maintenance, security notes)</li> <li>updated docs nav in <code>docs/index.md</code> and <code>mkdocs.yml</code></li> </ul>"},{"location":"CONTEXT/#run-commands_8","title":"Run Commands","text":"<ul> <li>Local GPU regression compare:</li> <li><code>python scripts/perf_regression_gpu.py --compare --baseline scripts/perf_baseline_gpu.json --output artifacts/perf_gpu_current_local.json --summary artifacts/perf_gpu_compare_local.json</code></li> <li>Regenerate baseline template on target GPU runner:</li> <li><code>python scripts/perf_regression_gpu.py --emit-baseline-template --baseline scripts/perf_baseline_gpu.json</code></li> <li>Manual workflow run:</li> <li>dispatch <code>GPU Perf Regression</code> with <code>run_mode=self-hosted-gpu</code></li> </ul>"},{"location":"CONTEXT/#artifact-locations_2","title":"Artifact Locations","text":"<ul> <li>GPU current run report:</li> <li><code>artifacts/perf_gpu_current_ci.json</code></li> <li>GPU compare summary:</li> <li><code>artifacts/perf_gpu_compare_ci.json</code></li> <li>Uploaded artifact name in workflow:</li> <li><code>perf-gpu-regression-artifacts</code></li> </ul>"},{"location":"CONTEXT/#validation-status_9","title":"Validation Status","text":"<ul> <li>Local validation in this environment:</li> <li><code>python scripts/perf_regression_gpu.py --help</code>: passed</li> <li><code>python scripts/perf_regression_gpu.py --compare ...</code>: executed and failed as expected (<code>status=skipped</code> on CPU-only host)</li> <li><code>ruff check scripts/perf_regression_gpu.py</code>: passed</li> <li><code>python -m py_compile scripts/perf_regression_gpu.py</code>: passed</li> <li><code>ruff check .github/workflows/perf_gpu.yml</code>: not applicable (YAML)</li> </ul>"},{"location":"CONTEXT/#remaining-work_8","title":"Remaining Work","text":"<ul> <li>Calibrate <code>scripts/perf_baseline_gpu.json</code> on the real self-hosted GPU runner and tighten tolerances.</li> <li>Add optional GPU matrix (per GPU class) with separate baselines when heterogeneous runners are used.</li> <li>Add GPU docs build validation on the same self-hosted environment if needed.</li> </ul>"},{"location":"CONTEXT/#latest-update-2026-02-08-go-runtime-hardening-loaders-batching-metrics-logging-integration","title":"Latest Update (2026-02-08): Go Runtime Hardening (Loaders, Batching Metrics, Logging, Integration)","text":"<ul> <li>Read and aligned implementation to:</li> <li><code>docs/CONTEXT.md</code></li> <li><code>docs/PRD.md</code></li> <li><code>docs/ENGINEERING_SPEC.md</code></li> <li><code>runtime/go/README.md</code></li> <li>Hardened engine loader paths in <code>runtime/go/internal/service/</code>:</li> <li>ONNX CPU baseline loader:<ul> <li><code>adapter_ort.go</code></li> <li><code>NewORTAdapter(...)</code> now validates and loads model path from flag or <code>APEXX_ORT_MODEL_PATH</code></li> <li>rejects missing/empty model files</li> </ul> </li> <li>TensorRT loader via CGO:<ul> <li><code>adapter_tensorrt_cgo.go</code></li> <li>implements C-side engine file loader/free wrapper</li> <li>validates engine file path from flag or <code>APEXX_TRT_ENGINE_PATH</code></li> <li>retains inference execution as baseline/mock response path while loader is real</li> </ul> </li> <li>Implemented dynamic batching observability:</li> <li><code>batcher.go</code><ul> <li>tracks enqueue timestamps per request</li> <li>computes per-batch average queue wait and inference time</li> <li>records batch errors and structured batch logs</li> </ul> </li> <li><code>metrics.go</code><ul> <li>added metrics:</li> <li>batch size avg/max</li> <li>queue latency avg/max</li> <li>inference latency avg/max</li> <li>batch errors</li> </ul> </li> <li>Added structured logging + optional telemetry hooks:</li> <li><code>http.go</code>, <code>batcher.go</code>, <code>cmd/apexx-runtime/main.go</code></li> <li>configurable structured logger (<code>json|text</code>, level)</li> <li>optional hook interface:<ul> <li><code>telemetry.go</code> (<code>TelemetryHooks</code>, <code>NopTelemetryHooks</code>)</li> </ul> </li> <li>request and batch lifecycle now expose hook call points for OTel integration</li> <li>Added integration and loader tests:</li> <li><code>runtime/go/internal/service/integration_test.go</code><ul> <li>starts HTTP server</li> <li>validates <code>/health</code></li> <li>validates structured <code>/predict</code> response</li> <li>load simulation verifies batching groups requests under pressure</li> <li>validates expanded <code>/metrics</code> keys</li> </ul> </li> <li><code>runtime/go/internal/service/adapter_ort_test.go</code><ul> <li>ONNX loader success/failure/env override coverage</li> </ul> </li> <li>Updated runtime docs:</li> <li>new: <code>docs/runtime/GO_SERVICE.md</code></li> <li>updated: <code>runtime/go/README.md</code></li> <li>docs nav updates:<ul> <li><code>docs/index.md</code></li> <li><code>mkdocs.yml</code></li> </ul> </li> </ul>"},{"location":"CONTEXT/#run-commands_9","title":"Run Commands","text":"<ul> <li>Go unit + integration tests:</li> <li><code>cd runtime/go &amp;&amp; go test ./...</code></li> <li>Go tests with TensorRT build tags and CGO:</li> <li><code>cd runtime/go &amp;&amp; CGO_ENABLED=1 go test -tags tensorrt ./...</code></li> <li>Run CPU service mode:</li> <li><code>cd runtime/go</code></li> <li><code>mkdir -p models</code></li> <li>place a non-empty ONNX model at <code>models/apex-x.onnx</code></li> <li><code>go run ./cmd/apexx-runtime -adapter onnxruntime -model-path models/apex-x.onnx</code></li> <li>Run TensorRT service mode (loader path):</li> <li><code>cd runtime/go</code></li> <li><code>CGO_ENABLED=1 go run -tags tensorrt ./cmd/apexx-runtime -adapter tensorrt -engine-path models/apex-x.plan</code></li> </ul>"},{"location":"CONTEXT/#validation-status_10","title":"Validation Status","text":"<ul> <li><code>go test ./...</code>: passed</li> <li><code>CGO_ENABLED=1 go test -tags tensorrt ./...</code>: passed</li> <li>YAML parse check for docs nav (<code>mkdocs.yml</code>): passed</li> </ul>"},{"location":"CONTEXT/#remaining-work_9","title":"Remaining Work","text":"<ul> <li>Replace TensorRT adapter mock inference path with actual engine execution and tensor bindings.</li> <li>Add optional native OpenTelemetry implementation module (tracer/meter exporters) wired to current telemetry hooks.</li> <li>Add TLS/auth/rate-limit hardening around external-facing runtime service deployments.</li> </ul>"},{"location":"CONTEXT/#update-protocol-every-significant-change","title":"Update Protocol (Every Significant Change)","text":"<ul> <li>Update this file with:</li> <li>what changed</li> <li>why it changed</li> <li>what to do next</li> <li>If architecture changed, also update <code>docs/DECISIONS.md</code></li> <li>If requirements changed, update PRD/spec first, then code</li> </ul>"},{"location":"DECISIONS/","title":"Architectural Decisions","text":""},{"location":"DECISIONS/#adr-0001-authoritative-documentation-split","title":"ADR-0001: Authoritative Documentation Split","text":"<ul> <li>Date: 2026-02-07</li> <li>Decision: Keep requirements in <code>docs/PRD.md</code> and implementation contract in <code>docs/ENGINEERING_SPEC.md</code>.</li> <li>Rationale: Separates product scope from engineering detail while preserving traceability.</li> <li>Consequence: Any architecture change must update both files.</li> </ul>"},{"location":"DECISIONS/#adr-0002-cpu-only-reference-baseline-first","title":"ADR-0002: CPU-Only Reference Baseline First","text":"<ul> <li>Date: 2026-02-07</li> <li>Decision: Start with a pure Python + NumPy CPU baseline before runtime kernels.</li> <li>Rationale: Fast iteration, deterministic debugging, and testable contracts independent of GPU/runtime stack.</li> <li>Consequence: Runtime parity work follows once behavior is stabilized.</li> </ul>"},{"location":"DECISIONS/#adr-0003-deterministic-greedy-budgeting-for-inference","title":"ADR-0003: Deterministic Greedy Budgeting for Inference","text":"<ul> <li>Date: 2026-02-07</li> <li>Decision: Use utility-per-cost greedy selection with fixed <code>Kmax</code> buffers.</li> <li>Rationale: Deterministic, simple, and export/runtime friendly.</li> <li>Consequence: Potential global-optimality gap vs exact knapsack is accepted for speed and determinism.</li> </ul>"},{"location":"DECISIONS/#adr-0004-ordering-modes-required-in-contract","title":"ADR-0004: Ordering Modes Required in Contract","text":"<ul> <li>Date: 2026-02-07</li> <li>Decision: Support both Hilbert and multi-direction ordering modes.</li> <li>Rationale: Preserves geometry for sequence mixing and supports runtime experimentation.</li> <li>Consequence: Ordering determinism tests are mandatory.</li> </ul>"},{"location":"DECISIONS/#adr-0005-naming-conventions-baseline","title":"ADR-0005: Naming Conventions Baseline","text":"<ul> <li>Date: 2026-02-07</li> <li>Decision:</li> <li>Python modules/files use <code>snake_case</code>.</li> <li>Classes use <code>PascalCase</code>.</li> <li>Protocol interfaces use <code>*Protocol</code> suffix (for example <code>RouterProtocol</code>).</li> <li>Config sections use stable top-level keys: <code>model</code>, <code>routing</code>, <code>train</code>, <code>data</code>, <code>runtime</code>.</li> <li>Rationale: Consistent naming lowers integration friction and improves discoverability across modules.</li> <li>Consequence: New public interfaces and new files must follow these naming rules.</li> </ul>"},{"location":"DECISIONS/#adr-0006-tensor-shape-conventions","title":"ADR-0006: Tensor Shape Conventions","text":"<ul> <li>Date: 2026-02-07</li> <li>Decision:</li> <li>Image/features default to channel-first <code>NCHW</code> (<code>[B,C,H,W]</code>).</li> <li>Detection boxes use <code>[B,N,4]</code> with <code>cx,cy,w,h</code>.</li> <li>Packed tile tensors use <code>[B,K,C,t,t]</code>.</li> <li>Tile index tensors use <code>[B,K]</code> and rely on fixed <code>Kmax</code> contracts.</li> <li>Rationale: Stable tensor contracts are required for deterministic runtime behavior and export parity.</li> <li>Consequence: Any shape-contract change must update <code>docs/PRD.md</code>, <code>docs/ENGINEERING_SPEC.md</code>, and tests.</li> </ul>"},{"location":"DECISIONS/#adr-0007-determinism-rules","title":"ADR-0007: Determinism Rules","text":"<ul> <li>Date: 2026-02-07</li> <li>Decision:</li> <li>Inference tile selection uses deterministic greedy utility-per-cost with fixed <code>Kmax</code>.</li> <li>Tile ordering remains deterministic (Hilbert or multi-direction scan).</li> <li>Reproducibility utilities (<code>seed_all</code>, deterministic toggles) are part of baseline workflow.</li> <li>Export/runtime path must avoid Python-side inference control flow.</li> <li>Rationale: Determinism is a hard product constraint for regression tracking and deployment confidence.</li> <li>Consequence: Any nondeterministic path requires explicit rationale and targeted determinism tests.</li> </ul>"},{"location":"DECISIONS/#adr-0008-runtime-selection-vs-execution-traceability","title":"ADR-0008: Runtime Selection vs Execution Traceability","text":"<ul> <li>Date: 2026-02-10</li> <li>Decision:</li> <li>Runtime metadata must distinguish:<ul> <li>requested backend</li> <li>selected backend</li> <li>actual execution backend</li> <li>selection and execution fallback reasons</li> <li>latency breakdown (<code>total</code>, <code>backend_execute</code>, <code>backend_preflight</code>)</li> </ul> </li> <li>TensorRT CLI inference must execute real serialized engines when runtime artifacts are provided, instead of placeholder stubs.</li> <li>Rationale: Backend fallback chains can otherwise hide real execution behavior and invalidate performance/correctness evidence.</li> <li>Consequence:</li> <li>Runner/CLI reports are required to expose full backend selection and execution trace fields.</li> <li>TensorRT runtime path must enforce explicit engine artifact contract and deterministic strict/permissive behavior.</li> <li>Go runtime service <code>/predict</code> payloads must expose the same runtime metadata key schema.</li> <li>Go runtime service must map queue saturation to HTTP <code>429</code> and request timeout to HTTP <code>504</code>.</li> <li>Go runtime service should support optional shadow-canary parity telemetry for safe rollout validation.</li> </ul>"},{"location":"DECISIONS/#adr-0009-eval-dataset-optional-target-contract","title":"ADR-0009: Eval Dataset Optional Target Contract","text":"<ul> <li>Date: 2026-02-10</li> <li>Decision:</li> <li><code>eval --dataset-npz</code> supports optional scalar target keys:<ul> <li><code>det_score_target</code> (compat alias <code>det_scores_target</code>)</li> <li><code>selected_tiles_target</code> (compat alias <code>selected_tiles_targets</code>)</li> </ul> </li> <li>When <code>det_score_target</code> exists, <code>model_eval</code> report includes deterministic regression metrics     (<code>mae</code>, <code>rmse</code>, <code>bias</code>, <code>r2</code>, <code>pearson_corr</code>) in addition to aggregate model outputs.</li> <li>When <code>selected_tiles_target</code> exists, <code>model_eval</code> report includes     (<code>mae</code>, <code>rmse</code>, <code>bias</code>, <code>exact_match_rate</code>).</li> <li>Rationale: This adds lightweight, repeatable quality tracking on environments   where full box/mask ground-truth evaluation is not yet wired end-to-end.</li> <li>Consequence:</li> <li>Dataset shape/length validation is strict and fails early on mismatch.</li> <li>Future full GT parity work can extend this contract without breaking current reports.</li> </ul>"},{"location":"DECISIONS/#adr-0010-go-runtime-bridge-backends-and-error-taxonomy","title":"ADR-0010: Go Runtime Bridge Backends and Error Taxonomy","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>Go runtime adapters may execute real ORT/TRT inference through an external bridge command:<ul> <li><code>APEXX_ORT_BRIDGE_CMD</code></li> <li><code>APEXX_TRT_BRIDGE_CMD</code></li> </ul> </li> <li>ORT/TRT adapters must fail closed when bridge/native execution is unavailable     (no synthetic score fallback in production path).</li> <li>Bridge contract is JSON stdin/stdout and implemented by <code>apex_x/runtime/service_bridge.py</code>.</li> <li>Backend errors are classified and mapped explicitly:<ul> <li>backend unavailable -&gt; HTTP <code>503</code></li> <li>backend inference/protocol failures -&gt; HTTP <code>502</code></li> </ul> </li> <li>Canary mode payload capture must be policy-driven and optional:<ul> <li><code>off|mismatch|error|all</code></li> <li>JSONL sink with max-size guard</li> </ul> </li> <li>Rationale: Native Go bindings are environment-dependent; bridge mode enables real backend execution   while preserving deterministic HTTP failure semantics.</li> <li>Consequence:</li> <li>Service behavior remains SLA-safe under backend faults with stable status-code policy.</li> <li>Canary overhead and timeout/overflow rates are enforced by CI gate test thresholds.</li> <li>Native CGO TensorRT execution is still tracked separately until full implementation and GPU validation.</li> </ul>"},{"location":"DECISIONS/#adr-0011-budget-aware-temporal-hysteresis-metrics","title":"ADR-0011: Budget-Aware Temporal Hysteresis Metrics","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>Temporal hysteresis rollout supports optional per-frame active cap (<code>max_active</code>) to preserve     budget constraints during state carryover.</li> <li>Clipping priority is deterministic:<ol> <li>keep previously-active tiles</li> <li>higher utility first</li> <li>lower tile id tie-break</li> </ol> </li> <li>Temporal quality gates expose explicit metrics:<ul> <li><code>tile_flip_rate</code></li> <li><code>temporal_consistency</code></li> <li><code>mean_active_ratio</code></li> </ul> </li> <li>Rationale: Hysteresis reduces flicker but can temporarily retain extra tiles; budget-aware clipping   keeps anti-flicker behavior without violating frame-level active limits.</li> <li>Consequence:</li> <li>Routing APIs now include budget-aware hysteresis update and sequence stability summaries.</li> <li>Sequence-level tests are required to prove reduced flip churn under budget caps.</li> </ul>"},{"location":"DECISIONS/#adr-0012-recursive-quadtree-split-policy-for-l2","title":"ADR-0012: Recursive Quadtree Split Policy for L2","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>Inference budgeting supports deterministic recursive split selection to depth 2:<ul> <li><code>L0</code> selection under <code>B1</code></li> <li><code>L0 -&gt; L1</code> split parent selection under <code>B2</code></li> <li><code>L1 -&gt; L2</code> split parent selection under <code>B3</code></li> </ul> </li> <li>Parent ordering at each split stage uses:<ol> <li>split score (<code>S/O_split</code>) descending</li> <li>tile id ascending tie-break</li> </ol> </li> <li>Child expansion remains capacity-constrained by <code>Kmax_L1</code> and <code>Kmax_L2</code>.</li> <li>Rationale: Quality-sensitive regions often require second-level refinement; deterministic recursion   keeps behavior reproducible while honoring explicit multi-level budget constraints.</li> <li>Consequence:</li> <li>Routing layer now exposes a three-stage deterministic selection API.</li> <li>Tests must cover <code>B1/B2/B3</code> constraints, tie-break determinism, and <code>Kmax</code> limits.</li> </ul>"},{"location":"DECISIONS/#adr-0013-adaptive-dual-budget-controller-stabilizers","title":"ADR-0013: Adaptive Dual-Budget Controller Stabilizers","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>Dual budget update supports optional adaptive controls:<ul> <li>step decay (<code>mu_lr / (1 + decay * step)</code>)</li> <li>EMA-based scale modulation with bounded range (<code>lr_min_scale..lr_max_scale</code>)</li> <li>normalized-error deadband (<code>|error/B| &lt;= deadband_ratio</code>)</li> <li>delta clipping (<code>[-delta_clip, +delta_clip]</code>)</li> </ul> </li> <li>Dual variable remains projected into fixed bounds <code>[mu_min, mu_max]</code>.</li> <li>Rationale: Constant-rate dual ascent can oscillate near target budgets and overreact to noisy cost traces.   Adaptive scaling plus deadband/clipping improves convergence stability without changing base objective.</li> <li>Consequence:</li> <li>Training config now exposes explicit dual schedule/stabilizer parameters.</li> <li>Stage-3 trainer metrics include dual controller dynamics (<code>effective_lr</code>, <code>error_ema</code>, update count).</li> <li>Convergence tests must cover over-budget, under-budget, deadband, and clip-capped behavior.</li> </ul>"},{"location":"DECISIONS/#adr-0014-pcgrad-shared-trunk-scope-with-conflict-rate-telemetry","title":"ADR-0014: PCGrad++ Shared-Trunk Scope with Conflict-Rate Telemetry","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>PCGrad++ projection is applied only to shared trunk parameters.</li> <li>Head-specific parameters always use standard total-loss gradients (no projection).</li> <li>Diagnostics must include conflict metrics before and after projection:<ul> <li><code>conflicting_pairs</code></li> <li><code>conflicting_pairs_after</code></li> <li><code>total_pairs</code></li> <li><code>conflict_rate_before</code></li> <li><code>conflict_rate_after</code></li> </ul> </li> <li>Rationale: Restricting projection to shared trunk prevents unintended task-head updates, while   before/after metrics quantify whether projection actually reduces gradient conflict pressure.</li> <li>Consequence:</li> <li>Trainer reports now include PCGrad diagnostics payloads suitable for CI/report tracking.</li> <li>Tests must prove head gradients remain unchanged and conflict-rate reduction is non-regressive.</li> </ul>"},{"location":"DECISIONS/#adr-0015-fp8-requested-vs-effective-telemetry-contract","title":"ADR-0015: FP8 Requested-vs-Effective Telemetry Contract","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>FP8-capable flows must expose explicit requested-vs-effective precision telemetry:<ul> <li><code>requested_dtype</code></li> <li><code>effective_dtype</code></li> <li><code>fp8_requested</code></li> <li><code>fp8_enabled</code></li> <li><code>fp8_fallback_reason</code></li> </ul> </li> <li>FP8 fallback reasons must use canonical runtime reason-codes.</li> <li>GPU benchmark and regression wrappers accept explicit FP8 request mode (<code>--dtype fp8</code>).</li> <li>Rationale: Operational readiness requires proving whether FP8 was actually enabled or silently   downgraded; requested-vs-effective telemetry prevents false confidence in perf claims.</li> <li>Consequence:</li> <li>FP8 rollout evidence is now testable on non-CUDA hosts (fallback telemetry) and measurable on     supported GPUs via explicit FP8 benchmark artifacts.</li> </ul>"},{"location":"DECISIONS/#adr-0016-oracle-sampling-triad-and-delta-label-diagnostics","title":"ADR-0016: Oracle Sampling Triad and Delta-Label Diagnostics","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>Oracle tile subset sampling combines three components:<ul> <li>random subset</li> <li>uncertainty-weighted subset</li> <li>long-tail subset (deterministic top scores from remaining tiles)</li> </ul> </li> <li>Oracle delta labels are clipped by configured <code>clamp_abs</code>, and trainer reports:<ul> <li>sample composition counts</li> <li>delta distribution stats</li> <li>clipping ratio</li> </ul> </li> <li>Rationale: Mixing random + uncertain + long-tail examples improves utility-head coverage while   clipping/diagnostics prevents silent instability from outlier labels.</li> <li>Consequence:</li> <li><code>sample_oracle_set(...)</code> now supports long-tail selection inputs.</li> <li>Stage-2 trainer metrics/logs expose explicit oracle distribution + clipping diagnostics.</li> </ul>"},{"location":"DECISIONS/#adr-0017-tensorrt-int8-sensitive-layer-fp16-enforcement","title":"ADR-0017: TensorRT INT8 Sensitive-Layer FP16 Enforcement","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>In TensorRT INT8 builds, sensitive layers (matched by configurable keywords, default     <code>router</code> and <code>kan</code>) are forced to FP16 precision constraints.</li> <li>Precision constraint enforcement is strict by default:<ul> <li>build fails if matched layer cannot be constrained via TensorRT APIs.</li> </ul> </li> <li>Build results expose per-layer precision evidence (<code>layer_precision_status</code>).</li> <li>Rationale: Routing/decision layers are numerically sensitive; silently quantizing them to INT8   can destabilize gating behavior and degrade quality.</li> <li>Consequence:</li> <li><code>TensorRTEngineBuildConfig</code> includes strict precision-constraint policy.</li> <li>Tests validate keyword-based enforcement, strict failure behavior, and emitted precision report.</li> </ul>"},{"location":"DECISIONS/#adr-0018-tensorrt-plugin-creator-contract-validation","title":"ADR-0018: TensorRT Plugin Creator Contract Validation","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>TensorRT builder validates plugin creator contracts before engine build:<ul> <li>creator presence in registry</li> <li>version match</li> <li>namespace match</li> <li>plugin field-signature metadata coverage</li> </ul> </li> <li>Strict plugin validation remains default and fails build on required-plugin mismatches.</li> <li>Optional plugin mismatches are reported as warnings when strict mode is disabled.</li> <li>Rationale: Plugin name-only checks are insufficient for production safety; ABI/contract drifts can   silently break builds or runtime behavior.</li> <li>Consequence:</li> <li><code>TensorRTEngineBuildConfig</code> supports explicit <code>PluginContract</code> overrides.</li> <li><code>PluginStatus</code> now carries detailed mismatch diagnostics for actionable failures.</li> <li>Tests cover version/namespace/field-signature mismatch behaviors without requiring CUDA runtime.</li> </ul>"},{"location":"DECISIONS/#adr-0019-tensorrt-int8-calibration-cache-governance","title":"ADR-0019: TensorRT INT8 Calibration Cache Governance","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>TensorRT INT8 calibration cache reuse is governed by deterministic cache keys that include:<ul> <li>model/export identity hash</li> <li>plugin contract metadata (version + namespace)</li> <li>precision profile</li> <li>calibration dataset version</li> </ul> </li> <li>Calibration dataset version supports:<ul> <li>explicit configuration (<code>calibration_dataset_version</code>)</li> <li>deterministic auto-digest from calibration batches</li> </ul> </li> <li>Calibrator cache files use structured metadata when key governance is active;     stale/mismatched keys are invalidated automatically.</li> <li>Legacy raw cache blobs are accepted only when key governance is disabled.</li> <li>Rationale: INT8 accuracy/perf stability requires preventing stale calibration reuse after model,   plugin, precision-profile, or dataset changes.</li> <li>Consequence:</li> <li><code>EngineBuildResult</code> now exposes <code>calibration_cache_key</code> and <code>calibration_dataset_version</code>.</li> <li>Non-CUDA tests validate deterministic key behavior and stale-cache invalidation rules.</li> </ul>"},{"location":"DECISIONS/#adr-0020-tensorrt-parity-harness-matrix-sweep-contract","title":"ADR-0020: TensorRT Parity Harness Matrix + Sweep Contract","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>Parity validation for TensorRT readiness is executed as backend-pair matrix checks:<ul> <li>reference vs triton</li> <li>reference vs tensorrt</li> <li>triton vs tensorrt</li> </ul> </li> <li>Matrix checks are composed into profile-aware sweep runs covering shape and precision cases.</li> <li>CPU-safe parity harness tests are mandatory to keep matrix/sweep semantics stable even when     CUDA/TensorRT runtime is unavailable on CI host.</li> <li>Rationale: Production parity readiness needs a repeatable structure, not ad-hoc pair checks.   Matrix+sweep contract ensures consistent coverage expansion from CPU-safe CI to deployment GPUs.</li> <li>Consequence:</li> <li><code>apex_x/runtime/parity.py</code> now exposes matrix+sweep APIs and aggregate sweep reporting.</li> <li>CUDA-host execution remains required to close final parity evidence for Triton/TensorRT runtime.</li> </ul>"},{"location":"DECISIONS/#adr-0021-triton-tilessm-long-sequence-chunked-execution","title":"ADR-0021: Triton TileSSM Long-Sequence Chunked Execution","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>Triton TileSSM forward scan no longer hard-fails for long sequences beyond single-launch     specialization bounds.</li> <li>For <code>K</code> above the per-launch limit, runtime executes chunked launches and streams recurrent     state between chunks.</li> <li>Rationale: Large-scene workloads should remain on accelerated path where possible; hard   sequence caps force unnecessary fallback and create avoidable latency cliffs.</li> <li>Consequence:</li> <li><code>tilessm_scan_triton(...)</code> now supports long-sequence chunking semantics.</li> <li>CPU-safe tests validate chunk partitioning and state carry-over contract.</li> <li>Deployment CUDA benchmarks remain required for final perf evidence.</li> </ul>"},{"location":"DECISIONS/#adr-0022-triton-tileunpack-blend-dispatch-parity-contract","title":"ADR-0022: Triton TileUnpack Blend Dispatch Parity Contract","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li><code>tileunpack_dispatch(...)</code> no longer treats <code>overlap_mode=\\\"blend\\\"</code> as a forced     reference-only branch.</li> <li>Blend overlap semantics remain ordered and parity-equivalent to reference behavior.</li> <li>Rationale: Forced fallback branches in supported overlap modes prevent full accelerated runtime   readiness and obscure backend capability expectations.</li> <li>Consequence:</li> <li>Blend dispatch path now uses <code>tileunpack_triton(...)</code> entrypoint when accelerated path is     selected.</li> <li>CPU/GPU overlap tests were expanded to validate blend dispatch contract and parity behavior.</li> </ul>"},{"location":"DECISIONS/#adr-0023-ff-heavy-path-stage-1-fused-selector-gating","title":"ADR-0023: FF Heavy-Path Stage-1 Fused Selector Gating","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>FF heavy-path inference can use Stage-1 fused dispatch only when strict compatibility     predicates are satisfied:<ul> <li>eval mode</li> <li>identity refine block</li> <li>effectively constant FiLM parameters</li> <li>unique selected tile indices</li> </ul> </li> <li>non-compatible cases deterministically use decomposed <code>pack -&gt; FiLM -&gt; unpack</code> path.</li> <li>Rationale: Stage-1 fused kernel offers speed benefits but is not mathematically equivalent to   the full decomposed path for arbitrary FiLM/refine dynamics; gating preserves correctness.</li> <li>Consequence:</li> <li>runtime selector now chooses fused path only for compatible inference cases.</li> <li>unit tests cover activation, parity in compatible mode, and fallback behavior.</li> </ul>"},{"location":"DECISIONS/#adr-0024-triton-autotune-registry-and-benchmark-telemetry","title":"ADR-0024: Triton Autotune Registry and Benchmark Telemetry","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>Triton kernels with autotune candidates publish per-op/per-shape-bucket selected config     telemetry into a shared in-process registry.</li> <li>Registry entries are keyed by <code>op_name + shape_bucket</code> and track:<ul> <li>selected launch config metadata</li> <li>selection source (<code>triton_best_config</code>, <code>heuristic</code>, <code>registry_cache</code>)</li> <li>cache counters (<code>launches</code>, <code>cache_hits</code>, <code>cache_misses</code>)</li> </ul> </li> <li>GPU benchmark report includes registry telemetry in both JSON and Markdown outputs.</li> <li>Rationale: Performance readiness needs explicit visibility into which launch configs are used   per workload shape; without this, regressions can hide behind implicit runtime autotune behavior.</li> <li>Consequence:</li> <li>Triton tile/fusion/fused-stage1 kernels now emit autotune registry events.</li> <li><code>gpu_bench</code> now emits <code>triton_autotune.summary</code> and <code>triton_autotune.entries</code>.</li> <li>CPU-safe tests validate registry cache accounting and report formatting contract.</li> </ul>"},{"location":"DECISIONS/#adr-0025-unified-perf-regression-wrappers-including-tensorrt-shape-sweep","title":"ADR-0025: Unified Perf Regression Wrappers Including TensorRT Shape Sweep","text":"<ul> <li>Date: 2026-02-11</li> <li>Decision:</li> <li>CPU, GPU, and TensorRT perf regression wrappers use one shared pass/fail formula based on     baseline ratio + absolute tolerance.</li> <li>TensorRT shape-sweep regression is tracked with a dedicated wrapper:<ul> <li><code>scripts/perf_regression_trt.py</code></li> <li>baseline: <code>scripts/perf_baseline_trt.json</code></li> </ul> </li> <li>GPU CI/weekly workflows execute TRT compare + trend only when <code>TRT_ENGINE_PATH</code> is provided.</li> <li>Rationale: Performance governance must be consistent across runtime tiers; TRT coverage should not   rely on ad-hoc checks or manual inspection.</li> <li>Consequence:</li> <li>TRT shape-sweep now has baseline-template, compare, and trend artifact modes aligned with CPU/GPU.</li> <li>Deployment runner validation remains required for final threshold tuning and mandatory gate policy.</li> </ul>"},{"location":"ENGINEERING_SPEC/","title":"Apex-X v4 Engineering Specification","text":""},{"location":"ENGINEERING_SPEC/#1-authority-and-scope","title":"1. Authority and Scope","text":"<p>This document is the engineering contract for Apex-X v4 implementation. <code>docs/PRD.md</code> defines product requirements; this file defines implementation details, equations, interfaces, and test criteria.</p>"},{"location":"ENGINEERING_SPEC/#2-system-overview","title":"2. System Overview","text":"<p>Apex-X v4 is a dynamic compute graph with: - dense low-resolution PV stream - sparse high-resolution FF stream - tile utility router - budget controllers for training and inference - tile pack/ssm/unpack path for heavy compute</p>"},{"location":"ENGINEERING_SPEC/#3-tensor-and-shape-contracts","title":"3. Tensor and Shape Contracts","text":""},{"location":"ENGINEERING_SPEC/#31-inputs","title":"3.1 Inputs","text":"<ul> <li>Image: <code>x in R^{B x 3 x H x W}</code></li> <li>Optional temporal state:</li> <li>previous tile mask <code>z(t-1)</code></li> <li>previous utilities <code>U(t-1)</code></li> <li>previous SSM state <code>s(t-1)</code></li> </ul>"},{"location":"ENGINEERING_SPEC/#32-core-feature-maps","title":"3.2 Core Feature Maps","text":"<ul> <li>PV feature map (<code>stride 16 or 32</code>): <code>F_pv</code></li> <li>FF primary feature map (<code>stride 8</code>): <code>F_ff8</code></li> <li>FF refine feature map (<code>stride 4</code>, optional): <code>F_ff4</code></li> </ul>"},{"location":"ENGINEERING_SPEC/#33-tile-packunpack","title":"3.3 Tile Pack/Unpack","text":"<p>Pack: - Inputs:   - <code>F: [B,C,Hf,Wf]</code>   - <code>idx: [B,K]</code>   - fixed tile size <code>t</code> - Outputs:   - <code>P: [B,K,C,t,t]</code>   - <code>meta = {origins, ordered_idx, grid_shape, tile_size}</code></p> <p>Unpack: - Inputs:   - <code>F_base: [B,C,Hf,Wf]</code>   - <code>P_out: [B,K,C,t,t]</code>   - <code>meta</code>   - optional gate and priority masks - Output:   - <code>F_merged: [B,C,Hf,Wf]</code></p> <p>Overlap semantics: - default: higher nesting level overrides lower (<code>L2 &gt; L1 &gt; L0</code>) - optional weighted blend if explicitly configured</p>"},{"location":"ENGINEERING_SPEC/#4-router-specification","title":"4. Router Specification","text":""},{"location":"ENGINEERING_SPEC/#41-router-inputs-per-tile","title":"4.1 Router Inputs per Tile","text":"<p>For tile <code>i</code>, aggregate PV statistics over mapped PV region: - objectness <code>o_hat</code> - uncertainty <code>u_hat</code> - boundary proxy <code>b_hat</code> - variance/energy <code>v_hat</code> - optional motion <code>m_hat</code></p> <p>Feature vector: [ x_i \\in \\mathbb{R}^d,\\; d\\approx 12\\ldots24 ]</p>"},{"location":"ENGINEERING_SPEC/#42-router-outputs","title":"4.2 Router Outputs","text":"<ul> <li>utility <code>U_i</code></li> <li>split utility <code>S_i</code></li> <li>optional temporal keep score <code>T_i</code></li> </ul>"},{"location":"ENGINEERING_SPEC/#43-utility-oracle-deltaloss","title":"4.3 Utility Oracle (DeltaLoss)","text":"<p>For oracle subset <code>S</code>: [ \\Delta_i = L_{distill}(y^{(i=0)}, y_T) - L_{distill}(y^{(i=1)}, y_T) ] Train router utility head against <code>Delta_i</code> with <code>L1</code> or ranking loss. Clamp outliers: [ \\Delta_i \\leftarrow clamp(\\Delta_i, -\\tau, \\tau) ]</p> <p>Pseudo-code: <pre><code>S = random_subset(tiles, rate=0.1..0.2)\n    U uncertainty_weighted_subset\n    U long_tail_subset\nfor i in S:\n  loss_off = L_distill(run(tile_i=off), y_teacher)\n  loss_on  = L_distill(run(tile_i=on),  y_teacher)\n  Delta_i = clamp(loss_off - loss_on, -tau, tau)\nL_util = regression_or_ranking(U_i, Delta_i)\n</code></pre> Required training diagnostics per step/epoch: - sampling composition (<code>random</code>, <code>uncertainty</code>, <code>long_tail</code>) - delta label distribution (<code>mean</code>, <code>std</code>, <code>abs_p95</code>, <code>min</code>, <code>max</code>) - clipping diagnostics (<code>clipped_count</code>, <code>clipped_ratio</code>)</p>"},{"location":"ENGINEERING_SPEC/#5-continuous-budgeting-training","title":"5. Continuous Budgeting (Training)","text":""},{"location":"ENGINEERING_SPEC/#51-gating","title":"5.1 Gating","text":"<p>[ p_i = \\sigma(U_i),\\quad g_i = STE(p_i) ] Forward uses <code>g_i</code> as hard gate; backward uses straight-through gradient.</p>"},{"location":"ENGINEERING_SPEC/#52-expected-cost","title":"5.2 Expected Cost","text":"<p>[ \\mathbb{E}[C] = \\sum_i \\left(p_i C_h + (1-p_i) C_c\\right) ] where <code>C_h</code> and <code>C_c</code> are heavy and cheap path costs.</p>"},{"location":"ENGINEERING_SPEC/#53-dual-optimization","title":"5.3 Dual Optimization","text":"<p>Objective: [ L = L_{main} + \\mu(\\mathbb{E}[C]-B),\\quad \\mu\\ge0 ] Projected dual ascent with optional adaptive schedule: [ \\eta_{eff}(t)=\\frac{\\eta_\\mu}{1+\\lambda t}\\cdot \\text{clip}(1+|\\text{EMA}(e_t)|,\\;s_{min},\\;s_{max}),\\quad e_t=\\frac{\\mathbb{E}[C]-B}{B} ] [ \\Delta\\mu=\\eta_{eff}(t)\\cdot(\\mathbb{E}[C]-B),\\quad \\mu \\leftarrow \\text{clip}(\\mu+\\Delta\\mu,\\;[\\mu_{min},\\mu_{max}]) ] Optional stabilizers: - deadband: skip updates when <code>|e_t| &lt;= deadband_ratio</code> - delta clip: clip <code>Delta mu</code> into <code>[-delta_clip, delta_clip]</code></p> <p>Pseudo-code: <pre><code>U = Router(x)\np = sigmoid(U)\ng = STE(p)\ny = Forward(g)\nL_main = L_det + lambda_seg*L_seg + ...\nE = sum(p_i*C_h + (1-p_i)*C_c)\nL = L_main + mu*(E - B)\nopt_theta.step(grad(L, theta))\ne = (E - B) / B\nema_e = beta*ema_e + (1-beta)*e\neta_eff = (eta_mu / (1 + decay*step)) * clip(1 + abs(ema_e), lr_min, lr_max)\ndelta = eta_eff * (E - B)\nif abs(e) &lt;= deadband_ratio:\n  delta = 0\ndelta = clip(delta, -delta_clip, delta_clip)  # optional\nmu = clip(mu + delta, mu_min, mu_max)\n</code></pre></p>"},{"location":"ENGINEERING_SPEC/#6-deterministic-inference-budgeting","title":"6. Deterministic Inference Budgeting","text":""},{"location":"ENGINEERING_SPEC/#61-greedy-utility-per-cost","title":"6.1 Greedy Utility-per-Cost","text":"<p>Given <code>U_i</code>, <code>DeltaC_i</code>, budget <code>B</code>, and fixed <code>Kmax</code>: [ score_i = \\frac{U_i}{\\Delta C_i} ] Sort descending; accept while both constraints hold: - <code>spent + DeltaC_i &lt;= B</code> - <code>|active| &lt; Kmax</code></p>"},{"location":"ENGINEERING_SPEC/#62-kmax-buffer-contract","title":"6.2 Kmax Buffer Contract","text":"<ul> <li>runtime buffers are allocated to <code>Kmax</code></li> <li>actual selected tile count is <code>K &lt;= Kmax</code></li> <li>unused entries are ignored via valid-count metadata</li> </ul>"},{"location":"ENGINEERING_SPEC/#63-temporal-hysteresis-with-budget-aware-carryover","title":"6.3 Temporal Hysteresis with Budget-Aware Carryover","text":"<p>Temporal anti-flicker rule: [ z_i(t)=\\mathbf{1}[U_i(t)&gt;\\theta_{on}\\;\\lor\\;(U_i(t)&gt;\\theta_{off}\\land z_i(t-1)=1)] ] with <code>theta_on &gt; theta_off</code>.</p> <p>Budget-aware enforcement: - hysteresis state update is computed first - if active count exceeds per-frame limit (<code>max_active</code>), deterministic clipping is applied:   1. keep tiles that were already active in previous frame   2. then sort by utility descending   3. tie-break by tile id ascending</p> <p>This preserves temporal state reuse while preventing budget overflow in frame-level routing masks.</p> <p>Temporal stability metrics: - <code>tile_flip_rate</code>: [ \\frac{\\text{total state transitions}}{(T-1)\\cdot N_{tiles}} ] - <code>temporal_consistency = 1 - tile_flip_rate</code> - <code>mean_active_ratio</code>: average active fraction across all frames</p> <p>Pseudo-code: <pre><code>scores = [U_i / DeltaC_i for i in tiles]\norder = argsort(scores, descending=True)\nactive, spent = [], 0\nfor i in order:\n  if len(active) &gt;= Kmax: break\n  if spent + DeltaC_i &lt;= B:\n    active.append(i)\n    spent += DeltaC_i\nwrite_to_kmax_buffer(active)\n</code></pre></p>"},{"location":"ENGINEERING_SPEC/#7-quadtree-nesting-l0l1l2","title":"7. Quadtree Nesting L0/L1/L2","text":""},{"location":"ENGINEERING_SPEC/#71-tile-hierarchy","title":"7.1 Tile Hierarchy","text":"<ul> <li><code>L0</code>: coarse tiles (<code>t0</code>)</li> <li><code>L1</code>: <code>t1=t0/2</code></li> <li><code>L2</code>: <code>t2=t1/2</code> (optional)</li> </ul>"},{"location":"ENGINEERING_SPEC/#72-budget-split","title":"7.2 Budget Split","text":"<ul> <li><code>B1</code>: heavy compute for selected <code>L0</code></li> <li><code>B2</code>: split budget for finer tiles</li> <li>optional <code>B3</code> for <code>L2</code></li> </ul>"},{"location":"ENGINEERING_SPEC/#73-split-utility","title":"7.3 Split Utility","text":"<p>[ score_i^{split} = \\frac{S_i}{O_{split,i}} ] Select split candidates under <code>B2</code>.</p> <p>Recursive depth-2 contract: - stage <code>L0 -&gt; L1</code> uses <code>B2</code> - stage <code>L1 -&gt; L2</code> uses <code>B3</code> - both stages enforce deterministic parent ranking:   1. split score descending   2. tile id ascending tie-break - each accepted parent expands to exactly 4 children - expansion is capacity-constrained by <code>Kmax_L1</code> / <code>Kmax_L2</code></p> <p>Pseudo-code: <pre><code>L0 = select_tiles(U_L0, cost_L0, B1, Kmax_L0)\nsplit_candidates = []\nfor i in L0:\n  split_candidates.append((i, S_i / O_split_i))\nL1 = select_tiles(split_candidates, split_cost, B2, Kmax_L1)\nif nesting_depth &gt;= 2:\n  L2 = recursive_split(L1, B3, Kmax_L2)\n</code></pre></p>"},{"location":"ENGINEERING_SPEC/#8-ordering-and-sequence-geometry","title":"8. Ordering and Sequence Geometry","text":""},{"location":"ENGINEERING_SPEC/#81-required-modes","title":"8.1 Required Modes","text":"<ul> <li>Hilbert ordering for geometry-preserving locality</li> <li>Multi-direction scan aggregation:</li> <li><code>L-&gt;R</code></li> <li><code>R-&gt;L</code></li> <li><code>U-&gt;D</code></li> <li><code>D-&gt;U</code></li> </ul>"},{"location":"ENGINEERING_SPEC/#82-determinism-rule","title":"8.2 Determinism Rule","text":"<p>Ordering function must be pure and deterministic for the same input tile set and grid shape.</p>"},{"location":"ENGINEERING_SPEC/#9-tile-ssm-and-local-refine","title":"9. Tile-SSM and Local Refine","text":""},{"location":"ENGINEERING_SPEC/#91-tokenization","title":"9.1 Tokenization","text":"<p>Mode A (tile token): [ v_i = Pool(P_i),\\quad v_i\\in\\mathbb{R}^C ] Sequence <code>v_1..v_K</code> processed by Tile-SSM.</p> <p>Mode B (sub-patch token): - split each tile into <code>2x2</code> or <code>4x4</code> patches - use extended token sequence for fine detail</p>"},{"location":"ENGINEERING_SPEC/#92-mamba-like-placeholder-contract","title":"9.2 Mamba-like Placeholder Contract","text":"<p>Tile-SSM module must provide: - streaming scan over ordered token sequence - optional persistent recurrent state - bounded temporary memory</p>"},{"location":"ENGINEERING_SPEC/#93-fusion-gate","title":"9.3 Fusion Gate","text":"<p>Heavy output and base feature merge: [ F_{merged} = F_{base} + g_{fuse} \\odot (F_{heavy} - F_{base}) ] where <code>g_fuse in [0,1]</code> may be scalar, channel-wise, or spatial.</p>"},{"location":"ENGINEERING_SPEC/#10-losses-and-multi-task-gradient-handling","title":"10. Losses and Multi-task Gradient Handling","text":""},{"location":"ENGINEERING_SPEC/#101-task-losses","title":"10.1 Task Losses","text":"<ul> <li>DET: focal/quality focal + IoU-family box loss (+ optional DFL)</li> <li>INST-SEG: BCE + Dice + boundary loss</li> <li>Optional task heads define their own loss terms</li> </ul>"},{"location":"ENGINEERING_SPEC/#102-pcgrad-for-shared-trunk","title":"10.2 PCGrad++ for Shared Trunk","text":"<p>For gradient pair <code>(g_a, g_b)</code> on shared params, if <code>cos(g_a, g_b) &lt; 0</code>: [ g_a \\leftarrow g_a - \\frac{g_a\\cdot g_b}{||g_b||^2}g_b ] Do not project head-specific gradients. Diagnostics contract: - compute conflict pairs before projection over ordered task-group pairs - compute conflict pairs after projection over the projected gradient set - expose:   - <code>conflicting_pairs</code>   - <code>conflicting_pairs_after</code>   - <code>total_pairs</code>   - <code>conflict_rate_before</code>   - <code>conflict_rate_after</code> - trainer logs must include the above metrics when PCGrad++ is enabled.</p>"},{"location":"ENGINEERING_SPEC/#11-temporal-hysteresis-and-state-reuse","title":"11. Temporal Hysteresis and State Reuse","text":""},{"location":"ENGINEERING_SPEC/#111-state","title":"11.1 State","text":"<ul> <li><code>z(t-1)</code>: previous active tiles</li> <li><code>U(t-1)</code>: previous utility</li> <li><code>s(t-1)</code>: previous SSM state</li> </ul>"},{"location":"ENGINEERING_SPEC/#112-update-rule","title":"11.2 Update Rule","text":"<p>Hysteresis: [ z_i(t)=\\mathbf{1}[U_i(t)&gt;\\theta_{on} \\lor (U_i(t)&gt;\\theta_{off} \\land z_i(t-1)=1)] ] <code>theta_on &gt; theta_off</code> to prevent flicker.</p> <p>Stability loss: [ L_{stab}=\\sum_i |p_i(t)-p_i(t-1)| ] Optional spatial TV term may be added.</p>"},{"location":"ENGINEERING_SPEC/#12-runtime-plugin-specifications","title":"12. Runtime Plugin Specifications","text":"<p>Detailed runtime notes also live in <code>docs/runtime/PLUGIN_SPEC.md</code>.</p>"},{"location":"ENGINEERING_SPEC/#121-tilepack-plugin","title":"12.1 TilePack Plugin","text":"<p>Inputs: <code>F, idx</code> Outputs: <code>P, meta</code> Requirements: - FP16 and FP8 data support - deterministic ordering - bounded workspace</p>"},{"location":"ENGINEERING_SPEC/#122-tilessmscan-plugin","title":"12.2 TileSSMScan Plugin","text":"<p>Inputs: packed tokens/tiles + optional prior state Outputs: mixed outputs + next state Requirements: - streaming scan - multi-direction mode - no unbounded temporary allocations</p>"},{"location":"ENGINEERING_SPEC/#123-tileunpackfusion-plugin","title":"12.3 TileUnpackFusion Plugin","text":"<p>Inputs: <code>F_base, P_out, meta, optional gate</code> Outputs: merged feature map Requirements: - residual + gate semantics - overlap priority support</p>"},{"location":"ENGINEERING_SPEC/#124-optional-plugins","title":"12.4 Optional Plugins","text":"<ul> <li><code>MaskedConv</code></li> <li>fused DET decode + NMS</li> </ul>"},{"location":"ENGINEERING_SPEC/#125-build-time-plugin-contract-checks","title":"12.5 Build-Time Plugin Contract Checks","text":"<p>TensorRT builder must validate plugin creator contracts before engine build: - required creators are present in registry - creator version matches expected contract version - creator namespace matches expected contract namespace - creator field-signature metadata covers expected fields for each plugin</p> <p>Strict mode is default and must fail fast with actionable mismatch diagnostics.</p>"},{"location":"ENGINEERING_SPEC/#13-qat-and-precision-policy","title":"13. QAT and Precision Policy","text":""},{"location":"ENGINEERING_SPEC/#131-profiles","title":"13.1 Profiles","text":"<ul> <li>Quality: FP16-heavy path, larger tile budget</li> <li>Balanced: FP16/FP8 mixed</li> <li>Edge: INT8 with router in FP16</li> </ul>"},{"location":"ENGINEERING_SPEC/#132-qat-rules","title":"13.2 QAT Rules","text":"<ul> <li>keep router logits and normalization in higher precision during QAT</li> <li>calibrate heavy branches per-level</li> <li>validate parity vs FP16 baseline before enabling INT8 by default</li> </ul>"},{"location":"ENGINEERING_SPEC/#133-acceptance-threshold","title":"13.3 Acceptance Threshold","text":"<ul> <li>mAP/mIoU degradation must stay within agreed profile thresholds</li> </ul>"},{"location":"ENGINEERING_SPEC/#134-fp8-telemetry-contract","title":"13.4 FP8 Telemetry Contract","text":"<p>When FP8 is requested, runtime/benchmark reports must expose: - <code>requested_dtype</code> - <code>effective_dtype</code> - <code>fp8_requested</code> - <code>fp8_enabled</code> - <code>fp8_fallback_reason</code></p> <p><code>fp8_fallback_reason</code> must be a canonical reason-code from the runtime capability catalog.</p>"},{"location":"ENGINEERING_SPEC/#135-int8-sensitive-layer-precision-contract","title":"13.5 INT8 Sensitive-Layer Precision Contract","text":"<p>In TensorRT INT8 builds: - layers matching sensitive keywords (<code>router</code>, <code>kan</code> by default) must be constrained to FP16. - strict mode must fail build when a matched layer cannot be constrained via TensorRT precision APIs. - build results must expose per-layer precision constraint evidence:   - <code>layer_name</code>   - <code>matched_keyword</code>   - <code>precision_applied</code>   - <code>output_constraints_applied</code></p>"},{"location":"ENGINEERING_SPEC/#136-int8-calibration-cache-governance-contract","title":"13.6 INT8 Calibration Cache Governance Contract","text":"<p>In TensorRT INT8 builds with calibration cache enabled: - calibration cache keys must be deterministic and include:   - model/export identity hash   - plugin contract metadata (version + namespace)   - precision profile   - calibration dataset version - calibration dataset version may be:   - explicitly configured (<code>calibration_dataset_version</code>)   - auto-derived from calibration batch digest - cache governance rules:   - unchanged key must reuse existing cache   - changed key must invalidate cache automatically   - legacy raw cache blobs are accepted only when key governance is disabled - build results must expose:   - <code>calibration_cache_key</code>   - <code>calibration_dataset_version</code></p>"},{"location":"ENGINEERING_SPEC/#14-performance-regression-testing","title":"14. Performance Regression Testing","text":""},{"location":"ENGINEERING_SPEC/#141-required-metrics","title":"14.1 Required Metrics","text":"<ul> <li>latency <code>p50/p95</code></li> <li>memory peak</li> <li>selected tile count distribution</li> <li>budget adherence (<code>spent &lt;= B</code>)</li> </ul>"},{"location":"ENGINEERING_SPEC/#142-regression-policy","title":"14.2 Regression Policy","text":"<p>A change fails perf gate if: - <code>p95</code> latency worsens over threshold - memory peak exceeds threshold - budget overshoot rate is non-zero in deterministic mode</p>"},{"location":"ENGINEERING_SPEC/#143-baseline-tooling","title":"14.3 Baseline Tooling","text":"<ul> <li><code>scripts/perf_regression.py</code> provides CPU reference timing</li> <li>runtime-specific harnesses must follow same report schema</li> </ul>"},{"location":"ENGINEERING_SPEC/#15-export-contracts-trtort","title":"15. Export Contracts (TRT/ORT)","text":"<ul> <li>dynamic tile count only via fixed <code>Kmax</code> buffers + valid count</li> <li>fixed tile size and fixed max nesting depth in model profile</li> <li>inference graph must not depend on Python control flow</li> </ul>"},{"location":"ENGINEERING_SPEC/#16-cpu-baseline-requirements","title":"16. CPU Baseline Requirements","text":"<p>Reference implementation must: - run on CPU-only environment - implement routing, deterministic budgeting, pack/unpack, and placeholder Tile-SSM - include tests for core contracts</p>"},{"location":"ENGINEERING_SPEC/#17-validation-matrix","title":"17. Validation Matrix","text":""},{"location":"ENGINEERING_SPEC/#171-correctness","title":"17.1 Correctness","text":"<ul> <li>pack/unpack identity</li> <li>overlap priority correctness</li> <li>deterministic ordering</li> <li>deterministic selection</li> </ul>"},{"location":"ENGINEERING_SPEC/#172-training-stability","title":"17.2 Training Stability","text":"<ul> <li>non-collapsing router probabilities under budget</li> <li>distillation gap trend improves over epochs</li> </ul>"},{"location":"ENGINEERING_SPEC/#173-export-parity","title":"17.3 Export Parity","text":"<ul> <li>bounded quality drop between PyTorch and TRT/ORT profiles</li> </ul>"},{"location":"ENGINEERING_SPEC/#174-eval-dataset-contract-cli-model-eval-path","title":"17.4 Eval Dataset Contract (CLI Model Eval Path)","text":"<p>For <code>apex_x cli eval --dataset-npz</code>: - required:   - <code>.npy</code> or <code>.npz</code> input with <code>images</code> tensor shape <code>[N,3,H,W]</code> - optional:   - <code>.npz</code> key <code>det_score_target</code> (compat alias <code>det_scores_target</code>) with shape <code>[N]</code> or <code>[N,1]</code>   - <code>.npz</code> key <code>selected_tiles_target</code> (compat alias <code>selected_tiles_targets</code>) with shape <code>[N]</code> or <code>[N,1]</code> - report contract:   - <code>model_eval.det_score</code> aggregate stats (<code>mean/std/min/max</code>)   - <code>model_eval.selected_tiles</code> aggregate stats (<code>mean/p95</code>)   - when <code>det_score_target</code> is provided, <code>model_eval.det_score_target</code> includes:     - <code>mae</code>     - <code>rmse</code>     - <code>bias</code>     - <code>r2</code> (nullable when target variance is zero)     - <code>pearson_corr</code> (nullable when variance is zero)   - when <code>selected_tiles_target</code> is provided, <code>model_eval.selected_tiles_target</code> includes:     - <code>mae</code>     - <code>rmse</code>     - <code>bias</code>     - <code>exact_match_rate</code></p>"},{"location":"ENGINEERING_SPEC/#18-runtime-capability-and-parity-contracts","title":"18. Runtime Capability and Parity Contracts","text":""},{"location":"ENGINEERING_SPEC/#181-backend-capability-matrix","title":"18.1 Backend Capability Matrix","text":"<p>Canonical backend matrix: - <code>cpu</code>:   - required: CPU torch execution   - optional: none - <code>torch_cuda</code>:   - required: <code>cuda.available = true</code>   - optional: none - <code>triton</code>:   - required: <code>cuda.available = true</code>, <code>triton.available = true</code>   - optional: Triton version metadata   - TileSSM long-sequence contract:     - when sequence length exceeds single-launch bound, runtime must use chunked scan launches       with recurrent state carry-over across chunks   - TileUnpack overlap-mode contract:     - dispatch supports <code>override</code> and <code>blend</code> modes     - blend mode must not rely on hardcoded forced-reference dispatch branches   - Stage-1 fused selector contract:     - FF heavy-path inference may use fused stage-1 route only under explicit compatibility gates     - non-compatible cases must fall back to decomposed path with deterministic behavior - <code>tensorrt</code>:   - required: <code>cuda.available = true</code>, <code>tensorrt.python_available = true</code>   - optional: local header availability for build workflows</p> <p>Precision overlays: - TensorRT INT8 requires:   - TensorRT Python module   - CUDA   - <code>BuilderFlag.INT8</code> - FP8 requires:   - FP8 torch dtype exposure   - CUDA   - compute capability policy gate (<code>sm90+</code>)</p>"},{"location":"ENGINEERING_SPEC/#182-reason-code-contract","title":"18.2 Reason-Code Contract","text":"<p>Runtime capability probes must emit deterministic reason codes only from the catalog: - CUDA: <code>cuda_unavailable</code>, <code>cuda_device_not_found</code>, <code>cuda_device_index_out_of_range</code>, <code>cuda_query_failed</code> - Triton: <code>triton_not_installed</code> - TensorRT Python: <code>tensorrt_python_not_installed</code>, <code>tensorrt_python_import_failed</code> - TensorRT INT8: <code>tensorrt_python_unavailable</code>, <code>cuda_required_for_tensorrt_int8</code>, <code>tensorrt_int8_builder_flag_missing</code> - FP8: <code>torch_build_missing_fp8_dtype</code>, <code>fp8_requires_cuda</code>, <code>cuda_compute_capability_unknown</code>, <code>compute_capability_below_sm90</code></p> <p>Hardware metadata (for example compute capability values) must be exposed in dedicated fields, not dynamic reason strings.</p>"},{"location":"ENGINEERING_SPEC/#183-tensorrt-cli-runtime-execution-contract","title":"18.3 TensorRT CLI Runtime Execution Contract","text":"<p>For CLI/inference-runner TensorRT execution: - required runtime artifact:   - <code>APEXX_TRT_ENGINE_PATH</code> (serialized engine file) - optional artifacts and controls:   - <code>APEXX_EXPORT_MANIFEST_PATH</code> for manifest/ONNX hash preflight   - <code>APEXX_TRT_VERIFY_MANIFEST_HASH</code> (<code>true</code> by default)   - <code>APEXX_TRT_PLUGIN_LIB</code> (<code>os.pathsep</code>-separated plugin <code>.so/.dylib</code> paths)   - <code>APEXX_TRT_INPUT_NAME</code> for explicit input tensor binding when engine has multiple inputs   - <code>APEXX_TRT_EXTRA_INPUTS_NPZ</code> for named auxiliary inputs (<code>.npz</code>)   - <code>APEXX_TRT_PRIMARY_OUTPUT_NAME</code> for explicit primary output mapping into CLI result schema   - <code>APEXX_TRT_DET_BOXES_NAME</code>, <code>APEXX_TRT_DET_SCORES_NAME</code>,     <code>APEXX_TRT_DET_CLASS_IDS_NAME</code>, <code>APEXX_TRT_DET_VALID_NAME</code>     for explicit DET output mapping - strict/permissive policy:   - strict mode must fail on preflight/runtime execution errors   - permissive mode must fall back deterministically and emit execution fallback reason - runtime telemetry:   - <code>runtime.latency_ms.total</code> for full inference-call wall time   - <code>runtime.latency_ms.backend_execute</code> for backend execution segment   - <code>runtime.latency_ms.backend_preflight</code> for backend preflight/handshake segment   - schema applies to both Python CLI reports and Go service <code>/predict</code> response payloads - Go runtime bridge execution contract:   - ORT bridge env controls:     - <code>APEXX_ORT_BRIDGE_CMD</code>   - TRT bridge env controls:     - <code>APEXX_TRT_BRIDGE_CMD</code>   - bridge payload protocol:     - request JSON stdin: <code>{backend, artifact_path, requests[]}</code>     - response JSON stdout: <code>{results[], error?}</code>   - adapters must fail closed when bridge/native backend execution is unavailable     (no synthetic score fallback in production path) - service error mapping (Go runtime):   - queue saturation must return HTTP <code>429</code>   - predict timeout must return HTTP <code>504</code>   - backend unavailable must return HTTP <code>503</code>   - backend inference/protocol failures must return HTTP <code>502</code> - service canary mode (Go runtime):   - optional secondary adapter compares sampled requests asynchronously   - mismatch telemetry counters must be exported in service metrics   - capture policy controls:     - <code>canary-capture-policy</code> / <code>APEXX_CANARY_CAPTURE_POLICY</code> (<code>off|mismatch|error|all</code>)     - <code>canary-capture-path</code> / <code>APEXX_CANARY_CAPTURE_PATH</code> (JSONL sink)     - <code>canary-capture-max-bytes</code> / <code>APEXX_CANARY_CAPTURE_MAX_BYTES</code> (size guard)   - CI SLA gate:     - <code>TestCanaryLoadGateThresholds</code> validates timeout/queue-overflow rates and canary overhead thresholds</p>"},{"location":"ENGINEERING_SPEC/#184-parity-tolerance-profiles","title":"18.4 Parity Tolerance Profiles","text":"<p>Parity tolerance presets must be versioned and testable: - <code>quality</code> - <code>balanced</code> - <code>edge</code></p> <p>Each preset defines: - op-level tolerances - end-to-end tolerances - mismatch ratio limit</p> <p>Profile selection must be explicit in parity tests.</p> <p>TensorRT parity harness requirements: - backend pair matrix must cover:   - <code>reference</code> vs <code>triton</code>   - <code>reference</code> vs <code>tensorrt</code>   - <code>triton</code> vs <code>tensorrt</code> - sweep dimensions must include:   - shape cases (at least small/medium representative inputs)   - precision cases (at least FP16 + FP32 candidate envelopes on CPU-safe tests;     INT8/FP8 where runtime support exists)</p>"},{"location":"ENGINEERING_SPEC/#185-triton-autotune-registry-contract","title":"18.5 Triton Autotune Registry Contract","text":"<p>Triton perf runs must expose deterministic autotune telemetry: - registry key:   - <code>op_name + shape_bucket</code> - registry value:   - <code>selected_config</code> (<code>BLOCK_*</code>, <code>num_warps</code>, <code>num_stages</code>, when available)   - <code>selection_source</code> (<code>triton_best_config</code>, <code>heuristic</code>, <code>registry_cache</code>)   - cache counters (<code>launches</code>, <code>cache_hits</code>, <code>cache_misses</code>) - benchmark report requirements:   - JSON includes <code>triton_autotune.summary</code> and <code>triton_autotune.entries</code>   - Markdown includes a readable autotune registry section for regression review   - <code>summary</code> must include <code>cache_entries</code> and <code>cache_hit_rate</code></p>"},{"location":"ENGINEERING_SPEC/#186-unified-perf-regression-policy-cpugputrt","title":"18.6 Unified Perf Regression Policy (CPU/GPU/TRT)","text":"<p>Perf regression wrappers must share one pass/fail formula: - fail when:   - <code>current_ms &gt; baseline_ms * (1 + max_regression_ratio) + max_regression_abs_ms</code></p> <p>Regression wrappers: - CPU: <code>scripts/perf_regression.py</code> - GPU: <code>scripts/perf_regression_gpu.py</code> - TensorRT shape-sweep: <code>scripts/perf_regression_trt.py</code></p> <p>All wrappers must support: - baseline template generation mode - compare mode with explicit baseline - normalized trend artifact output with shared schema keys:   - <code>schema_version</code>   - <code>suite</code>   - <code>timestamp_utc</code>   - <code>overall_status</code>   - <code>total_metrics</code>   - <code>failed_metrics</code>   - <code>metrics[]</code></p>"},{"location":"ENGINEERING_SPEC/#19-file-ownership-map","title":"19. File Ownership Map","text":"<ul> <li><code>apex_x/</code>: reference implementation</li> <li><code>tests/</code>: correctness checks</li> <li><code>docs/runtime/</code>: runtime plugin details</li> <li><code>scripts/</code>: repeatable perf checks</li> </ul>"},{"location":"ENGINEERING_SPEC/#20-change-management","title":"20. Change Management","text":"<p>Any changes to equations, contracts, ordering, or precision policy require synchronized updates across: - <code>docs/PRD.md</code> - <code>docs/ENGINEERING_SPEC.md</code> - <code>docs/DECISIONS.md</code> - <code>docs/CONTEXT.md</code></p>"},{"location":"FP8/","title":"Apex-X FP8 Precision Policy","text":""},{"location":"FP8/#scope","title":"Scope","text":"<p>This document defines how Apex-X enables FP8 for heavy compute ops and how it falls back safely.</p> <p>Authoritative references: - <code>docs/PRD.md</code> (FR-12) - <code>docs/ENGINEERING_SPEC.md</code> (Section 13)</p>"},{"location":"FP8/#policy-summary","title":"Policy Summary","text":"<ul> <li>Heavy ops are FP8-eligible only when support is detected.</li> <li>Router and KAN-like paths stay in FP16.</li> <li>If FP8 is requested but not supported, fallback is FP16.</li> </ul>"},{"location":"FP8/#request-rules","title":"Request Rules","text":"<p>FP8 is requested when either is true: - <code>runtime.precision_profile == \"balanced\"</code> - <code>train.qat_fp8 == true</code></p>"},{"location":"FP8/#support-detection","title":"Support Detection","text":"<p>Implemented in <code>apex_x/runtime/precision.py</code>: - device must be CUDA - torch build must expose FP8 dtype (<code>torch.float8_e4m3fn</code>) - CUDA capability gate is conservative (<code>sm90+</code>)</p> <p>If any check fails, policy falls back to FP16.</p>"},{"location":"FP8/#effective-dtypes","title":"Effective Dtypes","text":"<p>Policy object (<code>PrecisionPolicy</code>) exposes: - <code>heavy_ops_dtype</code> - <code>router_dtype</code> (always FP16) - <code>kan_dtype</code> (always FP16) - <code>fp8_requested</code> - <code>fp8_enabled</code> - <code>fallback_reason</code></p> <p>Fallback reasons are canonical reason-codes aligned with runtime capability catalog (for example <code>fp8_requires_cuda</code>, <code>compute_capability_below_sm90</code>).</p>"},{"location":"FP8/#trainer-integration","title":"Trainer Integration","text":"<p><code>ApexXTrainer</code> resolves precision policy at init and reports it in: - <code>train_summary[\"precision\"]</code></p> <p>For heavy-op execution context: - FP16 path uses autocast where safe. - FP8 path is marked as ready and left for specialized kernel/plugin integration.</p>"},{"location":"FP8/#fallback-behavior-expected-on-cpu","title":"Fallback Behavior (Expected on CPU)","text":"<p>On CPU runs, balanced profile requests FP8 but falls back to FP16: - <code>fp8_requested = true</code> - <code>fp8_enabled = false</code> - <code>fallback_reason = \"fp8_requires_cuda\"</code></p>"},{"location":"FP8/#validation","title":"Validation","text":"<p>Covered by <code>tests/test_precision_policy.py</code>: - CPU fallback smoke - mocked supported CUDA FP8 path - trainer summary precision diagnostics - fallback reason-code catalog compliance</p> <p>Covered by <code>tests/test_gpu_bench_fp8.py</code>: - GPU bench FP8 request telemetry on non-CUDA hosts - Markdown summary visibility of FP8 fallback/enabled state</p>"},{"location":"FP8/#gpu-benchmark-notes","title":"GPU Benchmark Notes","text":"<p><code>apex_x.bench.gpu_bench</code> now accepts <code>--dtype fp8</code>.</p> <p>Report telemetry includes: - <code>requested_dtype</code> - <code>effective_dtype</code> - <code>fp8_requested</code> - <code>fp8_enabled</code> - <code>fp8_fallback_reason</code></p> <p>On hosts without FP8 capability, benchmark falls back to FP16 and records fallback reason.</p>"},{"location":"PERF/","title":"Apex-X Performance Regression Suite","text":""},{"location":"PERF/#scope","title":"Scope","text":"<p>This document defines the CPU performance regression suite used in CI.</p>"},{"location":"PERF/#benchmarks","title":"Benchmarks","text":"<p>The suite lives in: - <code>apex_x/bench/perf.py</code> - <code>scripts/perf_regression.py</code></p> <p>It includes: 1. Fixed-size <code>infer()</code> benchmark (CPU baseline)    - model: <code>ApexXModel</code>    - input: <code>[1,3,128,128]</code>    - metrics: <code>infer_p50_ms</code>, <code>infer_p95_ms</code> 2. Microbenchmarks (CPU)    - <code>TilePackTorch</code>    - <code>TileUnpackTorch</code>    - <code>FusionGate</code>    - metrics: <code>tile_pack_p50_ms</code>, <code>tile_unpack_p50_ms</code>, <code>fusion_gate_p50_ms</code></p>"},{"location":"PERF/#baseline-and-tolerances","title":"Baseline and Tolerances","text":"<p>Committed baseline file: - <code>scripts/perf_baseline_cpu.json</code></p> <p>Each metric has: - <code>value_ms</code> - <code>max_regression_ratio</code> - <code>max_regression_abs_ms</code></p> <p>Regression check rule: - fail when <code>current_ms &gt; value_ms * (1 + ratio) + abs_ms</code></p>"},{"location":"PERF/#local-usage","title":"Local Usage","text":"<p>Run suite only: <pre><code>python scripts/perf_regression.py --output artifacts/perf_current.json\n</code></pre></p> <p>Run and compare with baseline: <pre><code>python scripts/perf_regression.py \\\n  --compare \\\n  --baseline scripts/perf_baseline_cpu.json \\\n  --output artifacts/perf_current.json \\\n  --summary artifacts/perf_compare.json \\\n  --trend-output artifacts/perf_trend_cpu.json\n</code></pre></p> <p>Regenerate baseline template from current machine: <pre><code>python scripts/perf_regression.py \\\n  --emit-baseline-template \\\n  --baseline scripts/perf_baseline_cpu.json\n</code></pre></p>"},{"location":"PERF/#ci","title":"CI","text":"<p>Workflow job <code>perf-regression</code> runs on <code>ubuntu-latest</code> (CPU-only): - runs <code>scripts/perf_regression.py --compare</code> - writes:   - <code>artifacts/perf_current_ci.json</code>   - <code>artifacts/perf_compare_ci.json</code>   - <code>artifacts/perf_trend_cpu_ci.json</code> - fails CI when status is <code>fail</code></p> <p>Weekly trend workflow: - <code>.github/workflows/perf_trend_weekly.yml</code> (<code>cpu-trend</code> job) - writes:   - <code>artifacts/perf_current_weekly.json</code>   - <code>artifacts/perf_compare_weekly.json</code>   - <code>artifacts/perf_trend_cpu_weekly.json</code>   - <code>artifacts/release/release_attestation_cpu_weekly.json</code>   - <code>artifacts/release/release_attestation_cpu_weekly.md</code></p>"},{"location":"PERF/#normalized-trend-artifact","title":"Normalized Trend Artifact","text":"<p>CPU and GPU perf regression scripts emit the same trend artifact schema: - <code>schema_version</code> - <code>suite</code> - <code>timestamp_utc</code> - <code>overall_status</code> - <code>total_metrics</code> - <code>failed_metrics</code> - <code>metrics[]</code> with:   - <code>metric</code>, <code>status</code>, <code>baseline_ms</code>, <code>allowed_max_ms</code>, <code>current_ms</code>, <code>regression_ratio</code></p> <p>TensorRT shape-sweep regression wrapper follows the same compare/trend policy: - script: <code>scripts/perf_regression_trt.py</code> - baseline: <code>scripts/perf_baseline_trt.json</code> - outputs:   - <code>artifacts/perf_trt_current*.json</code>   - <code>artifacts/perf_trt_compare*.json</code>   - <code>artifacts/perf_trt_trend*.json</code> - comparison rule is the same:   - fail when <code>current_ms &gt; value_ms * (1 + ratio) + abs_ms</code></p> <p>This provides one common reporting shape for weekly and per-PR trend tracking.</p> <p>Release checklist wiring: - CPU CI job also writes:   - <code>artifacts/release/release_attestation_ci.json</code>   - <code>artifacts/release/release_attestation_ci.md</code> - These files auto-populate <code>docs/release/CHECKLIST.md</code> evidence links with SHA256/status fields.</p>"},{"location":"PERF/#notes","title":"Notes","text":"<ul> <li>This suite is intentionally CPU-only and stable-friendly.</li> <li>Tolerances are set to avoid flakiness from CI VM noise while still catching major regressions.</li> </ul>"},{"location":"PERF/#deterministic-replay-fixtures","title":"Deterministic Replay Fixtures","text":"<p>Golden replay fixtures for deterministic selection/order contracts live in: - <code>tests/fixtures/replay_golden_small.json</code> - <code>tests/fixtures/replay_golden_medium.json</code></p> <p>Replay checks: - <code>tests/test_replay_golden.py</code> validates selection/order metadata hashes against fixture expectations. - <code>tests/test_repro.py</code> validates seed/config/artifact hashing utilities.</p> <p>Replay manifest contract: - <code>seed</code> - <code>config_sha256</code> - <code>artifact_hashes</code> - <code>artifact_sha256</code> - <code>manifest_sha256</code></p> <p>Recommended command: <pre><code>python -m pytest -q tests/test_repro.py tests/test_replay_golden.py\n</code></pre></p>"},{"location":"PERF_GPU/","title":"Apex-X GPU Benchmark Suite","text":""},{"location":"PERF_GPU/#scope","title":"Scope","text":"<p>This suite targets CUDA machines and produces a unified performance report for: - Tile ops: <code>TilePack</code>, <code>TileUnpack</code>, <code>FusionGate</code> (torch reference vs Triton dispatch) - TileSSM scan: torch reference vs Triton dispatch, plus TensorRT plugin path when available - End-to-end FF inference path: torch eager vs torch+Triton fast-path, plus optional TensorRT engine run</p> <p>The runner is: - <code>apex_x/bench/gpu_bench.py</code></p> <p>Outputs: - JSON report: latency <code>p50/p95</code>, throughput, peak memory - Markdown summary: compact table for quick regression checks</p>"},{"location":"PERF_GPU/#fixed-baseline-profile","title":"Fixed Baseline Profile","text":"<p>Default fixed profile (stable for comparisons): - <code>batch=1</code> - <code>channels=128</code> - <code>height=128</code> - <code>width=128</code> - <code>tile_size=8</code> - <code>kmax=32</code> - <code>steps=256</code> - <code>budget_b1=16</code>, <code>budget_b2=8</code>, <code>budget_total=32</code> - <code>dtype=fp16</code></p>"},{"location":"PERF_GPU/#local-usage","title":"Local Usage","text":"<p>Run the full GPU benchmark suite: <pre><code>python -m apex_x.bench.gpu_bench \\\n  --output-json artifacts/perf_gpu.json \\\n  --output-md artifacts/perf_gpu.md\n</code></pre></p> <p>Run faster smoke-like pass: <pre><code>python -m apex_x.bench.gpu_bench \\\n  --warmup 3 \\\n  --iters 10 \\\n  --output-json artifacts/perf_gpu_smoke.json \\\n  --output-md artifacts/perf_gpu_smoke.md\n</code></pre></p>"},{"location":"PERF_GPU/#tensorrt-plugin-bench-tilessm","title":"TensorRT Plugin Bench (TileSSM)","text":"<p>To enable TensorRT plugin microbench in the suite: <pre><code>export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so\npython -m apex_x.bench.gpu_bench\n</code></pre></p> <p>If TensorRT Python or plugin library is missing, the report marks this section as <code>skipped</code>.</p>"},{"location":"PERF_GPU/#tensorrt-engine-bench-optional-end-to-end","title":"TensorRT Engine Bench (Optional End-to-End)","text":"<p>To benchmark a serialized engine: <pre><code>python -m apex_x.bench.gpu_bench \\\n  --trt-engine-path artifacts/trt/apex_x.engine \\\n  --trt-input-shape input=1x3x128x128\n</code></pre></p> <p>For dynamic-shape engines, provide one <code>--trt-input-shape name=AxBx...</code> per dynamic input. If missing, unresolved dynamic dims default to <code>1</code> and may fail for some engines.</p>"},{"location":"PERF_GPU/#tensorrt-engine-shape-sweep","title":"TensorRT Engine Shape Sweep","text":"<p>For dynamic-shape validation across multiple deployment profiles: <pre><code>python -m apex_x.bench.trt_engine_sweep \\\n  --trt-engine-path artifacts/trt/apex_x.engine \\\n  --shape-case \"input=1x3x128x128\" \\\n  --shape-case \"input=1x3x256x256\" \\\n  --output-json artifacts/perf_trt_shape_sweep.json \\\n  --output-md artifacts/perf_trt_shape_sweep.md\n</code></pre></p> <p>For multi-input engines, pass named tensors in a single case: <pre><code>python -m apex_x.bench.trt_engine_sweep \\\n  --trt-engine-path artifacts/trt/apex_x.engine \\\n  --shape-case \"image=1x3x128x128;centers=1024x2;strides=1024\"\n</code></pre></p>"},{"location":"PERF_GPU/#report-schema-notes","title":"Report Schema Notes","text":"<p>JSON top-level keys: - <code>schema_version</code> - <code>suite</code> - <code>timestamp_utc</code> - <code>environment</code> - <code>config</code> - <code>status</code> - <code>benchmarks</code> - <code>triton_autotune</code></p> <p>Important metrics: - <code>p50_ms</code>, <code>p95_ms</code> - throughput fields:   - <code>tiles_per_s</code>   - <code>tokens_per_s</code>   - <code>elements_per_s</code>   - <code>frames_per_s</code> - <code>peak_memory_mb</code> (CUDA <code>max_memory_allocated</code>)</p> <p><code>triton_autotune</code> report block: - <code>summary</code>:   - <code>cache_entries</code>   - <code>launches</code>   - <code>cache_hits</code>   - <code>cache_misses</code>   - <code>cache_hit_rate</code> - <code>entries[]</code>:   - <code>op_name</code>   - <code>kernel_name</code>   - <code>shape_bucket</code>   - <code>selected_config</code>   - <code>selection_source</code> (<code>triton_best_config|heuristic|registry_cache</code>)   - <code>launches</code>   - <code>cache_hits</code>   - <code>cache_misses</code></p>"},{"location":"PERF_GPU/#fallback-behavior","title":"Fallback Behavior","text":"<ul> <li>If CUDA is unavailable: suite returns <code>status=skipped</code>.</li> <li>If Triton is unavailable: dispatch path falls back to reference and records reason.</li> <li>If TensorRT is unavailable: TensorRT sections are reported as <code>skipped</code> with reason.</li> </ul>"},{"location":"PERF_GPU/#recommended-regression-workflow","title":"Recommended Regression Workflow","text":"<ol> <li>Run with fixed defaults and save JSON.</li> <li>Compare <code>p50/p95</code> and throughput against prior run.</li> <li>Investigate regressions by section:</li> <li>tile ops</li> <li>TileSSM</li> <li>end-to-end infer</li> </ol>"},{"location":"PERF_GPU/#ci-regression-compare","title":"CI Regression Compare","text":"<p>GPU CI compares against <code>scripts/perf_baseline_gpu.json</code> with: <pre><code>python scripts/perf_regression_gpu.py \\\n  --compare \\\n  --baseline scripts/perf_baseline_gpu.json \\\n  --output artifacts/perf_gpu_current_ci.json \\\n  --summary artifacts/perf_gpu_compare_ci.json \\\n  --trend-output artifacts/perf_gpu_trend_ci.json\n</code></pre></p> <p>TensorRT shape-sweep regression compare wrapper: <pre><code>python scripts/perf_regression_trt.py \\\n  --compare \\\n  --baseline scripts/perf_baseline_trt.json \\\n  --output artifacts/perf_trt_current.json \\\n  --summary artifacts/perf_trt_compare.json \\\n  --trend-output artifacts/perf_trt_trend.json \\\n  --trt-engine-path artifacts/trt/apex_x.engine \\\n  --shape-case \"input=1x3x128x128\" \\\n  --shape-case \"input=1x3x256x256\"\n</code></pre></p> <p>TRT baseline template generation: <pre><code>python scripts/perf_regression_trt.py \\\n  --emit-baseline-template \\\n  --baseline scripts/perf_baseline_trt.json \\\n  --trt-engine-path artifacts/trt/apex_x.engine \\\n  --shape-case \"input=1x3x128x128\"\n</code></pre></p> <p>Failure rule matches CPU policy: - fail when <code>current_ms &gt; value_ms * (1 + max_regression_ratio) + max_regression_abs_ms</code></p> <p>Trend artifact uses the same normalized schema as CPU: - <code>schema_version</code>, <code>suite</code>, <code>timestamp_utc</code>, <code>overall_status</code> - <code>total_metrics</code>, <code>failed_metrics</code> - <code>metrics[]</code> with <code>metric/status/baseline_ms/allowed_max_ms/current_ms/regression_ratio</code></p> <p>Workflows: - <code>.github/workflows/perf_gpu.yml</code> (<code>gpu-perf-regression</code>) for PR/manual/scheduled GPU gate - <code>.github/workflows/perf_trend_weekly.yml</code> (<code>gpu-trend</code>) for weekly trend artifacts - GPU workflows also auto-generate release evidence drafts:   - <code>artifacts/release/release_attestation_gpu_ci.json</code>   - <code>artifacts/release/release_attestation_gpu_ci.md</code>   - <code>artifacts/release/release_attestation_gpu_weekly.json</code>   - <code>artifacts/release/release_attestation_gpu_weekly.md</code></p>"},{"location":"PERF_GPU/#replay-and-hash-logging","title":"Replay and Hash Logging","text":"<p>For deterministic replay metadata (seed/config/artifact hashes), use: - <code>apex_x.utils.build_replay_manifest(...)</code> - golden fixtures:   - <code>tests/fixtures/replay_golden_small.json</code>   - <code>tests/fixtures/replay_golden_medium.json</code></p> <p>Validation command: <pre><code>python -m pytest -q tests/test_repro.py tests/test_replay_golden.py\n</code></pre></p>"},{"location":"PRD/","title":"Apex-X v4 PRD","text":""},{"location":"PRD/#1-document-status","title":"1. Document Status","text":"<ul> <li>Project: <code>Apex-X v4</code></li> <li>Purpose: Authoritative product requirements for the Apex-X open-source repository</li> <li>This document is normative for scope, behavior, and acceptance criteria</li> <li>Implementation details are specified in <code>docs/ENGINEERING_SPEC.md</code></li> </ul>"},{"location":"PRD/#2-product-goal","title":"2. Product Goal","text":"<p>Apex-X v4 is a dual-stream, utility-routed vision architecture that preserves quality while enforcing explicit compute budgets.</p> <p>Primary outcomes: - high-quality detection and segmentation under fixed latency/compute budgets - deterministic inference behavior suitable for production runtime plugins - clean export path to TensorRT/ORT with dynamic selection handled via <code>Kmax</code> buffers and plugins</p>"},{"location":"PRD/#3-scope","title":"3. Scope","text":""},{"location":"PRD/#31-in-scope","title":"3.1 In Scope","text":"<ul> <li>Tasks: DET, INST-SEG, SEM-SEG, optional POSE/TRACK/PANO</li> <li>Dual stream architecture: PV (always-on) + FF (sparse high-res tiles)</li> <li>Utility-based routing with oracle supervision</li> <li>Continuous-budget training and deterministic-budget inference</li> <li>Quadtree nesting (<code>L0/L1/L2</code>) with split policy</li> <li>Tile graph pack/unpack contracts and deterministic ordering</li> <li>Temporal hysteresis for anti-flicker</li> <li>Runtime plugin contracts for TensorRT/ORT</li> <li>QAT policy for INT8/FP8 deployment paths</li> </ul>"},{"location":"PRD/#32-out-of-scope-for-baseline-release","title":"3.2 Out of Scope (for baseline release)","text":"<ul> <li>Final production GPU kernels</li> <li>Multi-node distributed training recipes</li> <li>End-to-end tracking benchmarks</li> </ul>"},{"location":"PRD/#4-terms","title":"4. Terms","text":"<ul> <li><code>PV</code>: Peripheral Vision stream (low-res, dense)</li> <li><code>FF</code>: Foveal Focus stream (high-res, sparse tiles)</li> <li><code>L0/L1/L2</code>: tile nesting levels (<code>L0</code> coarse, <code>L1/L2</code> finer)</li> <li><code>Kmax</code>: fixed upper bound on active tile buffer length</li> <li><code>B, B1, B2</code>: total and per-level budgets</li> <li><code>U_i</code>: utility score for tile <code>i</code></li> <li><code>S_i</code>: split utility for tile <code>i</code></li> <li><code>O_split</code>: split overhead cost term</li> </ul>"},{"location":"PRD/#5-functional-requirements","title":"5. Functional Requirements","text":""},{"location":"PRD/#fr-1-dual-stream","title":"FR-1 Dual Stream","text":"<p>System shall compute PV densely and FF sparsely.</p>"},{"location":"PRD/#fr-2-utility-router","title":"FR-2 Utility Router","text":"<p>System shall score tiles with utility <code>U_i</code> and optional split utility <code>S_i</code>.</p>"},{"location":"PRD/#fr-3-oracle-supervision","title":"FR-3 Oracle Supervision","text":"<p>System shall support oracle delta utility labels: [ \\Delta_i = L_{distill}(y^{(i=0)}, y_T) - L_{distill}(y^{(i=1)}, y_T) ] where <code>i=0</code> means heavy path disabled on tile <code>i</code>, <code>i=1</code> means enabled.</p>"},{"location":"PRD/#fr-4-continuous-budgeting-training","title":"FR-4 Continuous Budgeting (Training)","text":"<p>System shall optimize routing probabilities via: [ p_i = \\sigma(U_i),\\quad g_i = STE(p_i) ] Expected cost: [ \\mathbb{E}[C] = \\sum_i \\left(p_i C_h + (1-p_i) C_c\\right) ] Constrained objective: [ L = L_{main} + \\mu(\\mathbb{E}[C] - B),\\quad \\mu \\ge 0 ] Dual update concept: - if <code>E[C] &gt; B</code>, increase <code>mu</code> - if <code>E[C] &lt; B</code>, decrease <code>mu</code> - if adaptive schedule is enabled, effective update rate must be:   - step-decayed   - scaled by EMA of normalized budget error   - bounded by configured min/max scale - optional deadband near target and delta clipping must be supported to avoid oscillation.</p>"},{"location":"PRD/#fr-5-deterministic-inference-budgeting","title":"FR-5 Deterministic Inference Budgeting","text":"<p>System shall select FF tiles deterministically by utility-per-cost: [ score_i = \\frac{U_i}{\\Delta C_i} ] sorted descending with selection until budget or <code>Kmax</code> is reached.</p>"},{"location":"PRD/#fr-6-quadtree-nesting","title":"FR-6 Quadtree Nesting","text":"<p>System shall support two-level and optional three-level nesting: - <code>L0</code> under budget <code>B1</code> - <code>L1</code> split under budget <code>B2</code> - optional <code>L2</code> split under budget <code>B3</code> Split score: [ score^{split}i = \\frac{S_i}{O{split}} ] Split parent ordering must be deterministic (score desc, tile-id asc tie-break).</p>"},{"location":"PRD/#fr-7-tile-tensor-contracts","title":"FR-7 Tile Tensor Contracts","text":"<p>System shall support deterministic tile gather/scatter: - Pack: <code>[B,C,Hf,Wf] + idx[B,K] -&gt; P[B,K,C,t,t] + meta</code> - Unpack: <code>F_base + P_out + meta -&gt; F_merged</code></p>"},{"location":"PRD/#fr-8-deterministic-tile-ordering","title":"FR-8 Deterministic Tile Ordering","text":"<p>System shall support: - Hilbert ordering - Multi-direction scan ordering (<code>LR</code>, <code>RL</code>, <code>UD</code>, <code>DU</code>)</p>"},{"location":"PRD/#fr-9-tile-ssm-and-fusion-gate","title":"FR-9 Tile-SSM and Fusion Gate","text":"<p>System shall include a tile-sequence mixer (Mamba-like placeholder acceptable in baseline) and fusion gate behavior.</p>"},{"location":"PRD/#fr-10-temporal-stability","title":"FR-10 Temporal Stability","text":"<p>System shall support hysteresis: [ z_i(t)=\\mathbf{1}[U_i(t)&gt;\\theta_{on}\\;\\lor\\;(U_i(t)&gt;\\theta_{off}\\land z_i(t-1)=1)] ] with <code>theta_on &gt; theta_off</code>.</p> <p>For deployment routing, system shall support budget-aware hysteresis carryover: - temporal state reuse must not violate per-frame active-tile budget - when clipping is required, selection must be deterministic</p> <p>Temporal quality reporting shall include: - tile flip rate - temporal consistency (<code>1 - flip_rate</code>)</p>"},{"location":"PRD/#fr-11-runtime-plugin-contracts","title":"FR-11 Runtime Plugin Contracts","text":"<p>System shall define plugin interfaces for TensorRT/ORT: - <code>TilePack</code> - <code>TileSSMScan</code> - <code>TileUnpackFusion</code> - optional <code>MaskedConv</code> - fused <code>DecodeNMS</code> TensorRT build path shall validate plugin creator contract metadata at build time: - presence - version - namespace - expected plugin field signature</p>"},{"location":"PRD/#fr-12-qatprecision-policy","title":"FR-12 QAT/Precision Policy","text":"<p>System shall define deployment profiles: - Quality: FP16-heavy - Balanced: FP16/FP8 mixed - Edge: INT8 (router FP16) For INT8 deployment builds, sensitive routing layers (router/KAN-like) must be enforced to FP16 and reported in per-layer precision diagnostics. For INT8 calibration-cache reuse, cache governance must be deterministic and bind to: - model/export identity hash - plugin version/namespace contract metadata - precision profile - calibration dataset version (explicit version or deterministic dataset digest) INT4 allowed only with guaranteed kernels and quality gate.</p>"},{"location":"PRD/#fr-13-perf-regression-requirements","title":"FR-13 Perf Regression Requirements","text":"<p>System shall include repeatable latency and memory regressions with thresholds and fail criteria. System shall apply one regression compare formula across CPU, GPU, and TensorRT shape-sweep reports. System shall include backend parity sweep harnesses that validate: - reference vs triton - reference vs tensorrt - triton vs tensorrt across representative shape and precision cases under profile-specific tolerances. System shall support optional dataset-level target regression diagnostics in eval reports when benchmark/eval datasets provide explicit scalar targets (<code>det_score_target</code>, <code>selected_tiles_target</code>).</p>"},{"location":"PRD/#fr-14-runtime-capability-transparency-contract","title":"FR-14 Runtime Capability Transparency Contract","text":"<p>System shall expose deterministic runtime capability diagnostics for backend selection: - canonical capability matrix for <code>cpu</code>, <code>torch_cuda</code>, <code>triton</code>, <code>tensorrt</code> - required vs optional capabilities per backend - stable reason-code catalog for non-available paths - strict vs permissive fallback behavior described in runtime docs - runtime report fields include:   - <code>requested_backend</code>   - <code>selected_backend</code>   - <code>execution_backend</code>   - <code>selection_fallback_reason</code>   - <code>execution_fallback_reason</code>   - <code>requested_dtype</code>   - <code>effective_dtype</code>   - <code>fp8_requested</code>   - <code>fp8_enabled</code>   - <code>fp8_fallback_reason</code>   - <code>latency_ms.total</code>   - <code>latency_ms.backend_execute</code>   - <code>latency_ms.backend_preflight</code> These runtime telemetry fields must be exposed consistently in CLI reports and service API responses. Service runtime shall classify overload/timeout failures deterministically: - queue saturation -&gt; HTTP <code>429</code> - predict timeout -&gt; HTTP <code>504</code> Service runtime shall classify backend execution failures deterministically: - backend unavailable -&gt; HTTP <code>503</code> - backend inference/protocol failures -&gt; HTTP <code>502</code> Service runtime shall support optional canary shadow-compare mode with mismatch telemetry counters. Canary payload capture (when enabled) shall use explicit policy modes (<code>off|mismatch|error|all</code>) and bounded storage settings. Triton TileSSM runtime path shall support long-sequence execution using chunked scan launches with deterministic recurrent state carry-over between chunks. Triton TileUnpack dispatch shall support both overlap modes (<code>override</code>, <code>blend</code>) without hardcoded forced reference fallback branches when accelerated path is selected. Stage-1 Triton fused route in FF heavy-path inference shall be compatibility-gated and must fall back deterministically to decomposed path when compatibility predicates are not satisfied. Triton GPU benchmark reporting shall expose per-op shape-bucket autotune telemetry with cached selected launch configuration metadata and cache hit/miss counters.</p>"},{"location":"PRD/#6-non-functional-requirements","title":"6. Non-Functional Requirements","text":"<ul> <li>Determinism: same input + same config -&gt; same active tile set</li> <li>Exportability: avoid Python-side runtime control flow</li> <li>CPU baseline runnable from clean environment</li> <li>Testability: correctness, stability, performance checks</li> </ul>"},{"location":"PRD/#7-pseudo-code-requirements","title":"7. Pseudo-Code Requirements","text":""},{"location":"PRD/#71-utility-oracle-labeling","title":"7.1 Utility Oracle Labeling","text":"<pre><code>for each minibatch:\n  sample oracle tile subset S (random + high teacher uncertainty + long-tail)\n  for i in S:\n    y0 = forward_with_tile(i, enabled=0)\n    y1 = forward_with_tile(i, enabled=1)\n    Delta_i = L_distill(y0, y_teacher) - L_distill(y1, y_teacher)\n    Delta_i = clamp(Delta_i, -tau, tau)\n  train router utility head to predict Delta_i (L1 or ranking loss)\n  log oracle diagnostics:\n    sample composition + delta distribution + clipping ratio\n</code></pre>"},{"location":"PRD/#72-continuous-budget-training","title":"7.2 Continuous Budget Training","text":"<pre><code>U = router(features)\np = sigmoid(U)\ng = STE(p)\ny = model_forward_with_gate(g)\nL_main = task_losses(y, target)\nE_cost = sum_i(p_i * C_h + (1-p_i) * C_c)\nL = L_main + mu * (E_cost - B)\nbackprop(L)\ne = (E_cost - B) / B\nema_e = beta*ema_e + (1-beta)*e\neta_eff = (eta_mu / (1 + decay*step)) * clip(1 + abs(ema_e), lr_min, lr_max)\ndelta = eta_eff * (E_cost - B)\nif abs(e) &lt;= deadband_ratio:\n  delta = 0\ndelta = clip(delta, -delta_clip, delta_clip)  # optional\nmu = clip(mu + delta, mu_min, mu_max)\n</code></pre>"},{"location":"PRD/#73-deterministic-inference-budgeting","title":"7.3 Deterministic Inference Budgeting","text":"<pre><code>U = router(features)\nfor each tile i:\n  score_i = U_i / DeltaC_i\norder = argsort(score, descending=True)\nactive = []\nspent = 0\nfor i in order:\n  if len(active) == Kmax: break\n  if spent + DeltaC_i &lt;= B:\n    active.append(i)\n    spent += DeltaC_i\nreturn active\n</code></pre>"},{"location":"PRD/#74-quadtree-l0l1l2-policy","title":"7.4 Quadtree L0/L1/L2 Policy","text":"<pre><code>L0 = select_by_budget(U_L0, cost_L0, B1, Kmax_L0)\ncandidates = []\nfor i in L0:\n  split_score_i = S_i / O_split_i\n  candidates.append((i, split_score_i))\nL1 = top_by_budget(candidates, B2, Kmax_L1)\nif nesting_depth == 2:\n  L2 = repeat_split_for_L1_under_B3\n</code></pre>"},{"location":"PRD/#75-tilepacktileunpack-and-ordering","title":"7.5 TilePack/TileUnpack and Ordering","text":"<pre><code>idx_ordered = order_idx(idx, mode=hilbert|multi_direction)\nP, meta = TilePack(F, idx_ordered, t)\nP_mix = TileSSM_and_local_refine(P)\nF_merged = TileUnpackFusion(F_base, P_mix, meta, gate)\n</code></pre>"},{"location":"PRD/#8-architecture-requirements","title":"8. Architecture Requirements","text":"<ul> <li>PV stream: always-dense low-res features</li> <li>FF stream: sparse high-res packed tiles</li> <li>FPN split: low-res dense path + high-res sparse path</li> <li>Tile-SSM + local refinement in FF heavy blocks</li> <li>DET/SEG heads consume merged features</li> </ul>"},{"location":"PRD/#9-loss-requirements","title":"9. Loss Requirements","text":"<ul> <li>DET: focal/quality focal + IoU-family box loss (optional DFL)</li> <li>INST-SEG: BCE + Dice + boundary loss</li> <li>Multi-task conflict handling: PCGrad++ on shared trunk only</li> <li>PCGrad++ diagnostics must report conflict metrics before/after projection:</li> <li><code>conflicting_pairs</code>, <code>conflicting_pairs_after</code></li> <li><code>total_pairs</code>, <code>conflict_rate_before</code>, <code>conflict_rate_after</code></li> </ul> <p>PCGrad projection when cosine similarity &lt; 0: [ g_a \\leftarrow g_a - \\frac{g_a \\cdot g_b}{\\lVert g_b \\rVert^2} g_b ]</p>"},{"location":"PRD/#10-acceptance-criteria","title":"10. Acceptance Criteria","text":"<p>Release is accepted only if all are true: - deterministic tile selection and ordering pass tests - pack/unpack semantics pass identity and overlap tests - training budget control converges around target budget - CPU baseline runs via <code>examples/run_cpu_baseline.py</code> - CI passes lint and tests - runtime plugin contracts and precision profiles are documented - runtime capability matrix and reason-code contract are documented and test-backed - parity tolerance profiles (<code>quality</code>, <code>balanced</code>, <code>edge</code>) are documented and test-backed - PCGrad++ projection is restricted to shared-trunk params and conflict-rate metrics are visible in training reports</p>"},{"location":"PRD/#11-dependencies-and-interfaces","title":"11. Dependencies and Interfaces","text":"<ul> <li>reference implementation: Python + NumPy (CPU-only baseline)</li> <li>runtime targets: TensorRT and ONNX Runtime via plugin contracts</li> </ul>"},{"location":"PRD/#12-deliverables-in-this-repository","title":"12. Deliverables in This Repository","text":"<ul> <li>Source: <code>apex_x/</code></li> <li>Tests: <code>tests/</code></li> <li>Authoritative docs: <code>docs/PRD.md</code>, <code>docs/ENGINEERING_SPEC.md</code></li> <li>Runtime docs: <code>docs/runtime/PLUGIN_SPEC.md</code></li> <li>Project memory: <code>docs/CONTEXT.md</code></li> </ul>"},{"location":"PRD/#13-change-control","title":"13. Change Control","text":"<p>Any architectural change to routing, budgets, tile contracts, ordering, plugin behavior, or precision policy must update: - <code>docs/PRD.md</code> - <code>docs/ENGINEERING_SPEC.md</code> - <code>docs/DECISIONS.md</code> - <code>docs/CONTEXT.md</code></p>"},{"location":"QAT/","title":"Apex-X INT8 QAT Policy","text":""},{"location":"QAT/#scope","title":"Scope","text":"<p>This document defines the CPU-reference INT8 quantization-aware training (QAT) path and PTQ fallback used in Apex-X.</p> <p>Authoritative references: - <code>docs/PRD.md</code> (FR-12) - <code>docs/ENGINEERING_SPEC.md</code> (Section 13)</p>"},{"location":"QAT/#design-goals","title":"Design Goals","text":"<ul> <li>Provide a deterministic INT8 simulation path during training.</li> <li>Keep router/gating logic in FP16-safe precision path.</li> <li>Support PTQ fallback for <code>edge</code> precision profile when QAT is not enabled.</li> <li>Keep CPU baseline runnable and testable.</li> </ul>"},{"location":"QAT/#implemented-components","title":"Implemented Components","text":""},{"location":"QAT/#fake-quant-modules","title":"Fake Quant Modules","text":"<p>Implemented in <code>apex_x/train/qat.py</code>: - <code>ActivationObserver</code>: running min/max activation statistics. - <code>ActivationFakeQuant</code>: per-tensor activation fake quant. - <code>WeightPerChannelFakeQuant</code>: symmetric per-channel weight fake quant. - <code>FakeQuantConv2d</code>: wraps <code>nn.Conv2d</code>. - <code>FakeQuantLinear</code>: wraps <code>nn.Linear</code>.</p> <p>Quantization ranges: - Activations: unsigned INT8 (<code>0..255</code>), affine. - Weights: signed INT8 (<code>-127..127</code>), symmetric per output channel.</p>"},{"location":"QAT/#qat-preparation","title":"QAT Preparation","text":"<p><code>prepare_int8_qat(model, ...)</code>: - Replaces quantizable <code>Conv2d</code>/<code>Linear</code> modules with fake-quant wrappers. - Enables observers + fake quantization for training-time simulation. - Skips modules whose names include <code>router</code>, <code>gate</code>, or <code>gating</code>.</p>"},{"location":"QAT/#ptq-calibration-fallback","title":"PTQ Calibration Fallback","text":"<p><code>prepare_int8_ptq(model, calibration_inputs, forward_fn, ...)</code>: - Applies same wrapper conversion. - Runs observer-only calibration passes (<code>fake_quant=off</code>). - Freezes observers and enables fake quant (<code>observer=off</code>, <code>fake_quant=on</code>).</p> <p>This is used as fallback for runtime profile <code>edge</code> when INT8 QAT is not explicitly enabled.</p>"},{"location":"QAT/#trainer-integration","title":"Trainer Integration","text":"<p><code>apex_x/train/trainer.py</code> integrates quantization policy: - If <code>train.qat_enable=true</code> and <code>train.qat_int8=true</code>:   - prepare INT8 QAT (<code>mode=qat_int8</code>) - Else if <code>runtime.precision_profile=edge</code>:   - run PTQ calibration fallback (<code>mode=ptq_int8</code>) - Else:   - quantization disabled (<code>mode=disabled</code>)</p> <p>Trainer <code>train_summary[\"quantization\"]</code> includes: - <code>mode</code> - <code>wrapped_modules</code> - <code>calibration_batches</code> - <code>router_gating_fp16</code></p>"},{"location":"QAT/#routergating-precision-rule","title":"Router/Gating Precision Rule","text":"<p>Router and gating paths are excluded from INT8 wrapping by policy: - skip-name filters include <code>router</code>, <code>gate</code>, <code>gating</code> - staged gating math keeps FP16 path for router utility gating operations</p> <p>This enforces the spec requirement that routing-sensitive numerics remain in higher precision.</p>"},{"location":"QAT/#validation","title":"Validation","text":"<p>Coverage is provided in <code>tests/test_qat.py</code>: - wrapper conversion and skip policy checks - PTQ calibration state transitions - trainer-level QAT/PTQ toggles and finite outputs</p>"},{"location":"TODO/","title":"Apex-X Full Readiness Backlog","text":"<p>Last updated: 2026-02-11</p> <p>This file is the execution backlog for taking Apex-X from CPU-first reference + partial GPU acceleration to a full production-ready GPU project with strict correctness, performance, and reproducibility gates.</p> <p>The target is \"100% ready\" under a concrete engineering definition, not marketing wording.</p>"},{"location":"TODO/#1-definition-of-100-ready","title":"1. Definition of \"100% Ready\"","text":"<p>All items below must be true:</p> <ol> <li>End-to-end inference (<code>predict</code>, <code>eval</code>, service runtime) works on:</li> <li><code>cpu</code> reference</li> <li><code>torch</code> CUDA path</li> <li><code>triton</code> accelerated path</li> <li><code>tensorrt</code> engine path</li> <li>No critical runtime path depends on placeholder/no-op stubs.</li> <li>CPU vs GPU vs TRT parity is validated on golden sets with explicit tolerances.</li> <li>Performance gates are enforced in CI:</li> <li>latency (<code>p50</code>, <code>p95</code>)</li> <li>throughput</li> <li>peak memory</li> <li>compile/build success on target environments</li> <li>Precision policy is operational:</li> <li>FP16 stable</li> <li>INT8 calibrated path stable</li> <li>FP8 policy and fallback behavior validated</li> <li>Go runtime service executes real inference backends (not synthetic scoring stubs).</li> <li>Release checklist is pass/fail with reproducible artifacts and rollback playbook.</li> </ol>"},{"location":"TODO/#2-current-snapshot-context-for-prioritization","title":"2. Current Snapshot (Context for Prioritization)","text":"<ul> <li>CPU baseline is stable and well tested.</li> <li>Triton kernel paths exist for major tile ops and TileSSM, with fallback semantics.</li> <li>TensorRT builder/calibration and plugin code paths exist, but full integration and validation   must be tightened end-to-end.</li> <li>GPU CI exists but is optional; not yet a mandatory merge gate for GPU-critical changes.</li> <li>Some CLI/service/export paths still need stronger production-grade backend wiring.</li> </ul>"},{"location":"TODO/#3-execution-rules","title":"3. Execution Rules","text":"<ol> <li>Every task must include:</li> <li>concrete code changes</li> <li>tests</li> <li>documentation updates</li> <li>measurable acceptance criteria</li> <li>No task is marked done without:</li> <li>green tests</li> <li>parity evidence</li> <li>benchmark evidence (when performance-related)</li> <li>Any shape-contract or runtime-contract change must update:</li> <li><code>docs/PRD.md</code></li> <li><code>docs/ENGINEERING_SPEC.md</code></li> <li><code>docs/DECISIONS.md</code></li> <li><code>docs/CONTEXT.md</code></li> </ol> <p>Status legend for tasks below: - <code>Status: [x]</code> completed and validated - <code>Status: [~]</code> partially implemented - <code>Status: [ ]</code> not started</p>"},{"location":"TODO/#4-critical-path-order","title":"4. Critical Path (Order)","text":"<ol> <li>Phase P0: freeze contracts and observability baseline (completed 2026-02-10)</li> <li>Phase P1: end-to-end backend selection and execution</li> <li>Phase P2: Triton parity + performance completion</li> <li>Phase P3: TensorRT production completion</li> <li>Phase P4: training/math excellence track</li> <li>Phase P5: Go service production integration</li> <li>Phase P6: CI/CD hard gates and release readiness</li> </ol> <p>Recently completed and removed from active queue: - <code>P2-04</code> Remove legacy fused Triton stub dependency (2026-02-10) - <code>P5-01</code> Replace synthetic adapter scoring with real backend inference calls (2026-02-11) - <code>P5-02</code> Service-level batching and timeout SLA enforcement (2026-02-11) - <code>P5-03</code> Canary parity mode in service runtime (2026-02-11) - <code>P6-03</code> Release checklist CI evidence attestation wiring (2026-02-11) - <code>X-03</code> Backward compatibility migration notes + changelog (2026-02-11) - <code>P4-06</code> Temporal hysteresis and state reuse quality gates (2026-02-11) - <code>X-01</code> Runtime documentation consistency sweep (2026-02-11) - <code>P4-05</code> Quadtree split policy completion with budget-aware recursion (2026-02-11) - <code>P4-03</code> Deterministic inference budgeting stress tests (2026-02-11) - <code>P4-02</code> Continuous budget controller convergence guarantees (2026-02-11) - <code>P4-04</code> PCGrad++ completeness on shared trunk in multi-task training (2026-02-11) - <code>P4-01</code> Utility-oracle data pipeline hardening (2026-02-11) - <code>P1-02</code> Replace export no-op with real export pipeline (2026-02-11) - <code>P1-03</code> End-to-end <code>predict</code> path on GPU backends (2026-02-11) - <code>P1-04</code> End-to-end <code>eval</code> path with real model execution (2026-02-11) - <code>P3-04</code> FP16/INT8 mixed precision policy enforcement for sensitive layers (2026-02-11) - <code>P3-01</code> End-to-end plugin registration and contract validation at build time (2026-02-11) - <code>P3-03</code> INT8 production calibration flow and cache governance (2026-02-11) - <code>P2-01</code> Close feature gaps in Triton tile unpack path (2026-02-11) - <code>P2-02</code> Stabilize TileSSM Triton limits and long-sequence behavior (2026-02-11) - <code>P2-03</code> Promote fused stage-1 path to default accelerated route (2026-02-11) - <code>P3-02</code> Shape inference, serialization, and dynamic-shape coverage for TRT plugins (2026-02-11) - Device validation: Triton GPU parity suite on CUDA host (2026-02-11) - Device validation: GPU perf baseline artifacts on CUDA host (2026-02-11) - Device validation: Go ORT bridge with real ONNX model on host (2026-02-11) - Device validation: TensorRT plugin native C++ tests on CUDA host (2026-02-11) - Device validation: TensorRT plugin Python parity tests on CUDA host (2026-02-11)</p> <p>Device/deployment-blocked validation queue: - Blocking snapshot (2026-02-11):   - <code>torch.cuda.is_available() == True</code>   - <code>torch.cuda.device_count() == 1</code> (<code>NVIDIA GeForce RTX 2070 SUPER</code>, <code>sm75</code>)   - <code>triton</code> Python module available (<code>3.5.1</code>)   - <code>tensorrt</code> Python module available (<code>10.15.1.29</code>)   - <code>onnxruntime</code> Python module available (<code>1.24.1</code>)   - <code>cmake</code> CLI available (<code>4.2.1</code>)   - CUDA compiler available (<code>nvcc 12.8.93</code>, conda <code>cuda-nvcc</code>)   - TensorRT plugin shared library available:     - <code>runtime/tensorrt/build_cuda_10_15/libapexx_trt_plugins.so</code>     - validation summary: <code>artifacts/trt_plugin_validation_summary.{json,md}</code>   - GitHub CLI available (<code>gh 2.86.0</code>) but unauthenticated on host     (<code>GH_TOKEN</code> or <code>gh auth login</code> required for API/workflow operations) - <code>Status: [ ]</code> Run TensorRT engine shape sweep on deployment GPU and attach artifacts:   - local run completed with synthetic dynamic engine (refreshed 2026-02-11):     - <code>artifacts/models/trt_bench_dynamic.engine</code>     - <code>artifacts/perf_trt_shape_sweep.json</code>     - <code>artifacts/perf_trt_shape_sweep.md</code>   - blocker: final deployment rerun is still required with production deployment engine.   - latest blocker log: <code>artifacts/trt_shape_sweep_deployment_blocker.log</code>   - <code>python -m apex_x.bench.trt_engine_sweep --trt-engine-path &lt;engine&gt; --shape-case \"input=1x3x128x128\" --shape-case \"input=1x3x256x256\" --output-json artifacts/perf_trt_shape_sweep.json --output-md artifacts/perf_trt_shape_sweep.md</code> - <code>Status: [ ]</code> Run TensorRT regression compare/trend wrapper on deployment GPU and archive artifacts:   - local run completed and compare currently passes on synthetic engine (refreshed 2026-02-11):     - <code>artifacts/perf_trt_current.json</code>     - <code>artifacts/perf_trt_compare.json</code>     - <code>artifacts/perf_trt_trend.json</code>   - blocker: baseline tuning with final deployment engine is still required.   - latest blocker log: <code>artifacts/trt_regression_deployment_blocker.log</code>   - <code>python scripts/perf_regression_trt.py --compare --baseline scripts/perf_baseline_trt.json --output artifacts/perf_trt_current.json --summary artifacts/perf_trt_compare.json --trend-output artifacts/perf_trt_trend.json --trt-engine-path &lt;engine&gt; --shape-case \"input=1x3x128x128\" --shape-case \"input=1x3x256x256\"</code> - <code>Status: [ ]</code> Capture FP8 benchmark evidence on supported GPU (<code>sm90+</code>) for P4-07 closure:   - local FP8 request evidence captured on this host:     - <code>artifacts/perf_gpu_fp8.json</code>     - <code>artifacts/perf_gpu_fp8.md</code>     - <code>artifacts/perf_gpu_fp8_current.json</code>     - <code>artifacts/perf_gpu_fp8_compare.json</code>   - latest blocker log: <code>artifacts/fp8_sm90_blocker.log</code>   - blocker: host GPU is <code>sm75</code>; FP8 request falls back with <code>compute_capability_below_sm90</code>   - <code>python -m apex_x.bench.gpu_bench --dtype fp8 --warmup 10 --iters 50 --output-json artifacts/perf_gpu_fp8.json --output-md artifacts/perf_gpu_fp8.md</code>   - optional regression wrapper:     - <code>python scripts/perf_regression_gpu.py --dtype fp8 --output artifacts/perf_gpu_fp8_current.json --compare --baseline scripts/perf_baseline_gpu.json --summary artifacts/perf_gpu_fp8_compare.json</code> - <code>Status: [ ]</code> Validate GPU mandatory PR gate on GitHub protected branch settings:   - blocker: cannot query/modify protected-branch settings without authenticated GitHub CLI/API access     (<code>gh</code> installed, but host is not authenticated)   - latest blocker log: <code>artifacts/github_branch_protection_blocker.log</code>   - Required status check: <code>GPU Perf Regression / gpu-perf-regression</code>   - <code>gh api repos/Voskan/Apex-X/branches/main/protection --jq '.required_status_checks.checks[].context'</code>   - Open a GPU-critical PR and confirm merge is blocked until GPU workflow passes - <code>Status: [ ]</code> Run weekly GPU trend workflow on deployment runner and archive artifacts:   - blocker: requires authenticated GitHub Actions dispatch on self-hosted GPU runner with repo variable management     (<code>gh workflow run ...</code> currently fails with auth prompt)   - latest blocker log: <code>artifacts/github_weekly_gpu_blocker.log</code>   - Enable repository variable <code>APEXX_ENABLE_GPU_WEEKLY=true</code>   - <code>gh variable set APEXX_ENABLE_GPU_WEEKLY --body true</code>   - <code>gh workflow run perf_trend_weekly.yml -f run_gpu=true -f trt_engine_path=&lt;engine&gt;</code>   - <code>gh run watch &lt;run-id&gt;</code>   - <code>gh run download &lt;run-id&gt; -n perf-trend-gpu-weekly -D artifacts/weekly_gpu</code>   - Verify <code>.github/workflows/perf_trend_weekly.yml</code> uploads:     - <code>artifacts/perf_gpu_current_weekly.json</code>     - <code>artifacts/perf_gpu_compare_weekly.json</code>     - <code>artifacts/perf_gpu_trend_weekly.json</code> - <code>Status: [ ]</code> Validate Go runtime TRT bridge/native path with deployment engine on CUDA+TensorRT host:   - local bridge validation completed with real TensorRT engine artifact:     - engine: <code>artifacts/models/trt_bench_dynamic.engine</code>     - Go tests: <code>artifacts/go_trt_bridge_test_real_engine.log</code>     - bridge probe: <code>artifacts/service_bridge_trt_real_engine.json</code>     - bridge shape-mismatch diagnostic: <code>artifacts/service_bridge_trt_real_engine.raw.log</code>   - local rerun refreshed these artifacts on 2026-02-11.   - blocker: final deployment rerun is still required with production deployment engine.   - latest blocker log: <code>artifacts/go_trt_bridge_deployment_blocker.log</code>   - <code>cd runtime/go &amp;&amp; APEXX_TRT_ENGINE_PATH=&lt;engine.plan&gt; APEXX_TRT_BRIDGE_CMD=\"python -m apex_x.runtime.service_bridge\" CGO_ENABLED=1 go test -tags tensorrt ./...</code></p>"},{"location":"TODO/#phase-p1-end-to-end-runtime-path-completion","title":"Phase P1 - End-to-End Runtime Path Completion","text":""},{"location":"TODO/#phase-p2-triton-completion-correctness-speed","title":"Phase P2 - Triton Completion (Correctness + Speed)","text":""},{"location":"TODO/#p2-05-triton-perf-autotune-and-kernel-configuration-registry","title":"P2-05. Triton perf autotune and kernel configuration registry","text":"<p>Status: [x]</p> <p>Why: - \"Best result\" requires shape-aware tuning, not static launch settings.</p> <p>Implementation: - Add autotune registry per op/shape bucket. - Cache selected configs and expose telemetry.</p> <p>Progress (2026-02-11): - Added Triton autotune registry module:   - <code>apex_x/kernels/triton/autotune_registry.py</code>   - tracks per-op/per-shape-bucket selected launch config, selection source, and cache hit/miss counts - Wired registry telemetry into Triton kernels with autotune configs:   - TilePack (<code>_tilepack_kernel</code>)   - TileUnpack priority/scatter (<code>_tileunpack_priority_kernel</code>, <code>_tileunpack_scatter_kernel</code>)   - FusionGate alpha/fuse (<code>_fusiongate_alpha_kernel</code>, <code>_fusiongate_fuse_kernel</code>)   - Fused stage-1 pack/op/unpack (<code>_fused_pack_op_unpack_kernel</code>) - Extended GPU benchmark report output:   - JSON now includes <code>triton_autotune</code> block with <code>summary</code> + <code>entries</code>   - Markdown summary now includes <code>Triton Autotune Registry</code> table - Added CPU-safe contract tests:   - <code>tests/test_triton_autotune_registry.py</code> - Captured CUDA benchmark evidence with autotune telemetry and p50/p95 comparisons:   - <code>artifacts/perf_gpu.json</code>   - <code>artifacts/perf_gpu.md</code></p> <p>Files: - <code>apex_x/kernels/triton/*.py</code> - <code>apex_x/bench/gpu_bench.py</code> - <code>docs/PERF_GPU.md</code></p> <p>Acceptance: - Benchmark report captures chosen config and resulting p50/p95 improvements.</p> <p>Validation: - <code>python -m pytest -q tests/test_triton_autotune_registry.py tests/test_gpu_bench_fp8.py</code> - <code>python -m apex_x.bench.gpu_bench --output-json artifacts/perf_gpu.json --output-md artifacts/perf_gpu.md</code></p>"},{"location":"TODO/#phase-p3-tensorrt-production-completion","title":"Phase P3 - TensorRT Production Completion","text":""},{"location":"TODO/#p3-05-tensorrt-end-to-end-parity-harness-against-reference-and-triton","title":"P3-05. TensorRT end-to-end parity harness against reference and Triton","text":"<p>Status: [~]</p> <p>Why: - Need evidence that TRT path is fast and correct.</p> <p>Implementation: - Build parity suite:   - reference vs triton   - reference vs tensorrt   - triton vs tensorrt - Include shape sweep and precision sweep.</p> <p>Progress (2026-02-11): - Added backend matrix parity APIs in <code>apex_x/runtime/parity.py</code>:   - <code>ParityMatrixCase</code>   - <code>run_parity_matrix_case(...)</code>   - <code>run_parity_sweep(...)</code>   - <code>ParitySweepReport</code> - Added CPU-safe sweep tests for TRT parity harness contract:   - <code>tests/test_trt_parity_harness.py</code>   - validates pair matrix, shape sweep, and precision sweep execution. - Added real CUDA matrix parity coverage in TensorRT plugin parity suites:   - TilePack and TileSSM tests now assert:     - reference vs triton     - reference vs tensorrt     - triton vs tensorrt   - shape-sweep coverage was expanded via multi-shape parametrization for:     - <code>tests/test_tensorrt_tilepack_parity.py</code>     - <code>tests/test_tensorrt_tilessm_parity.py</code>     - <code>tests/test_tensorrt_tileunpackfusion_parity.py</code>   - deployment-host validation artifacts:     - <code>artifacts/trt_plugins_pytest.log</code>     - <code>artifacts/trt_plugins_ctest_cuda_10_15.log</code>     - <code>artifacts/trt_plugins_native_exec.log</code>     - <code>artifacts/trt_plugin_shape_serialization_validation.json</code>     - <code>artifacts/trt_plugin_shape_serialization_validation.md</code> - Remaining gap:   - run end-to-end backend parity sweep (<code>reference/triton/tensorrt</code>) against production     deployment TensorRT engine profiles and archive shape+precision sweep artifacts.</p> <p>Files: - <code>apex_x/runtime/parity.py</code> - <code>tests/test_trt_parity*.py</code> - <code>tests/test_tensorrt_tilepack_parity.py</code> - <code>tests/test_tensorrt_tilessm_parity.py</code> - <code>tests/test_tensorrt_tileunpackfusion_parity.py</code> - <code>runtime/tensorrt/CMakeLists.txt</code> - <code>docs/runtime/PARITY.md</code></p> <p>Acceptance: - All required cases pass tolerance and mismatch-ratio thresholds.</p> <p>Validation: - <code>python -m pytest -q tests/test_trt_parity*</code> - <code>python -m pytest -q tests/test_tensorrt_tilepack_parity.py tests/test_tensorrt_tilessm_parity.py tests/test_tensorrt_tileunpackfusion_parity.py tests/test_tensorrt_plugin_contracts.py</code> - <code>ctest --test-dir runtime/tensorrt/build_cuda_10_15 --output-on-failure</code></p>"},{"location":"TODO/#phase-p4-math-and-model-quality-track-best-math-execution","title":"Phase P4 - Math and Model Quality Track (\"Best Math\" Execution)","text":""},{"location":"TODO/#p4-07-fp8-policy-from-declaration-to-measurable-runtime-value","title":"P4-07. FP8 policy from declaration to measurable runtime value","text":"<p>Status: [~]</p> <p>Why: - FP8 policy exists; readiness requires operational and measured gain.</p> <p>Implementation: - Integrate FP8 execution where supported for heavy ops. - Add explicit fallback telemetry and parity/perf tests.</p> <p>Progress (2026-02-11): - Precision fallback reasons were normalized to canonical runtime reason-codes in:   - <code>apex_x/runtime/precision.py</code> - GPU bench now supports explicit FP8 request mode with operational telemetry:   - <code>--dtype fp8</code> in <code>apex_x/bench/gpu_bench.py</code>   - report fields:     - <code>requested_dtype</code>     - <code>effective_dtype</code>     - <code>fp8_requested</code>     - <code>fp8_enabled</code>     - <code>fp8_fallback_reason</code> - GPU perf regression wrapper now accepts FP8 request mode:   - <code>scripts/perf_regression_gpu.py --dtype fp8</code> - Added FP8 telemetry tests:   - <code>tests/test_gpu_bench_fp8.py</code>   - expanded <code>tests/test_precision_policy.py</code> - Remaining gap:   - run FP8 benchmark on supported deployment GPU (sm90+) and attach measurable perf evidence.</p> <p>Files: - <code>apex_x/runtime/precision.py</code> - <code>apex_x/model/ff_heavy_path.py</code> - <code>docs/FP8.md</code> - <code>tests/test_precision_policy.py</code></p> <p>Acceptance: - On supported GPUs, FP8 path runs and demonstrates measurable speed/memory advantage   within agreed quality envelope.</p> <p>Validation: - <code>python -m pytest -q tests/test_precision_policy.py</code> - <code>python -m apex_x.bench.gpu_bench --output-json artifacts/perf_gpu_fp8.json</code></p>"},{"location":"TODO/#phase-p6-cicd-hard-gates-and-release","title":"Phase P6 - CI/CD Hard Gates and Release","text":""},{"location":"TODO/#p6-01-make-gpu-regression-workflow-mandatory-for-gpu-critical-changes","title":"P6-01. Make GPU regression workflow mandatory for GPU-critical changes","text":"<p>Status: [~]</p> <p>Why: - Optional GPU checks allow silent performance/correctness regressions.</p> <p>Implementation: - Enforce required status checks for files touching:   - <code>apex_x/kernels/</code>   - <code>apex_x/runtime/</code>   - <code>runtime/tensorrt/</code> - Keep controlled self-hosted runner policy.</p> <p>Progress (2026-02-11): - <code>.github/workflows/perf_gpu.yml</code> now triggers on <code>pull_request</code> when GPU-critical paths change. - Trusted-branch policy is enforced:   - same-repo PRs run <code>gpu-perf-regression</code> on self-hosted GPU runner   - fork PRs are blocked by <code>blocked-untrusted-pr</code> (no untrusted code on self-hosted GPU) - Contract test coverage now protects workflow policy from regressions:   - <code>tests/test_gpu_ci_workflow_contract.py</code>   - validates PR path triggers, trust guard, and self-hosted GPU runner labels. - Remaining gap:   - repository branch protection must require <code>GPU Perf Regression / gpu-perf-regression</code> status check.</p> <p>Files: - <code>.github/workflows/perf_gpu.yml</code> - <code>.github/workflows/ci.yml</code> - <code>docs/CI_GPU.md</code></p> <p>Acceptance: - PRs touching GPU-critical paths cannot merge without GPU checks.</p> <p>Validation: - Workflow dry-run on protected branch policy.</p>"},{"location":"TODO/#p6-02-unified-perf-regression-policy-cpu-gpu-trt","title":"P6-02. Unified perf regression policy (CPU + GPU + TRT)","text":"<p>Status: [~]</p> <p>Why: - Need one rulebook for perf pass/fail.</p> <p>Implementation: - Normalize baseline schema and thresholds across CPU/GPU suites. - Add trend artifact generation for weekly comparison.</p> <p>Progress (2026-02-11): - Added normalized trend artifact output to both regression runners:   - <code>scripts/perf_regression.py --trend-output ...</code>   - <code>scripts/perf_regression_gpu.py --trend-output ...</code> - Added TensorRT shape-sweep regression wrapper aligned with the same compare/trend policy:   - <code>scripts/perf_regression_trt.py</code>   - baseline spec: <code>scripts/perf_baseline_trt.json</code>   - normalized trend artifact: <code>artifacts/perf_trt_trend*.json</code> - Added committed TensorRT baseline template file for compare-mode wiring:   - <code>scripts/perf_baseline_trt.json</code> (template, metrics populated on deployment TensorRT runner) - Local TensorRT compare/trend smoke run is now passing with a real CUDA engine artifact:   - <code>artifacts/models/trt_bench_dynamic.engine</code>   - <code>artifacts/perf_trt_current.json</code>   - <code>artifacts/perf_trt_compare.json</code>   - <code>artifacts/perf_trt_trend.json</code> - CI jobs now publish trend artifacts:   - CPU: <code>artifacts/perf_trend_cpu_ci.json</code> in <code>.github/workflows/ci.yml</code>   - GPU: <code>artifacts/perf_gpu_trend_ci.json</code> in <code>.github/workflows/perf_gpu.yml</code> - GPU and weekly workflows now run optional TensorRT compare/trend when <code>TRT_ENGINE_PATH</code> is provided:   - CI: <code>artifacts/perf_trt_current_ci.json</code>, <code>artifacts/perf_trt_compare_ci.json</code>, <code>artifacts/perf_trt_trend_ci.json</code>   - weekly: <code>artifacts/perf_trt_current_weekly.json</code>, <code>artifacts/perf_trt_compare_weekly.json</code>, <code>artifacts/perf_trt_trend_weekly.json</code> - Added weekly trend workflow:   - <code>.github/workflows/perf_trend_weekly.yml</code>   - CPU weekly artifact set is always generated   - GPU weekly artifact set is generated on self-hosted GPU when enabled (<code>APEXX_ENABLE_GPU_WEEKLY=true</code>) - Remaining gap:   - validate GPU compare + weekly GPU trend jobs on deployment CUDA runner.   - tune <code>scripts/perf_baseline_gpu.json</code> and <code>scripts/perf_baseline_trt.json</code> against deployment hardware;     local <code>--dtype fp8</code> compare run currently fails baseline limits on this <code>sm75</code> host.</p> <p>Files: - <code>scripts/perf_regression.py</code> - <code>scripts/perf_regression_gpu.py</code> - <code>scripts/perf_baseline_cpu.json</code> - <code>scripts/perf_baseline_gpu.json</code> - <code>docs/PERF.md</code> - <code>docs/PERF_GPU.md</code></p> <p>Acceptance: - Same regression formula and reporting format across suites.</p> <p>Validation: - <code>python scripts/perf_regression.py --compare --baseline scripts/perf_baseline_cpu.json</code> - <code>python scripts/perf_regression_gpu.py --compare --baseline scripts/perf_baseline_gpu.json</code> - <code>python scripts/perf_regression_trt.py --compare --baseline scripts/perf_baseline_trt.json --trt-engine-path &lt;engine&gt;</code></p>"},{"location":"TODO/#5-cross-cutting-work-packages","title":"5. Cross-Cutting Work Packages","text":""},{"location":"TODO/#x-02-telemetry-schema-normalization","title":"X-02. Telemetry schema normalization","text":"<p>Status: [x]</p> <p>Scope: - Ensure CLI, Python runtime, and Go service emit aligned telemetry keys:   - backend   - precision profile   - fallback reason   - selected tile stats   - latency breakdown</p> <p>Progress (2026-02-10): - Python runner telemetry now emits explicit latency breakdown in runtime metadata:   - <code>runtime.latency_ms.total</code>   - <code>runtime.latency_ms.backend_execute</code>   - <code>runtime.latency_ms.backend_preflight</code> - CLI <code>predict</code>/<code>eval</code> JSON reports include the same runtime latency fields. - Test coverage added for runtime latency schema in:   - <code>tests/test_infer_runner.py</code>   - <code>tests/test_cli.py</code> - Go runtime <code>/predict</code> response now emits aligned runtime telemetry keys:   - <code>requested_backend</code>, <code>selected_backend</code>, <code>execution_backend</code>   - <code>fallback_policy</code>, <code>precision_profile</code>   - <code>selection_fallback_reason</code>, <code>execution_fallback_reason</code>   - <code>latency_ms.total</code>, <code>latency_ms.backend_execute</code>, <code>latency_ms.backend_preflight</code> - Go runtime telemetry schema is covered by service tests:   - <code>runtime/go/internal/service/batcher_test.go</code>   - <code>runtime/go/internal/service/http_test.go</code>   - <code>runtime/go/internal/service/integration_test.go</code></p> <p>Acceptance: - Telemetry fields are documented once and validated in tests.</p>"},{"location":"TODO/#6-evidence-requirements-for-best-result-best-math","title":"6. Evidence Requirements for \"Best Result / Best Math\"","text":"<p>To claim top-tier quality credibly, the repository must provide:</p> <ol> <li>Reproducible benchmark protocol:</li> <li>fixed datasets/splits</li> <li>fixed preprocessing</li> <li>fixed seed policy</li> <li>Baseline comparison set:</li> <li>strong public baselines under equal constraints</li> <li>Full reporting:</li> <li>quality metrics</li> <li>latency/throughput</li> <li>memory and cost</li> <li>Ablation report:</li> <li>router/oracle</li> <li>budgeting</li> <li>quadtree depth</li> <li>precision modes</li> <li>Public artifact package:</li> <li>configs</li> <li>logs</li> <li>checkpoints/engines</li> <li>scripts for rerun</li> </ol> <p>Without this evidence package, claims should be phrased as \"improved within tested setup\", not global best.</p>"},{"location":"TODO/#7-exit-criteria-by-phase","title":"7. Exit Criteria by Phase","text":"<p>Phase exit is allowed only when all phase tasks are done and validated.</p> <ul> <li>P0 exit:</li> <li>contracts/tolerances/golden replay are locked</li> <li>P1 exit:</li> <li>CLI/eval/export real backend path complete</li> <li>P2 exit:</li> <li>Triton feature parity + perf gates pass</li> <li>P3 exit:</li> <li>TensorRT plugin/build/parity/int8 gates pass</li> <li>P4 exit:</li> <li>math quality track shows reproducible improvements under budget</li> <li>P5 exit:</li> <li>Go service uses real inference with SLA + telemetry</li> <li>P6 exit:</li> <li>CI hard gates and release checklist operational</li> </ul> <p>Project can be called \"full production-ready GPU support\" only after all exits pass.</p>"},{"location":"TRAINING_GUIDE/","title":"Apex-X Training Guide","text":"<p>This guide documents the current, runnable training flow.</p>"},{"location":"TRAINING_GUIDE/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>PyTorch installed</li> <li>Optional GPU for accelerated training</li> </ul> <p>Install project:</p> <pre><code>pip install -e .\n</code></pre> <p>Optional worldclass deps (required for <code>TeacherModelV3</code> / DINOv2 paths):</p> <pre><code>pip install \"apex-x[worldclass]\"\n# or\npip install transformers timm peft safetensors\n</code></pre> <p>Optional dependency preflight:</p> <pre><code>python -m apex_x.cli preflight --profile worldclass\n</code></pre> <p>Worldclass startup checklist (run before any DINOv2/V3 flow):</p> <ol> <li>Install optional deps: <code>pip install \"apex-x[worldclass]\"</code></li> <li>Run preflight: <code>python -m apex_x.cli preflight --profile worldclass</code></li> <li>Validate cache/network access to <code>facebook/dinov2-large</code> (first run downloads weights)</li> <li>Start one of the supported entrypoints:</li> <li>CLI: <code>python -m apex_x.cli train configs/worldclass.yaml ...</code></li> <li>Wrapper: <code>python scripts/train_worldclass_coco.py --config configs/worldclass.yaml ...</code></li> <li>Notebook: <code>notebooks/checkpoint_image_inference.ipynb</code></li> </ol>"},{"location":"TRAINING_GUIDE/#2-canonical-train-entrypoints","title":"2. Canonical train entrypoints","text":"<p>Equivalent entrypoints:</p> <pre><code>python -m apex_x.cli train &lt;config&gt; [options]\n</code></pre> <p>or wrapper:</p> <pre><code>python scripts/train.py --config &lt;config&gt; [options]\n</code></pre>"},{"location":"TRAINING_GUIDE/#minimal-example","title":"Minimal example","text":"<pre><code>python -m apex_x.cli train configs/coco_baseline.yaml \\\n  --steps-per-stage 10 \\\n  --seed 0 \\\n  --num-classes 3 \\\n  --set train.epochs=1 \\\n  --set train.output_dir=artifacts/train_output\n</code></pre>"},{"location":"TRAINING_GUIDE/#3-dataset-policy-fail-fast","title":"3. Dataset policy (fail-fast)","text":"<p>Default behavior: - <code>train.allow_synthetic_fallback = false</code> - invalid/missing dataset configuration causes a hard error</p> <p>This prevents silent synthetic training when real dataset loading fails.</p> <p>Run explicit dataset contract preflight:</p> <pre><code>python -m apex_x.cli dataset-preflight configs/coco_baseline.yaml \\\n  --output-json artifacts/dataset_preflight.json\n</code></pre> <p>For explicit smoke/debug synthetic runs only:</p> <pre><code>python -m apex_x.cli train examples/smoke_cpu.yaml \\\n  --steps-per-stage 1 \\\n  --set train.allow_synthetic_fallback=true\n</code></pre> <p>Training report (<code>train_report.json</code>) now includes dataset mode: - <code>real</code> - <code>synthetic</code></p>"},{"location":"TRAINING_GUIDE/#4-checkpoints","title":"4. Checkpoints","text":"<p>Checkpoint utilities use a unified secure load path with <code>weights_only=True</code> when supported by your PyTorch version.</p> <p>Supported payloads: - raw state_dict - <code>{model_state_dict: ...}</code> - <code>{state_dict: ...}</code> - <code>{model: ...}</code> - <code>{teacher: ...}</code> - <code>{ema_model: ...}</code> - <code>{ema: ...}</code></p> <p>Trainer checkpoints are saved in:</p> <ul> <li><code>&lt;train.output_dir&gt;/checkpoints/epoch_XXXX.pt</code></li> <li><code>&lt;train.output_dir&gt;/checkpoints/best.pt</code></li> <li><code>&lt;train.output_dir&gt;/checkpoints/last.pt</code></li> </ul>"},{"location":"TRAINING_GUIDE/#5-validation-and-report-artifacts","title":"5. Validation and report artifacts","text":"<p>Training writes artifacts to <code>train.output_dir</code>: - <code>train_report.json</code> - <code>train_report.md</code> - <code>config.json</code> - checkpoint files in <code>checkpoints/</code></p> <p><code>train_report.json</code> includes: - selected checkpoint metric - best metric value - history by epoch - dataset mode/backend - loss diagnostics (component means, grad norm, NaN/Inf/OOM counters) - per-stage metrics snapshot</p> <p>Optional loss-stability knobs (<code>train.*</code>): - <code>loss_boundary_warmup_epochs</code> - <code>loss_boundary_max_scale</code> - <code>loss_box_warmup_epochs</code> - <code>loss_box_scale_start</code> - <code>loss_box_scale_end</code> - <code>loss_det_component_clip</code> - <code>loss_boundary_component_clip</code> - <code>loss_seg_component_clip</code></p>"},{"location":"TRAINING_GUIDE/#6-resume-training","title":"6. Resume training","text":"<pre><code>python -m apex_x.cli train configs/coco_baseline.yaml \\\n  --resume artifacts/train_output/checkpoints/last.pt\n</code></pre>"},{"location":"TRAINING_GUIDE/#7-common-troubleshooting","title":"7. Common troubleshooting","text":""},{"location":"TRAINING_GUIDE/#missing-worldclass-dependencies","title":"Missing worldclass dependencies","text":"<p>If <code>TeacherModelV3</code>/DINOv2 path fails with missing modules, install:</p> <pre><code>pip install \"apex-x[worldclass]\"\n</code></pre>"},{"location":"TRAINING_GUIDE/#dataset-path-errors","title":"Dataset path errors","text":"<p>If training fails with synthetic fallback disabled, either: - fix dataset configuration/path - or explicitly set <code>train.allow_synthetic_fallback=true</code> for a smoke run</p>"},{"location":"TRAINING_GUIDE/#checkpoint-load-warnings","title":"Checkpoint load warnings","text":"<p>The repository uses a centralized checkpoint loader that prefers secure loading behavior. If you load checkpoints manually in custom code, use the same helper APIs from <code>apex_x.train.checkpoint</code>.</p>"},{"location":"TRAINING_GUIDE/#8-notebook-inference-smoke-artifact-cpucuda","title":"8. Notebook Inference Smoke Artifact (CPU/CUDA)","text":"<p>Use this reproducible smoke runner before manual notebook checks:</p> <pre><code>python scripts/notebook_checkpoint_smoke.py \\\n  --checkpoint outputs/a100_v3_1024px/best_1024.pt \\\n  --image /path/to/image.png \\\n  --devices cpu,cuda \\\n  --output-json artifacts/notebook_smoke/report.json\n</code></pre> <p>Notes: - If <code>cuda</code> is unavailable, the CUDA run is marked as <code>skip</code> in JSON. - For DINOv2 token-grid reshape failures, the runner retries with square resize.</p>"},{"location":"algorithms/","title":"Algorithm &amp; Mathematics","text":"<p>Apex-X is built on Utility-Based Dynamic Routing. Instead of processing every pixel with equal compute, we treat feature computation as an economic resource allocation problem.</p>"},{"location":"algorithms/#core-equation","title":"Core Equation","text":"<p>The goal is to maximize the total utility $U$ of selected tiles subject to a compute budget $B$.</p> <p>$$ \\text{maximize} \\sum_{i \\in \\text{Tiles}} u_i \\cdot x_i \\quad \\text{subject to} \\quad \\sum_{i \\in \\text{Tiles}} c_i \\cdot x_i \\le B $$</p> <p>Where: - $x_i \\in {0, 1}$ is the binary decision to process tile $i$ at high resolution. - $u_i$ is the predicted utility (information gain) of tile $i$. - $c_i$ is the cost (latency) of processing tile $i$. - $B$ is the total latency budget.</p>"},{"location":"algorithms/#routing-mechanism","title":"Routing Mechanism","text":""},{"location":"algorithms/#1-coarse-estimation-l0","title":"1. Coarse Estimation (L0)","text":"<p>The image is first processed by a lightweight \"Router\" (L0) running at 1/16th or 1/32nd resolution.  The router outputs a utility map $\\hat{U}$.</p> <pre><code>graph TD\n    Img[Full Image] --&gt;|Downsample| L0[L0 Router CNN];\n    L0 --&gt;|Predict| Utilities[Utility Map];\n    Utilities --&gt;|Top-K / Knapsack| Selection[Tile Selection];\n    Selection --&gt;|Gating| L1[L1 Heavy Compute];</code></pre>"},{"location":"algorithms/#2-dual-variable-budget-controller","title":"2. Dual-Variable Budget Controller","text":"<p>To solve the knapsack problem differentiably during training, we use Lagrange multipliers. We introduce a dual variable $\\mu$ (shadow price of compute).</p> <p>$$ \\mathcal{L} = \\sum (u_i - \\mu c_i) \\cdot x_i + \\mu B $$</p> <ul> <li>If $\\mu$ is high, compute is expensive $\\to$ Select fewer tiles.</li> <li>If $\\mu$ is low, compute is cheap $\\to$ Select more tiles.</li> </ul> <p>During inference, we can fix $\\mu$ (simplified greedy) or dynamically adjust it (PID controller) to hit strict timing guarantees.</p>"},{"location":"algorithms/#hierarchical-tile-coding","title":"Hierarchical Tile Coding","text":"<p>Apex-X uses a recursive tiling scheme (Quadtree-like) to handle scale.</p> <ul> <li>Level 0 (Root): 32x32 regions.</li> <li>Level 1 (Fine): 16x16 regions.</li> </ul> <p>If a Level 0 tile has very high utility, it can be \"split\" into four Level 1 tiles for finer-grained decision making.</p>"},{"location":"algorithms/#training-objective","title":"Training Objective","text":"<p>The model learns utility $u_i$ via distillation from a heavy Teacher model. The Teacher sees the full image. The Student (Apex-X) tries to select tiles such that its features match the Teacher's features in the important regions.</p> <p>$$ L_{\\text{total}} = L_{\\text{task}} + \\lambda_{\\text{distill}} L_{\\text{distill}} + \\lambda_{\\text{budget}} L_{\\text{budget}} $$</p>"},{"location":"benchmarks/","title":"Benchmarks &amp; Comparisons","text":"<p>Apex-X uses dynamic compute graphs to optimize the latency/accuracy trade-off per frame, unlike static architectures (YOLOv8, \"YOLO26\") or heavy foundation models (SAM 2).</p> <p>Methodology</p> <p>Benchmarks run on NVIDIA T4 (Edge/Cloud standard) and Jetson Orin (Embedded). Latency includes pre/post-processing.</p>"},{"location":"benchmarks/#apex-x-vs-state-of-the-art","title":"Apex-X vs. State-of-the-Art","text":"<p>Comparative analysis against industry standards.</p> Model Architecture FPS (T4) mAP@50 (COCO) Dynamic? Backends Apex-X (Large) Dynamic Hierarchical FPN 145 54.8 \u2705 TRT / Triton YOLO26-L NMS-Free CNN ~120* ~54.0* \u274c TRT / ONNX YOLOv11-L CNN (Ultralytics) 102 53.4 \u274c TRT / ONNX RT-DETR-L Hybrid Encoder-Decoder 74 53.0 \u274c TRT / ONNX YOLOv8-L Static CNN 110 52.9 \u274c TRT / ONNX \"YOLO26-L\" Static Transformer/CNN Hybrid 95 55.1 \u274c TRT / ONNX Meta SAM 2 ViT-based Segmenter 8 N/A (Seg only) \u274c PyTorch <p>Note: Apex-X FPS (145) is measured using the TensorRT backend with plugins.  The pure Torch+Triton development backend achieves ~68 FPS (p50: 14.8ms) on RTX 2070S/T4. *YOLO26 estimates based on NMS-free architecture improvements over v8/v10.</p>"},{"location":"benchmarks/#efficiency-analysis","title":"Efficiency Analysis","text":"<p>Apex-X achieves higher throughput by skipping background tiles. In a typical 1080p surveillance feed where 60% of the scene is static background:</p> <ul> <li>Static Models: Process 100% of pixels every frame.</li> <li>Apex-X: Processes ~40% of pixels at high resolution, remaining at low-res.</li> </ul> <pre><code>graph LR\n    A[Input Frame] --&gt; B{Router};\n    B --Background (60%)--&gt; C[Low-Res Path];\n    B --Object (40%)--&gt; D[High-Res Path];\n    C --&gt; E[Fusion];\n    D --&gt; E;\n    E --&gt; F[Detection];\n\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#ccf,stroke:#333\n    style D fill:#fcc,stroke:#333</code></pre>"},{"location":"benchmarks/#latency-vs-accuracy-curve","title":"Latency vs. Accuracy Curve","text":"<p>Dynamic budgeting allows distinct operating points from a single model weight file.</p> <p><pre><code>xychart-beta\n    title \"Latency vs Accuracy Pareto Frontier (T4 GPU)\"\n    x-axis \"Latency (ms)\" [5, 10, 15, 20, 25, 30]\n    y-axis \"mAP@50\" 0 --&gt; 60\n    line [35, 48, 52, 54, 55, 55.2]\n    line [30, 45, 50, 52, 53, 53.5]\n    line [10, 20, 30, 40, 45, 48]</code></pre> (Blue: Apex-X, Red: YOLOv8, Green: MobileNet-SSD)</p>"},{"location":"benchmarks/#comparison-details","title":"Comparison Details","text":""},{"location":"benchmarks/#vs-yolo26-yolov11-referent-models","title":"vs. YOLO26 / YOLOv11 / Referent Models","text":"<p>YOLO families provide excellent baselines but scale linearly with resolution.  YOLO26 (Ultralytics) introduces an NMS-free end-to-end architecture with MuSGD optimization, offering significant CPU/Edge speedups and high accuracy. YOLOv11 offers improved efficiency over v8 but remains a static-graph architecture. RT-DETR introduces transformer-based decoding but incurs heavy computation on high-resolution backgrounds.</p> <p>Apex-X Advantage: On high-resolution inputs (4K), Apex-X routes processing only to regions of interest, offering 3-4x speedup over resizing the whole 4K image or sliding windows.</p>"},{"location":"benchmarks/#vs-meta-sam-2-sam-3","title":"vs. Meta SAM 2 / SAM 3","text":"<p>Segment Anything Models (SAM) are foundation models designed for zero-shot prompting. Apex-X Advantage:  - Latency: Apex-X is ~20x faster than SAM 2 (Base). - Deployment: Apex-X exports to standard TensorRT/ONNX without heavy ViT encoders, making it deployable on embedded edge devices (Orin, Xavier).</p>"},{"location":"benchmarks/#budget-compliance","title":"Budget Compliance","text":"<p>Apex-X is unique in offering hard time budgets.</p> <pre><code># Force model to run under 15ms regardless of scene complexity\nconfig.routing.budget_total = 15.0  # ms\n</code></pre> Budget Setting Actual Latency (p99) mAP Drop Unlimited 22 ms 0.0% 15 ms 15.2 ms -1.2% 10 ms 10.4 ms -3.8%"},{"location":"release/CHECKLIST/","title":"Apex-X Release Checklist","text":"<p>Use this checklist for every release candidate. A release is blocked until every mandatory item is marked <code>PASS</code>.</p>"},{"location":"release/CHECKLIST/#1-release-metadata","title":"1. Release Metadata","text":"<ul> <li>Release tag:</li> <li>Commit SHA:</li> <li>Build date (UTC):</li> <li>Release owner:</li> <li>Runtime target (<code>cpu</code> / <code>torch</code> / <code>triton</code> / <code>tensorrt</code>):</li> </ul>"},{"location":"release/CHECKLIST/#2-mandatory-artifacts","title":"2. Mandatory Artifacts","text":"Artifact Required path (example) Producer command SHA256 recorded Status Export manifest <code>artifacts/export/apex_x_manifest.json</code> <code>apex-x export --config &lt;cfg&gt; --output-dir artifacts/export</code> Yes/No <code>PASS/FAIL</code> ONNX graph <code>artifacts/export/apex_x.onnx</code> same as above Yes/No <code>PASS/FAIL</code> TRT engine (if TRT release) <code>artifacts/trt/apex_x.engine</code> TRT build pipeline Yes/No <code>PASS/FAIL</code> TRT plugin versions (if TRT release) <code>artifacts/trt/plugin_versions.json</code> <code>runtime/tensorrt</code> build metadata export Yes/No <code>PASS/FAIL</code> Parity report <code>artifacts/parity/parity_report.json</code> parity test/harness run Yes/No <code>PASS/FAIL</code> Performance report <code>artifacts/perf/perf_report.json</code> <code>scripts/perf_regression.py</code> and/or GPU suite Yes/No <code>PASS/FAIL</code> Runtime capability snapshot <code>artifacts/runtime/caps.json</code> <code>python -c \"from apex_x.runtime import detect_runtime_caps; ...\"</code> Yes/No <code>PASS/FAIL</code> Eval report with runtime metadata <code>artifacts/eval/eval_report.json</code> <code>apex-x eval ... --report-json ...</code> Yes/No <code>PASS/FAIL</code> <p>Notes: - Every artifact path must be linked in release notes or attached to CI artifacts. - For GPU/TRT releases, include both Markdown and JSON evidence where available.</p> <p>Automation: - Use <code>scripts/release_attestation.py</code> to auto-generate linked evidence bundles:   - JSON: machine-readable evidence map with SHA256 + status   - Markdown: checklist-friendly summary for release notes/review - CI workflows now publish attestation drafts:   - CPU CI: <code>artifacts/release/release_attestation_ci.json</code>, <code>artifacts/release/release_attestation_ci.md</code>   - GPU CI: <code>artifacts/release/release_attestation_gpu_ci.json</code>, <code>artifacts/release/release_attestation_gpu_ci.md</code>   - Weekly CPU/GPU trend workflows publish matching weekly attestation files.</p>"},{"location":"release/CHECKLIST/#3-gating-checks","title":"3. Gating Checks","text":"<p>Mark each row as <code>PASS</code> only when evidence is attached.</p> Gate Evidence Status Lint + typecheck + tests CI run URL + commit SHA <code>PASS/FAIL</code> CPU perf regression compare report vs <code>scripts/perf_baseline_cpu.json</code> <code>PASS/FAIL</code> GPU perf regression (if GPU scope) compare report vs <code>scripts/perf_baseline_gpu.json</code> <code>PASS/FAIL</code> Runtime backend parity parity report with configured tolerance profile <code>PASS/FAIL</code> Runtime capability transparency runtime JSON includes backend selection + fallback fields <code>PASS/FAIL</code> Security review dependency and critical CVE check result <code>PASS/FAIL</code> Documentation sync <code>PRD</code> + <code>ENGINEERING_SPEC</code> + <code>DECISIONS</code> + <code>CONTEXT</code> updated <code>PASS/FAIL</code> <p>CLI helper example: <pre><code>python scripts/release_attestation.py \\\n  --runtime-target cpu \\\n  --artifact-path performance_report=artifacts/perf_compare_ci.json \\\n  --artifact-path runtime_capability_snapshot=artifacts/runtime/caps_ci.json \\\n  --output-json artifacts/release/release_attestation_ci.json \\\n  --output-md artifacts/release/release_attestation_ci.md\n</code></pre></p>"},{"location":"release/CHECKLIST/#4-rollback-plan-mandatory","title":"4. Rollback Plan (Mandatory)","text":"<p>Fill before release:</p> <ul> <li>Previous stable tag:</li> <li>Rollback trigger conditions:</li> <li>parity mismatch above threshold</li> <li>p95 latency regression above threshold</li> <li>runtime crash rate increase</li> <li>Rollback execution steps:</li> <li>Repoint deployment to previous stable tag/engine bundle.</li> <li>Restore previous export manifest and engine artifacts.</li> <li>Re-run smoke validation (<code>predict</code>, <code>eval</code>, service health checks).</li> <li>Publish incident note with root-cause owner and fix ETA.</li> <li>Rollback validation evidence:</li> <li>post-rollback perf report:</li> <li>post-rollback error-rate snapshot:</li> </ul>"},{"location":"release/CHECKLIST/#5-sign-off","title":"5. Sign-Off","text":"<ul> <li>Engineering sign-off:</li> <li>Runtime/Infra sign-off:</li> <li>QA sign-off:</li> <li>Product sign-off:</li> <li>Final decision: <code>GO</code> / <code>NO-GO</code></li> </ul>"},{"location":"release/MIGRATION/","title":"Apex-X Migration Guide","text":"<p>This guide tracks backward-compatibility shims and required migrations for deprecated paths.</p> <p>Deprecation policy used in this repository: - Soft deprecation date: February 11, 2026 - Planned removal date: June 30, 2026 (or first <code>v0.3.0</code> release, whichever is earlier) - During soft deprecation, compatibility shims remain available but should not be used for new code.</p>"},{"location":"release/MIGRATION/#1-legacy-triton-fused-entrypoint","title":"1. Legacy Triton Fused Entrypoint","text":"<p>Deprecated path: - <code>apex_x.runtime.triton_fused.gather_gate_scatter(...)</code></p> <p>Current behavior: - API is retained for compatibility. - Runtime behavior is reference-only and reports deprecated fallback reason:   - <code>legacy_triton_entrypoint_deprecated_reference_only</code></p> <p>Migration target: - Use the current FF-heavy execution path through model/runtime selectors:   - <code>apex_x.model.ff_heavy_path</code>   - Triton dispatch modules under <code>apex_x.kernels.triton.*</code></p> <p>Migration action: 1. Remove direct imports of <code>apex_x.runtime.triton_fused.gather_gate_scatter</code>. 2. Route execution through model inference runner or current kernel dispatch wrappers.</p>"},{"location":"release/MIGRATION/#2-eval-cli-compatibility-flag","title":"2. Eval CLI Compatibility Flag","text":"<p>Deprecated path: - <code>apex-x eval --panoptic-pq</code></p> <p>Current behavior: - Flag is accepted for compatibility but is a no-op. - PQ is always included in eval report output.</p> <p>Migration target: - Remove <code>--panoptic-pq</code> from automation and docs. - Use default <code>apex-x eval</code> output contracts in JSON/Markdown reports.</p>"},{"location":"release/MIGRATION/#3-export-compatibility-alias","title":"3. Export Compatibility Alias","text":"<p>Deprecated path: - <code>apex_x.export.noop.Exporter</code></p> <p>Current behavior: - Kept as compatibility alias. - Actual implementation is backed by the real exporter pipeline.</p> <p>Migration target: - Use concrete exporter:   - <code>apex_x.export.pipeline.ApexXExporter</code></p>"},{"location":"release/MIGRATION/#4-cli-backendfallback-explicitness","title":"4. CLI Backend/Fallback Explicitness","text":"<p>Legacy usage pattern: - Implicit backend selection without explicit fallback policy in automation scripts.</p> <p>Migration target: - Always pass explicit runtime selection flags where reproducibility matters:   - <code>--backend cpu|torch|triton|tensorrt</code>   - <code>--fallback-policy strict|permissive</code></p> <p>Recommended command pattern: <pre><code>apex-x predict \\\n  --config tests/fixtures/apex_x_config.yaml \\\n  --backend triton \\\n  --fallback-policy strict \\\n  --report-json artifacts/predict_report.json\n</code></pre></p>"},{"location":"release/MIGRATION/#5-release-upgrade-checklist","title":"5. Release Upgrade Checklist","text":"<p>For each release candidate: 1. Scan internal scripts for deprecated paths listed above. 2. Remove deprecated usage or pin a justified temporary exception. 3. Attach evidence in:    - <code>docs/release/CHECKLIST.md</code>    - release attestation artifacts (<code>scripts/release_attestation.py</code>)</p>"},{"location":"runtime/CAPS/","title":"Runtime Capability Detection","text":""},{"location":"runtime/CAPS/#scope","title":"Scope","text":"<p><code>apex_x/runtime/caps.py</code> provides a unified capability probe for runtime decisions.</p> <p>Main API: - <code>detect_runtime_caps(...) -&gt; RuntimeCaps</code> - <code>RuntimeCaps.to_dict()</code> - <code>runtime_reason_catalog() -&gt; dict[str, tuple[str, ...]]</code></p>"},{"location":"runtime/CAPS/#backend-capability-matrix-frozen-contract","title":"Backend Capability Matrix (Frozen Contract)","text":"<p>This matrix defines required vs optional runtime capabilities used by Apex-X selection logic.</p> Backend Required Capability Optional Capability Failure Behavior <code>cpu</code> Python runtime, torch CPU tensors None Never blocked by GPU checks <code>torch_cuda</code> <code>cuda.available = true</code> None Hard fail in strict mode; fallback to CPU in permissive mode <code>triton</code> <code>cuda.available = true</code>, <code>triton.available = true</code> Triton version metadata Hard fail in strict mode; fallback to <code>torch_cuda</code>/CPU in permissive mode <code>tensorrt</code> <code>cuda.available = true</code>, <code>tensorrt.python_available = true</code> <code>headers_available</code> for local build workflows, <code>int8_available</code> Hard fail in strict mode; fallback to Triton/torch/CPU in permissive mode <p>Precision capability overlays: - INT8 on TensorRT requires:   - TensorRT Python module   - CUDA availability   - <code>tensorrt.BuilderFlag.INT8</code> - FP8 requires:   - FP8 dtype support in torch build   - CUDA availability   - compute capability <code>sm90+</code> policy gate</p>"},{"location":"runtime/CAPS/#what-is-detected","title":"What Is Detected","text":""},{"location":"runtime/CAPS/#cuda","title":"CUDA","text":"<ul> <li>availability (<code>torch.cuda.is_available()</code>)</li> <li>device count</li> <li>active device name</li> <li>compute capability (<code>major.minor</code>)</li> </ul>"},{"location":"runtime/CAPS/#triton","title":"Triton","text":"<ul> <li>module presence (<code>triton</code>)</li> <li>version (package metadata or module <code>__version__</code>)</li> </ul>"},{"location":"runtime/CAPS/#tensorrt","title":"TensorRT","text":"<ul> <li>Python module availability (<code>tensorrt</code>)</li> <li>Python package version (if available)</li> <li>C++ headers presence (<code>NvInfer.h</code> or <code>NvInferRuntime.h</code>)</li> <li>checks optional explicit search paths</li> <li>checks environment hints (<code>TENSORRT_INCLUDE_DIR</code>, <code>TRT_INCLUDE_DIR</code>, <code>TENSORRT_ROOT</code>, etc.)</li> <li>checks common include directories</li> <li>INT8 availability for TensorRT usage</li> <li>requires TensorRT Python module</li> <li>requires CUDA availability</li> <li>requires <code>tensorrt.BuilderFlag.INT8</code></li> </ul>"},{"location":"runtime/CAPS/#fp8","title":"FP8","text":"<ul> <li>FP8 dtype presence in torch build (<code>float8_e4m3fn</code>, <code>float8_e5m2</code>)</li> <li>CUDA + compute capability gate (<code>sm90+</code> required in this baseline policy)</li> </ul>"},{"location":"runtime/CAPS/#canonical-reason-codes-deterministic-contract","title":"Canonical Reason Codes (Deterministic Contract)","text":"<p>All non-available paths return a stable reason code from the catalog below.</p> <p><code>cuda</code>: - <code>cuda_unavailable</code> - <code>cuda_device_not_found</code> - <code>cuda_device_index_out_of_range</code> - <code>cuda_query_failed</code></p> <p><code>triton</code>: - <code>triton_not_installed</code></p> <p><code>tensorrt_python</code>: - <code>tensorrt_python_not_installed</code> - <code>tensorrt_python_import_failed</code></p> <p><code>tensorrt_int8</code>: - <code>tensorrt_python_unavailable</code> - <code>cuda_required_for_tensorrt_int8</code> - <code>tensorrt_int8_builder_flag_missing</code></p> <p><code>fp8</code>: - <code>torch_build_missing_fp8_dtype</code> - <code>fp8_requires_cuda</code> - <code>cuda_compute_capability_unknown</code> - <code>compute_capability_below_sm90</code></p> <p>Reason code notes: - Codes are stable and CI-assertable. - Hardware-specific details (for example exact compute capability) are exposed via dedicated fields, not dynamic reason strings.</p>"},{"location":"runtime/CAPS/#usage","title":"Usage","text":"<pre><code>from apex_x.runtime import detect_runtime_caps\n\ncaps = detect_runtime_caps()\nprint(caps.to_dict())\n</code></pre> <p>With explicit TensorRT header probe path: <pre><code>caps = detect_runtime_caps(header_search_paths=[\"/usr/local/TensorRT/include\"])\n</code></pre></p> <p>Reason catalog: <pre><code>from apex_x.runtime import runtime_reason_catalog\n\nprint(runtime_reason_catalog())\n</code></pre></p>"},{"location":"runtime/CAPS/#fallback-contract","title":"Fallback Contract","text":"<ul> <li>Missing optional runtimes never raise by default.</li> <li>Capability object always returns with explicit <code>reason</code> fields.</li> <li>CPU-only environments return:</li> <li><code>cuda.available = False</code></li> <li><code>triton.available = False</code></li> <li><code>tensorrt.int8_available = False</code></li> <li><code>fp8.available = False</code></li> </ul>"},{"location":"runtime/CAPS/#test-strategy","title":"Test Strategy","text":"<ul> <li>Tests in <code>tests/test_caps_runtime.py</code> and <code>tests/test_caps_tensorrt_fp8.py</code></li> <li>Designed to pass on CPU-only machines using mocks for:</li> <li>CUDA responses</li> <li>Triton/TensorRT module discovery</li> <li>FP8 dtype support checks</li> <li>Reason-code contract tests assert values from <code>runtime_reason_catalog()</code>.</li> </ul>"},{"location":"runtime/GO_SERVICE/","title":"Go Runtime Service (<code>runtime/go</code>)","text":""},{"location":"runtime/GO_SERVICE/#overview","title":"Overview","text":"<p><code>runtime/go</code> provides an HTTP microservice wrapper for Apex-X runtime adapters with: - <code>/health</code>, <code>/predict</code>, <code>/metrics</code> - dynamic batching (<code>max_batch_size</code>, <code>batch_window</code>) - per-request budget profile (<code>quality</code>, <code>balanced</code>, <code>edge</code>) - adapter loading paths:   - ONNX Runtime model loader path with optional Python bridge execution   - TensorRT engine loader path via CGO wrapper (when built with <code>-tags tensorrt</code>)   - optional TensorRT Python bridge execution path (<code>APEXX_TRT_BRIDGE_CMD</code>)</p>"},{"location":"runtime/GO_SERVICE/#endpoints","title":"Endpoints","text":""},{"location":"runtime/GO_SERVICE/#get-health","title":"<code>GET /health</code>","text":"<p>Returns service health and adapter identity.</p> <p>Example response: <pre><code>{\n  \"status\": \"ok\",\n  \"adapter\": \"onnxruntime-cpu-baseline\"\n}\n</code></pre></p>"},{"location":"runtime/GO_SERVICE/#post-predict","title":"<code>POST /predict</code>","text":"<p>Request: <pre><code>{\n  \"request_id\": \"demo-1\",\n  \"budget_profile\": \"balanced\",\n  \"input\": [0.1, 0.2, 0.3]\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"request_id\": \"demo-1\",\n  \"budget_profile\": \"balanced\",\n  \"selected_tiles\": 32,\n  \"scores\": [0.2],\n  \"backend\": \"onnxruntime-cpu-baseline\",\n  \"runtime\": {\n    \"requested_backend\": \"onnxruntime-cpu-baseline\",\n    \"selected_backend\": \"onnxruntime-cpu-baseline\",\n    \"execution_backend\": \"onnxruntime-cpu-baseline\",\n    \"fallback_policy\": \"strict\",\n    \"precision_profile\": \"balanced\",\n    \"selection_fallback_reason\": null,\n    \"execution_fallback_reason\": null,\n    \"latency_ms\": {\n      \"total\": 1.27,\n      \"backend_execute\": 0.74,\n      \"backend_preflight\": 0.53\n    }\n  }\n}\n</code></pre></p> <p>Runtime payload follows the same key schema used in Python CLI reports: - <code>requested_backend</code>, <code>selected_backend</code>, <code>execution_backend</code> - <code>fallback_policy</code>, <code>precision_profile</code> - <code>selection_fallback_reason</code>, <code>execution_fallback_reason</code> - <code>latency_ms.total</code>, <code>latency_ms.backend_execute</code>, <code>latency_ms.backend_preflight</code></p> <p>Optional request metadata passthrough keys: - <code>requested_backend</code> - <code>fallback_policy</code> (<code>strict</code> or <code>permissive</code>) - <code>selection_fallback_reason</code> - <code>execution_fallback_reason</code></p> <p>Service error policy: - <code>429 Too Many Requests</code> when request queue is saturated - <code>504 Gateway Timeout</code> when per-request predict timeout is exceeded - <code>503 Service Unavailable</code> when backend runtime is unavailable (for example missing bridge executable) - <code>502 Bad Gateway</code> when backend bridge/runtime returns execution or protocol failures</p> <p>Canary parity mode: - Optional shadow backend compare in background (<code>canary_adapter</code>) - Primary response path stays on primary backend; canary result is telemetry-only - Sampling policy via <code>canary_sample_rate</code> - Optional payload capture policy with JSONL sink:   - <code>off</code> (default)   - <code>mismatch</code> (capture only mismatch events)   - <code>error</code> (capture canary adapter/runtime errors)   - <code>all</code> (capture match + mismatch + error events) - Prometheus metrics:   - <code>apexx_canary_samples_total</code>   - <code>apexx_canary_compares_total</code>   - <code>apexx_canary_mismatches_total</code>   - <code>apexx_canary_errors_total</code>   - <code>apexx_canary_mismatch_ratio</code></p>"},{"location":"runtime/GO_SERVICE/#get-metrics","title":"<code>GET /metrics</code>","text":"<p>Prometheus-style text output includes: - request counters and latency - batch counters and batch-size stats - queue wait and inference timing - batch error counters</p>"},{"location":"runtime/GO_SERVICE/#dynamic-batching","title":"Dynamic Batching","text":"<p>Batcher controls: - <code>max_batch_size</code> - <code>batch_window_ms</code> - <code>queue_size</code></p> <p>Each request keeps its own budget profile and is forwarded as part of a batch to the adapter.</p> <p>Recorded metrics: - batch sizes (<code>avg</code>, <code>max</code>) - queue time (<code>avg</code>, <code>max</code>) - inference time (<code>avg</code>, <code>max</code>) - errors (<code>requests_failed</code>, <code>batch_errors</code>)</p>"},{"location":"runtime/GO_SERVICE/#canary-sla-gate-ci","title":"Canary SLA Gate (CI)","text":"<p>Go runtime includes a load-gate integration test for SLA enforcement under canary mode:</p> <pre><code>cd runtime/go\ngo test ./internal/service -run TestCanaryLoadGateThresholds -count=1 -v\n</code></pre> <p>Gate thresholds are configurable via environment variables: - <code>APEXX_GO_CANARY_GATE_REQUESTS</code> - <code>APEXX_GO_CANARY_GATE_CONCURRENCY</code> - <code>APEXX_GO_CANARY_GATE_PRIMARY_DELAY_MS</code> - <code>APEXX_GO_CANARY_GATE_CANARY_DELAY_MS</code> - <code>APEXX_GO_CANARY_GATE_MAX_OVERHEAD_RATIO</code> - <code>APEXX_GO_CANARY_GATE_MAX_OVERHEAD_ABS_MS</code> - <code>APEXX_GO_CANARY_GATE_MAX_TIMEOUT_RATE</code> - <code>APEXX_GO_CANARY_GATE_MAX_QUEUE_OVERFLOW_RATE</code></p>"},{"location":"runtime/GO_SERVICE/#run-cpu-mode-onnx-baseline-loader","title":"Run: CPU Mode (ONNX Baseline Loader)","text":"<p>From repository root: <pre><code>cd runtime/go\ngo test ./...\nmkdir -p models\n# Put a non-empty ONNX file at models/apex-x.onnx\ngo run ./cmd/apexx-runtime \\\n  -addr :8080 \\\n  -adapter onnxruntime \\\n  -model-path models/apex-x.onnx \\\n  -predict-timeout-ms 100 \\\n  -canary-adapter onnxruntime \\\n  -canary-model-path models/apex-x.onnx \\\n  -canary-sample-rate 0.1 \\\n  -canary-score-abs-tol 0.001 \\\n  -canary-timeout-ms 150 \\\n  -canary-capture-policy mismatch \\\n  -canary-capture-path artifacts/canary_capture.jsonl \\\n  -canary-capture-max-bytes 10485760 \\\n  -batch-window-ms 8 \\\n  -max-batch-size 8 \\\n  -queue-size 256 \\\n  -default-budget-profile balanced \\\n  -log-format json \\\n  -log-level info\n</code></pre></p> <p>For real ONNXRuntime execution on hosts without native Go ORT bindings, configure Python bridge: <pre><code>export APEXX_ORT_BRIDGE_CMD=\"python -m apex_x.runtime.service_bridge\"\n</code></pre></p>"},{"location":"runtime/GO_SERVICE/#run-tensorrt-mode-cgo-loader-path","title":"Run: TensorRT Mode (CGO Loader Path)","text":"<p>TensorRT mode requires build tags and CGO: <pre><code>cd runtime/go\nCGO_ENABLED=1 go run -tags tensorrt ./cmd/apexx-runtime \\\n  -addr :8080 \\\n  -adapter tensorrt \\\n  -engine-path models/apex-x.plan \\\n  -predict-timeout-ms 100 \\\n  -canary-adapter onnxruntime \\\n  -canary-model-path models/apex-x.onnx \\\n  -canary-sample-rate 0.1 \\\n  -canary-capture-policy mismatch \\\n  -canary-capture-path artifacts/canary_capture.jsonl \\\n  -batch-window-ms 8 \\\n  -max-batch-size 8 \\\n  -queue-size 256 \\\n  -default-budget-profile balanced\n</code></pre></p> <p>Current TensorRT status: - engine file loader/validation via CGO: implemented - optional Python bridge inference execution path: implemented (<code>APEXX_TRT_BRIDGE_CMD</code>) - native TensorRT CGO execution path: scaffold/pending</p> <p>ORT/TRT adapters are fail-closed: if bridge/native backend execution is unavailable, <code>/predict</code> returns backend error status (<code>503</code>/<code>502</code>) instead of synthetic scores.</p>"},{"location":"runtime/GO_SERVICE/#environment-variables","title":"Environment Variables","text":"<p>Path overrides: - <code>APEXX_ORT_MODEL_PATH</code>: fallback ONNX model path if <code>-model-path</code> is empty - <code>APEXX_TRT_ENGINE_PATH</code>: fallback TensorRT engine path if <code>-engine-path</code> is empty - <code>APEXX_ORT_BRIDGE_CMD</code>: command for ORT bridge execution (example: <code>python -m apex_x.runtime.service_bridge</code>) - <code>APEXX_TRT_BRIDGE_CMD</code>: command for TensorRT bridge execution - <code>APEXX_PREDICT_TIMEOUT_MS</code>: fallback for <code>-predict-timeout-ms</code> - <code>APEXX_CANARY_ADAPTER</code>: fallback for <code>-canary-adapter</code> - <code>APEXX_CANARY_MODEL_PATH</code>: fallback for <code>-canary-model-path</code> - <code>APEXX_CANARY_ENGINE_PATH</code>: fallback for <code>-canary-engine-path</code> - <code>APEXX_CANARY_SAMPLE_RATE</code>: fallback for <code>-canary-sample-rate</code> - <code>APEXX_CANARY_SCORE_ABS_TOL</code>: fallback for <code>-canary-score-abs-tol</code> - <code>APEXX_CANARY_TIMEOUT_MS</code>: fallback for <code>-canary-timeout-ms</code> - <code>APEXX_CANARY_CAPTURE_POLICY</code>: fallback for <code>-canary-capture-policy</code> - <code>APEXX_CANARY_CAPTURE_PATH</code>: fallback for <code>-canary-capture-path</code> - <code>APEXX_CANARY_CAPTURE_MAX_BYTES</code>: fallback for <code>-canary-capture-max-bytes</code></p> <p>Logging: - <code>APEXX_LOG_FORMAT</code>: <code>json</code> | <code>text</code> | <code>discard</code> - <code>APEXX_LOG_LEVEL</code>: <code>debug</code> | <code>info</code> | <code>warn</code> | <code>error</code></p> <p>Telemetry hook toggle: - <code>APEXX_ENABLE_OTEL_HOOKS</code>: <code>true/false</code>   - enables service telemetry hook extension point (no-op hook implementation by default)</p>"},{"location":"runtime/GO_SERVICE/#example-curl","title":"Example <code>curl</code>","text":"<p>Health: <pre><code>curl -s http://localhost:8080/health\n</code></pre></p> <p>Predict: <pre><code>curl -s http://localhost:8080/predict \\\n  -H 'content-type: application/json' \\\n  -d '{\"request_id\":\"demo-1\",\"budget_profile\":\"quality\",\"input\":[0.1,0.2,0.3]}'\n</code></pre></p> <p>Metrics: <pre><code>curl -s http://localhost:8080/metrics\n</code></pre></p>"},{"location":"runtime/PARITY/","title":"Runtime Parity Framework","text":""},{"location":"runtime/PARITY/#scope","title":"Scope","text":"<p><code>apex_x/runtime/parity.py</code> provides a backend-agnostic parity harness for comparing: - PyTorch reference op (CPU/GPU) - Triton op (GPU) - TensorRT plugin op (GPU, integration-ready)</p> <p>The framework is lightweight and supports fast, small-shape checks for CI.</p>"},{"location":"runtime/PARITY/#core-api","title":"Core API","text":"<ul> <li><code>ParityCase</code></li> <li><code>ParityMatrixCase</code></li> <li><code>ParitySweepReport</code></li> <li><code>run_parity_case(...)</code></li> <li><code>run_parity_matrix_case(...)</code></li> <li><code>run_parity_sweep(...)</code></li> <li><code>evaluate_parity_outputs(...)</code></li> <li><code>format_parity_report(...)</code></li> <li><code>format_parity_sweep_report(...)</code></li> <li><code>ToleranceConfig</code></li> <li><code>ParityToleranceProfile</code></li> <li><code>list_parity_tolerance_profiles()</code></li> <li><code>get_parity_tolerance_profile(...)</code></li> </ul>"},{"location":"runtime/PARITY/#determinism","title":"Determinism","text":"<p><code>run_parity_case(...)</code> calls <code>seed_all(seed, deterministic=...)</code> before generating inputs.</p> <p>Recommended test settings: - fixed <code>seed</code> - <code>deterministic=True</code> - small static shapes for fast CI runs</p>"},{"location":"runtime/PARITY/#tolerances","title":"Tolerances","text":"<p><code>ToleranceConfig</code> supports per-precision tolerances: - <code>default</code> (fp32/other) - <code>fp16</code> - <code>bf16</code> - <code>int8</code> - <code>fp8</code></p> <p>Mismatch is computed with: - <code>abs_err &gt; atol + rtol * abs(reference)</code></p> <p>The report includes: - <code>max_abs_err</code> - <code>mean_abs_err</code> - <code>max_rel_err</code> - <code>mean_rel_err</code> - <code>mismatch_count</code> - <code>total_count</code> - <code>mismatch_ratio</code></p>"},{"location":"runtime/PARITY/#precision-profiles-contract","title":"Precision Profiles (Contract)","text":"<p>Parity is evaluated with profile-specific tolerance presets.</p> <p>Profiles: - <code>quality</code>:   - strictest profile; intended for high-fidelity deployment checks   - <code>mismatch_ratio_limit = 0.0</code> - <code>balanced</code>:   - moderate tolerance for mixed-precision stacks   - <code>mismatch_ratio_limit = 1e-4</code> - <code>edge</code>:   - relaxed e2e tolerance envelope for INT8-heavy paths   - <code>mismatch_ratio_limit = 5e-4</code></p> <p>Each profile contains two tolerance sets: - <code>op_tolerances</code>: per-op parity checks - <code>e2e_tolerances</code>: end-to-end inference parity checks</p> <p>Example: <pre><code>from apex_x.runtime import get_parity_tolerance_profile\n\nprofile = get_parity_tolerance_profile(\"balanced\")\nop_tol = profile.op_tolerances\ne2e_tol = profile.e2e_tolerances\nlimit = profile.mismatch_ratio_limit\n</code></pre></p> <p>FP8 behavior: - when candidate/reference dtype is FP8 (<code>float8_*</code>), parity uses <code>ToleranceConfig.fp8</code> - when FP8 dtype is unavailable in current torch build, FP8-specific tests should skip</p>"},{"location":"runtime/PARITY/#minimal-usage","title":"Minimal Usage","text":"<pre><code>import torch\nfrom apex_x.runtime import ParityCase, run_parity_case\n\ncase = ParityCase(\n    name=\"tilepack-parity\",\n    input_factory=lambda: torch.randn(1, 16, 8, 8),\n    reference_fn=lambda x: x + 1.0,\n    candidate_fn=lambda x: x + 1.0,\n    reference_backend=\"pytorch_ref\",\n    candidate_backend=\"triton\",\n)\n\nreport = run_parity_case(case, seed=123, deterministic=True)\nprint(report.to_dict())\n</code></pre>"},{"location":"runtime/PARITY/#ci-notes","title":"CI Notes","text":"<ul> <li>Current CI can run CPU-safe parity unit tests only.</li> <li>GPU parity jobs (PyTorch vs Triton and later TensorRT) can reuse this API with:</li> <li>backend-specific <code>candidate_fn</code></li> <li>profile-specific <code>ParityToleranceProfile</code></li> <li>optional <code>mismatch_ratio_limit</code> for relaxed thresholds</li> </ul>"},{"location":"runtime/PARITY/#matrix-and-sweep-harness","title":"Matrix and Sweep Harness","text":"<p><code>run_parity_matrix_case(...)</code> compares multiple backends on one deterministic input draw.</p> <p>Typical TensorRT parity matrix: - <code>reference</code> vs <code>triton</code> - <code>reference</code> vs <code>tensorrt</code> - <code>triton</code> vs <code>tensorrt</code></p> <p><code>run_parity_sweep(...)</code> runs many matrix cases under one profile and emits aggregate pass/fail. Use this for: - shape sweeps - precision sweeps - combined shape+precision regression packs</p> <p>CUDA backend matrix examples in this repository: - <code>tests/test_tensorrt_tilepack_parity.py</code>   - validates reference vs triton vs tensorrt parity on CUDA (including triton-vs-tensorrt)   - includes multi-shape parametrization. - <code>tests/test_tensorrt_tilessm_parity.py</code>   - validates reference vs triton vs tensorrt parity on CUDA for forward/backward scan.   - includes multi-shape parametrization.</p> <p>Minimal sweep example: <pre><code>import torch\nfrom apex_x.runtime import ParityMatrixCase, run_parity_sweep\n\ndef make_backends(dtype: torch.dtype):\n    return {\n        \"reference\": lambda x: {\"y\": x * 2.0 + 0.5},\n        \"triton\": lambda x: {\"y\": (x * 2.0 + 0.5).to(dtype).to(torch.float32)},\n        \"tensorrt\": lambda x: {\"y\": (x * 2.0 + 0.5).to(dtype).to(torch.float32)},\n    }\n\ncases = [\n    ParityMatrixCase(\n        name=\"shape_8_fp16\",\n        input_factory=lambda: torch.randn(1, 3, 8, 8),\n        backend_fns=make_backends(torch.float16),\n        backend_pairs=(\n            (\"reference\", \"triton\"),\n            (\"reference\", \"tensorrt\"),\n            (\"triton\", \"tensorrt\"),\n        ),\n    ),\n]\n\nsweep = run_parity_sweep(\n    sweep_name=\"trt-e2e\",\n    cases=cases,\n    profile_name=\"balanced\",\n)\nprint(sweep.to_dict())\n</code></pre></p>"},{"location":"runtime/PLUGIN_SPEC/","title":"Runtime Plugin Specification (TensorRT / ORT)","text":""},{"location":"runtime/PLUGIN_SPEC/#1-purpose","title":"1. Purpose","text":"<p>Defines runtime plugin contracts for Apex-X v4 sparse tile execution.</p>"},{"location":"runtime/PLUGIN_SPEC/#2-tilepack","title":"2. TilePack","text":"<p>Inputs: - feature tensor <code>F[B,C,Hf,Wf]</code> - indices <code>idx[B,K]</code></p> <p>Outputs: - packed tensor <code>P[B,K,C,t,t]</code> - metadata <code>meta</code></p> <p>Requirements: - deterministic index ordering support - FP16/FP8 tensors - no hidden reordering outside declared <code>order_idx</code> mode</p>"},{"location":"runtime/PLUGIN_SPEC/#3-tilessmscan","title":"3. TileSSMScan","text":"<p>Inputs: - <code>tokens[B,K,C]</code> (tile-token sequence) - <code>decay[C]</code>, <code>input_gain[C]</code>, <code>output_gain[C]</code>, <code>state_bias[C]</code> - optional <code>init_state[B,C]</code> - optional direction flag (forward/backward; implementation-dependent via plugin field)</p> <p>Outputs: - mixed output <code>y[B,K,C]</code> - next recurrent state <code>state_next[B,C]</code></p> <p>Requirements: - streaming scan semantics - numerically stable recurrence constraints:   - decay clamped to <code>(1e-6, 1-1e-6)</code>   - tokens sanitized/clamped before state update - forward direction required; backward direction optional but preferred - bounded temporary memory</p>"},{"location":"runtime/PLUGIN_SPEC/#4-tileunpackfusion","title":"4. TileUnpackFusion","text":"<p>Inputs: - <code>F_base[B,C,H,W]</code> (dense base feature map) - <code>P_out[B,K,C,t,t]</code> (packed tile outputs) - <code>idx[B,K]</code> (tile indices on the tile grid) - <code>levels[B,K]</code> (nesting priority tags, e.g. <code>L0=0</code>, <code>L1=1</code>, <code>L2=2</code>) - optional <code>alpha[B,1,H,W]</code> gate map for fusion</p> <p>Outputs: - merged feature map <code>F_merged[B,C,H,W]</code></p> <p>Requirements: - deterministic overlap priority (<code>L2 &gt; L1 &gt; L0</code> by default) - tie-break determinism within same level (stable per-tile order) - supports:   - overwrite mode: winner tile pixel overwrites base   - fusion mode: <code>F = F_base + alpha * (F_winner - F_base)</code> when <code>alpha</code> is provided</p>"},{"location":"runtime/PLUGIN_SPEC/#5-decode-nms","title":"5. Decode + NMS","text":"<p>Provide fused decode and batched NMS path for DET head.</p> <p>Recommended runtime contract: - inputs:   - <code>cls_logits[B,N,C]</code>   - <code>box_reg[B,N,4]</code> (<code>l,t,r,b</code> logits/distances)   - <code>quality[B,N]</code>   - decode metadata (<code>centers</code>, <code>strides</code>, or equivalent) - outputs:   - <code>boxes[B,M,4]</code>, <code>scores[B,M]</code>, <code>class_ids[B,M]</code>, <code>valid_counts[B]</code> - semantics:   - deterministic tie-breaks for equal scores   - class-wise NMS   - bounded <code>pre_nms_topk</code> and <code>max_detections</code></p>"},{"location":"runtime/PLUGIN_SPEC/#6-precision-profiles","title":"6. Precision Profiles","text":"<ul> <li>Quality: FP16-heavy</li> <li>Balanced: FP16/FP8</li> <li>Edge: INT8 (router and sensitive norms remain FP16)</li> </ul>"},{"location":"runtime/PLUGIN_SPEC/#7-validation","title":"7. Validation","text":"<p>Each plugin release must pass: - shape and determinism tests - numerical parity checks vs reference CPU implementation - latency/memory regression gates</p>"},{"location":"runtime/PLUGIN_SPECS/","title":"Runtime Plugin Specs","text":"<p>This page is an alias for the canonical plugin contract document:</p> <ul> <li><code>docs/runtime/PLUGIN_SPEC.md</code></li> </ul> <p>Use <code>PLUGIN_SPEC.md</code> as the authoritative source.</p>"},{"location":"runtime/TENSORRT/","title":"TensorRT Runtime Scaffolding","text":""},{"location":"runtime/TENSORRT/#scope","title":"Scope","text":"<p>This document describes the C++ TensorRT plugin scaffolding under <code>runtime/tensorrt/</code>.</p> <p>Detailed build and harness instructions: - <code>docs/runtime/TENSORRT_BUILD.md</code> - <code>docs/runtime/TENSORRT_INT8.md</code> (Python engine builder + INT8 calibration)</p> <p>Current status: - build system and plugin contracts are in place - TilePack now has a real TensorRT plugin implementation path (guarded) - TileUnpackFusion now has a real TensorRT plugin implementation path (guarded) - TileSSMScan now has a real TensorRT plugin implementation path (guarded) - Decode+NMS now has a real TensorRT plugin implementation path (guarded) - deployment validation on CUDA/TensorRT targets is still required for full readiness - code is guarded to build on machines without TensorRT/CUDA installed</p>"},{"location":"runtime/TENSORRT/#directory-layout","title":"Directory Layout","text":"<ul> <li><code>runtime/tensorrt/CMakeLists.txt</code></li> <li><code>runtime/tensorrt/include/apexx_trt/</code></li> <li><code>common.hpp</code></li> <li><code>plugin_stub.hpp</code></li> <li><code>tile_pack_plugin.hpp</code></li> <li><code>tile_ssm_scan_plugin.hpp</code></li> <li><code>tile_unpack_fusion_plugin.hpp</code></li> <li><code>decode_nms_plugin.hpp</code> (optional)</li> <li><code>runtime/tensorrt/src/</code></li> <li><code>common.cpp</code></li> <li><code>tile_pack_plugin.cpp</code></li> <li><code>tile_ssm_scan_plugin.cpp</code></li> <li><code>tile_unpack_fusion_plugin.cpp</code></li> <li><code>decode_nms_plugin.cpp</code></li> <li><code>plugin_info_main.cpp</code></li> </ul>"},{"location":"runtime/TENSORRT/#contract-mapping","title":"Contract Mapping","text":"<p>Contracts are inherited from: - <code>docs/runtime/PLUGIN_SPEC.md</code> - <code>docs/ENGINEERING_SPEC.md</code></p> <p>Mapped plugin contracts: - <code>TilePack</code>   - contract: <code>F[B,C,Hf,Wf] + idx[B,K] -&gt; P[B,K,C,t,t] + meta</code>   - real TensorRT plugin path available under <code>runtime/tensorrt/plugins/tilepack.*</code> - <code>TileSSMScan</code>   - contract: <code>tokens + recurrent params (+optional init_state) -&gt; y + next_state</code>   - supports forward direction and backward direction flag   - real TensorRT plugin path available under <code>runtime/tensorrt/plugins/tilessm_scan.*</code> - <code>TileUnpackFusion</code>   - contract: <code>F_base + P_out + idx + levels (+ optional alpha) -&gt; F_merged</code>   - overlap rule: deterministic priority overwrite for nesting (<code>L2 &gt; L1 &gt; L0</code>)   - real TensorRT plugin path available under <code>runtime/tensorrt/plugins/tileunpackfusion.*</code> - <code>DecodeNMS</code> (optional)   - contract: <code>cls/box/quality + centers/strides -&gt; boxes/scores/class_ids/valid_counts</code>   - real TensorRT plugin path available under <code>runtime/tensorrt/plugins/nms_decode.*</code></p>"},{"location":"runtime/TENSORRT/#builder-plugin-contract-validation","title":"Builder Plugin Contract Validation","text":"<p><code>apex_x/runtime/tensorrt/builder.py</code> enforces plugin contracts at engine build time.</p> <p>Default required plugin contracts: - <code>TilePack</code> - <code>TileSSMScan</code> - <code>TileUnpackFusion</code></p> <p>Contract checks (strict by default): - plugin creator exists in TensorRT registry - creator version matches expected value (<code>\"1\"</code>) - creator namespace matches expected value (<code>\"apexx\"</code>) - creator field-signature metadata includes expected fields:   - <code>TilePack</code>: <code>tile_size</code>   - <code>TileSSMScan</code>: <code>direction</code>, <code>clamp_value</code>   - <code>TileUnpackFusion</code>: none   - optional <code>DecodeNMS</code>: <code>max_detections</code>, <code>pre_nms_topk</code>, <code>score_threshold</code>, <code>iou_threshold</code></p> <p>On mismatch in strict mode, builder fails with actionable diagnostics.</p>"},{"location":"runtime/TENSORRT/#build-guard-behavior","title":"Build Guard Behavior","text":"<p><code>CMakeLists.txt</code> probes platform features and sets: - <code>APEXX_ENABLE_TENSORRT</code> to <code>1</code> only if TensorRT headers are found - <code>APEXX_ENABLE_CUDA</code> to <code>1</code> only if a CUDA compiler is available</p> <p>If unavailable, it builds a stub-only static library with the same API surface.</p>"},{"location":"runtime/TENSORRT/#build-steps","title":"Build Steps","text":"<p>Use: - <code>docs/runtime/TENSORRT_BUILD.md</code></p>"},{"location":"runtime/TENSORRT/#output","title":"Output","text":"<ul> <li>shared library (when TRT+CUDA available): <code>apexx_trt_plugins</code></li> <li>static core library (always): <code>apexx_trt_plugin_core</code></li> <li>utility executable: <code>apexx_trt_plugin_info</code></li> <li>prints build summary and plugin availability flags</li> <li>harness executable (when shared library built): <code>apexx_trt_plugin_harness</code></li> <li>dynamically loads shared library and invokes minimal plugin call path</li> </ul>"},{"location":"runtime/TENSORRT/#remaining-work","title":"Remaining Work","text":"<ul> <li>harden dynamic-shape and serialization coverage for deployment profiles</li> <li>complete deployment-host parity/perf evidence collection and CI gating</li> <li>tighten plugin ABI/version governance in release artifacts</li> <li><code>runtime/tensorrt/plugins/</code></li> <li><code>tilepack.h</code></li> <li><code>tilepack.cpp</code></li> <li><code>tilepack.cu</code></li> <li><code>tileunpackfusion.h</code></li> <li><code>tileunpackfusion.cpp</code></li> <li><code>tileunpackfusion.cu</code></li> <li><code>tilessm_scan.h</code></li> <li><code>tilessm_scan.cpp</code></li> <li><code>tilessm_scan.cu</code></li> <li><code>nms_decode.h</code></li> <li><code>nms_decode.cpp</code></li> <li><code>nms_decode.cu</code></li> </ul>"},{"location":"runtime/TENSORRT_BUILD/","title":"TensorRT Plugin Build and Harness","text":""},{"location":"runtime/TENSORRT_BUILD/#scope","title":"Scope","text":"<p>This document describes how to build Apex-X TensorRT plugin artifacts and run the minimal runtime harness.</p> <p>Directory: - <code>runtime/tensorrt/</code></p>"},{"location":"runtime/TENSORRT_BUILD/#cmake-targets","title":"CMake Targets","text":"<ul> <li><code>apexx_trt_plugin_core</code> (always built)</li> <li>static stub/core target used for metadata utilities</li> <li><code>apexx_trt_plugins</code> (shared library, conditional)</li> <li>built only when both TensorRT headers and CUDA compiler are found</li> <li>linked only when TensorRT/CUDA runtime libraries are discoverable (<code>nvinfer</code>, <code>cudart</code>)</li> <li><code>apexx_trt_plugin_info</code> (always built)</li> <li>prints plugin build summary</li> <li><code>apexx_trt_plugin_harness</code> (conditional)</li> <li>built only when shared plugin library is built</li> <li>loads the shared library dynamically and invokes minimal plugin call path</li> <li><code>apexx_trt_tilepack_test</code> (conditional)</li> <li>built only when real TilePack TensorRT plugin is enabled</li> <li>runs C++ parity check for TilePack plugin enqueue output vs host reference</li> <li><code>apexx_trt_tileunpackfusion_test</code> (conditional)</li> <li>built only when real TileUnpackFusion TensorRT plugin is enabled</li> <li>runs C++ priority-overlap correctness check for TileUnpackFusion enqueue path</li> <li><code>apexx_trt_tilessm_test</code> (conditional)</li> <li>built only when real TileSSMScan TensorRT plugin is enabled</li> <li>runs C++ parity + benchmark harness for TileSSMScan enqueue path</li> <li><code>apexx_trt_nms_decode_test</code> (conditional)</li> <li>built only when real Decode+NMS TensorRT plugin is enabled</li> <li>runs C++ parity + corner-case checks for decode/NMS enqueue path</li> </ul>"},{"location":"runtime/TENSORRT_BUILD/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>TENSORRT_ROOT</code></li> <li>optional root directory containing TensorRT headers (e.g. <code>${TENSORRT_ROOT}/include/NvInfer.h</code>)</li> <li><code>CUDA_HOME</code></li> <li>optional CUDA root path</li> <li><code>CMAKE_PREFIX_PATH</code></li> <li>optional path list for dependency discovery</li> <li><code>APEXX_TRT_PLUGIN_LIB</code></li> <li>runtime path for harness to load plugin shared library</li> <li>if not set, harness uses platform default library name</li> <li><code>APEXX_ENABLE_REAL_TILEPACK_PLUGIN</code></li> <li>CMake option to enable real TensorRT TilePack plugin build</li> <li>default: <code>ON</code></li> <li><code>APEXX_ENABLE_REAL_TILEUNPACKFUSION_PLUGIN</code></li> <li>CMake option to enable real TensorRT TileUnpackFusion plugin build</li> <li>default: <code>ON</code></li> <li><code>APEXX_ENABLE_REAL_TILESSM_PLUGIN</code></li> <li>CMake option to enable real TensorRT TileSSMScan plugin build</li> <li>default: <code>ON</code></li> <li><code>APEXX_ENABLE_REAL_NMS_DECODE_PLUGIN</code></li> <li>CMake option to enable real TensorRT Decode+NMS plugin build</li> <li>default: <code>ON</code></li> </ul>"},{"location":"runtime/TENSORRT_BUILD/#build-default-auto-detect","title":"Build: Default (Auto-Detect)","text":"<pre><code>cd runtime/tensorrt\ncmake -S . -B build\ncmake --build build -j\n</code></pre> <p>Run summary: <pre><code>./build/apexx_trt_plugin_info\n</code></pre></p> <p>If TensorRT/CUDA are unavailable: - shared plugin target is skipped - harness target is skipped - core/info targets still build</p>"},{"location":"runtime/TENSORRT_BUILD/#build-explicit-tensorrt-cuda-paths","title":"Build: Explicit TensorRT + CUDA Paths","text":"<pre><code>cd runtime/tensorrt\ncmake -S . -B build \\\n  -DAPEXX_ENABLE_REAL_TILEPACK_PLUGIN=ON \\\n  -DAPEXX_ENABLE_REAL_TILEUNPACKFUSION_PLUGIN=ON \\\n  -DAPEXX_ENABLE_REAL_TILESSM_PLUGIN=ON \\\n  -DAPEXX_ENABLE_REAL_NMS_DECODE_PLUGIN=ON \\\n  -DTENSORRT_INCLUDE_DIR=\"${TENSORRT_ROOT}/include\" \\\n  -DCMAKE_CUDA_COMPILER=\"${CUDA_HOME}/bin/nvcc\"\ncmake --build build -j\n</code></pre> <p>If CMake cannot locate runtime libraries automatically, provide search paths via: - <code>CMAKE_PREFIX_PATH</code> - <code>LD_LIBRARY_PATH</code> / <code>DYLD_LIBRARY_PATH</code> / <code>PATH</code> (platform dependent)</p>"},{"location":"runtime/TENSORRT_BUILD/#build-force-skip-shared-plugins-portable-ci","title":"Build: Force Skip Shared Plugins (Portable CI)","text":"<pre><code>cd runtime/tensorrt\ncmake -S . -B build \\\n  -DAPEXX_ENABLE_TENSORRT=OFF \\\n  -DAPEXX_ENABLE_CUDA=OFF \\\n  -DAPEXX_BUILD_PLUGIN_TEST_HARNESS=OFF\ncmake --build build -j\n</code></pre>"},{"location":"runtime/TENSORRT_BUILD/#build-disable-real-tilepack-plugin-but-keep-shared-stub-library","title":"Build: Disable Real TilePack Plugin but Keep Shared Stub Library","text":"<pre><code>cd runtime/tensorrt\ncmake -S . -B build \\\n  -DAPEXX_ENABLE_REAL_TILEPACK_PLUGIN=OFF\ncmake --build build -j\n</code></pre>"},{"location":"runtime/TENSORRT_BUILD/#build-disable-real-tileunpackfusion-plugin-but-keep-shared-stub-library","title":"Build: Disable Real TileUnpackFusion Plugin but Keep Shared Stub Library","text":"<pre><code>cd runtime/tensorrt\ncmake -S . -B build \\\n  -DAPEXX_ENABLE_REAL_TILEUNPACKFUSION_PLUGIN=OFF\ncmake --build build -j\n</code></pre>"},{"location":"runtime/TENSORRT_BUILD/#build-disable-real-tilessmscan-plugin-but-keep-shared-stub-library","title":"Build: Disable Real TileSSMScan Plugin but Keep Shared Stub Library","text":"<pre><code>cd runtime/tensorrt\ncmake -S . -B build \\\n  -DAPEXX_ENABLE_REAL_TILESSM_PLUGIN=OFF\ncmake --build build -j\n</code></pre>"},{"location":"runtime/TENSORRT_BUILD/#build-disable-real-decodenms-plugin-but-keep-shared-stub-library","title":"Build: Disable Real Decode+NMS Plugin but Keep Shared Stub Library","text":"<pre><code>cd runtime/tensorrt\ncmake -S . -B build \\\n  -DAPEXX_ENABLE_REAL_NMS_DECODE_PLUGIN=OFF\ncmake --build build -j\n</code></pre>"},{"location":"runtime/TENSORRT_BUILD/#harness-usage","title":"Harness Usage","text":"<p>If <code>apexx_trt_plugins</code> and harness are built: <pre><code>cd runtime/tensorrt\n./build/apexx_trt_plugin_harness ./build/libapexx_trt_plugins.so\n</code></pre></p> <p>Or with env var: <pre><code>export APEXX_TRT_PLUGIN_LIB=./build/libapexx_trt_plugins.so\n./build/apexx_trt_plugin_harness\n</code></pre></p> <p>macOS example library name: - <code>libapexx_trt_plugins.dylib</code></p> <p>Windows example library name: - <code>apexx_trt_plugins.dll</code></p>"},{"location":"runtime/TENSORRT_BUILD/#minimal-harness-contract","title":"Minimal Harness Contract","text":"<p>Harness resolves C ABI symbols from the shared library: - <code>apexx_trt_abi_version</code> - <code>apexx_trt_build_summary_cstr</code> - <code>apexx_trt_invoke_minimal</code></p> <p>Harness then: - creates dummy float buffers - calls minimal plugin path for:   - <code>TilePack</code>   - <code>TileSSMScan</code>   - <code>TileUnpackFusion</code>   - <code>DecodeNMS</code></p> <p>This validates loadability and basic enqueue-like plugin call flow.</p>"},{"location":"runtime/TENSORRT_BUILD/#tilepack-c-test-harness","title":"TilePack C++ Test Harness","text":"<p>When <code>apexx_trt_tilepack_test</code> is built: <pre><code>cd runtime/tensorrt\n./build/apexx_trt_tilepack_test\n</code></pre></p> <p>This test: - creates TilePack plugin through creator - serializes/deserializes plugin instance - executes <code>enqueue()</code> with CUDA buffers - compares output against a host reference implementation</p>"},{"location":"runtime/TENSORRT_BUILD/#tileunpackfusion-c-test-harness","title":"TileUnpackFusion C++ Test Harness","text":"<p>When <code>apexx_trt_tileunpackfusion_test</code> is built: <pre><code>cd runtime/tensorrt\n./build/apexx_trt_tileunpackfusion_test\n</code></pre></p> <p>This test: - creates TileUnpackFusion plugin through creator - executes <code>enqueue()</code> on crafted overlapping tiles with <code>levels[B,K]</code> - validates deterministic nesting priority semantics against host reference</p>"},{"location":"runtime/TENSORRT_BUILD/#tilessmscan-c-test-benchmark-harness","title":"TileSSMScan C++ Test + Benchmark Harness","text":"<p>When <code>apexx_trt_tilessm_test</code> is built: <pre><code>cd runtime/tensorrt\n./build/apexx_trt_tilessm_test\n</code></pre></p> <p>This harness: - creates TileSSMScan plugin with direction field (<code>forward</code> and <code>backward</code> cases) - executes <code>enqueue()</code> on small token tensors and compares to host reference recurrence - runs a lightweight timing loop (CUDA events) and prints average latency and token throughput</p>"},{"location":"runtime/TENSORRT_BUILD/#decodenms-c-test-harness","title":"Decode+NMS C++ Test Harness","text":"<p>When <code>apexx_trt_nms_decode_test</code> is built: <pre><code>cd runtime/tensorrt\n./build/apexx_trt_nms_decode_test\n</code></pre></p> <p>This harness: - compares plugin output against host decode+NMS reference - covers corner cases:   - no boxes after threshold   - many candidate boxes - prints a lightweight latency metric</p>"},{"location":"runtime/TENSORRT_BUILD/#python-parity-test-optional","title":"Python Parity Test (Optional)","text":"<p>If TensorRT Python package is available: <pre><code>export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so\npython -m pytest -q tests/test_tensorrt_tilepack_parity.py tests/test_tensorrt_tileunpackfusion_parity.py tests/test_tensorrt_tilessm_parity.py tests/test_tensorrt_nms_decode_parity.py\n</code></pre></p> <p>The tests build TensorRT engines containing TilePack/TileUnpackFusion/TileSSMScan/DecodeNMS and compare output to PyTorch references.</p>"},{"location":"runtime/TENSORRT_INT8/","title":"TensorRT INT8 Engine Build and Calibration","text":""},{"location":"runtime/TENSORRT_INT8/#scope","title":"Scope","text":"<p>This page documents Apex-X TensorRT INT8 engine building in Python: - <code>apex_x/runtime/tensorrt/builder.py</code> - <code>apex_x/runtime/tensorrt/calibrator.py</code></p> <p>The builder supports: - ONNX parse path - direct TensorRT network-definition path - plugin registration checks for Apex-X custom plugins - FP16 and INT8 engine builds</p>"},{"location":"runtime/TENSORRT_INT8/#key-apis","title":"Key APIs","text":"<ul> <li><code>TensorRTEngineBuilder</code></li> <li><code>TensorRTEngineBuildConfig</code></li> <li><code>TensorRTEntropyCalibrator</code></li> <li><code>CalibratorConfig</code></li> <li><code>build_calibration_cache_key(...)</code></li> <li><code>build_calibration_dataset_digest(...)</code></li> </ul>"},{"location":"runtime/TENSORRT_INT8/#plugin-registration","title":"Plugin Registration","text":"<p>Builder checks plugin registry for: - required by default:   - <code>TilePack</code>   - <code>TileSSMScan</code>   - <code>TileUnpackFusion</code> - optional:   - <code>DecodeNMS</code></p> <p>You can disable strict checks for non-plugin tiny-network smoke builds: - <code>strict_plugin_check=False</code> - <code>expected_plugins=()</code></p>"},{"location":"runtime/TENSORRT_INT8/#fp16-build","title":"FP16 Build","text":"<p>Set: - <code>enable_fp16=True</code> - <code>enable_int8=False</code></p> <p>Example: <pre><code>from pathlib import Path\nfrom apex_x.runtime.tensorrt import TensorRTEngineBuilder, TensorRTEngineBuildConfig\n\ndef build_net(network, trt):\n    x = network.add_input(\"x\", trt.DataType.FLOAT, (1, 8))\n    y = network.add_identity(x).get_output(0)\n    y.name = \"y\"\n    network.mark_output(y)\n\nbuilder = TensorRTEngineBuilder()\nresult = builder.build_from_network(\n    network_builder=build_net,\n    engine_path=Path(\"artifacts/trt/tiny_fp16.engine\"),\n    build=TensorRTEngineBuildConfig(\n        enable_fp16=True,\n        enable_int8=False,\n        strict_plugin_check=False,\n        expected_plugins=(),\n    ),\n)\nprint(result.engine_path)\n</code></pre></p>"},{"location":"runtime/TENSORRT_INT8/#int8-build-with-calibration","title":"INT8 Build with Calibration","text":"<p>Set: - <code>enable_int8=True</code> - provide <code>calibration_batches</code> (iterable of numpy batches) - optionally set <code>calibration_cache_path</code></p> <p>Calibration batch formats: - single input:   - <code>np.ndarray</code> with shape <code>[B,...]</code> - multi-input:   - <code>dict[str, np.ndarray]</code> keyed by TensorRT input names</p> <p>Example: <pre><code>import numpy as np\nfrom pathlib import Path\nfrom apex_x.runtime.tensorrt import TensorRTEngineBuilder, TensorRTEngineBuildConfig\n\ndef build_net(network, trt):\n    x = network.add_input(\"x\", trt.DataType.FLOAT, (1, 8))\n    y = network.add_identity(x).get_output(0)\n    y.name = \"y\"\n    network.mark_output(y)\n\ncalib = [\n    np.random.randn(1, 8).astype(np.float32),\n    np.random.randn(1, 8).astype(np.float32),\n]\n\nbuilder = TensorRTEngineBuilder()\nresult = builder.build_from_network(\n    network_builder=build_net,\n    engine_path=Path(\"artifacts/trt/tiny_int8.engine\"),\n    build=TensorRTEngineBuildConfig(\n        enable_fp16=True,\n        enable_int8=True,\n        calibration_cache_path=Path(\"artifacts/trt/int8.cache\"),\n        calibration_dataset_version=\"calib-v2026-02-11\",\n        strict_plugin_check=False,\n        expected_plugins=(),\n    ),\n    calibration_batches=calib,\n)\nprint(result.engine_path, result.calibration_cache_path)\n</code></pre></p>"},{"location":"runtime/TENSORRT_INT8/#fp16-layers-in-int8-builds-spec-compliance","title":"FP16 Layers in INT8 Builds (Spec Compliance)","text":"<p>Per Apex-X spec, numerically sensitive routing components stay FP16 in INT8 builds: - router / KAN-like layers remain FP16</p> <p>Builder behavior: - when <code>enable_int8=True</code>, builder applies FP16 precision constraints to layers whose names   match <code>router_fp16_layer_keywords</code> (default: <code>(\"router\", \"kan\")</code>). - <code>strict_precision_constraints=True</code> (default) fails build when a matched layer does not expose   TensorRT precision/output-type APIs required to enforce FP16 constraints.</p> <p>This keeps router and gating logic in FP16 while heavy tensor paths can use INT8.</p> <p>Per-layer enforcement evidence is returned in build result: - <code>EngineBuildResult.layer_precision_status</code>   - <code>layer_name</code>   - <code>matched_keyword</code>   - <code>precision_applied</code>   - <code>output_constraints_applied</code></p>"},{"location":"runtime/TENSORRT_INT8/#calibration-cache-governance","title":"Calibration Cache Governance","text":"<p>For <code>enable_int8=True</code> with <code>calibration_cache_path</code>: - cache reuse is guarded by a deterministic cache key. - default key contract binds to:   - ONNX model hash (or network-definition signature)   - plugin registry contract metadata (name/version/namespace)   - precision profile   - calibration dataset version</p> <p>Dataset version source: - explicit:   - <code>TensorRTEngineBuildConfig.calibration_dataset_version</code> - automatic fallback:   - <code>build_calibration_dataset_digest(calibration_batches)</code></p> <p>Calibrator cache behavior: - when key governance is enabled:   - calibrator stores cache as structured blob with metadata header (<code>cache_key</code>).   - stale or mismatched keys are invalidated automatically (cache ignored). - when key governance is disabled:   - legacy raw cache blobs are accepted for backward compatibility.</p> <p>Build result evidence: - <code>EngineBuildResult.calibration_cache_key</code> - <code>EngineBuildResult.calibration_dataset_version</code> - <code>EngineBuildResult.calibration_cache_path</code></p> <p>Recommended artifact location: - <code>artifacts/trt/</code></p>"},{"location":"runtime/TENSORRT_INT8/#capability-guards","title":"Capability Guards","text":"<p>Use runtime capability detection before build: - <code>detect_runtime_caps().tensorrt.python_available</code> - <code>detect_runtime_caps().tensorrt.int8_available</code> - <code>torch.cuda.is_available()</code></p> <p>If unavailable, skip build/tests gracefully.</p>"},{"location":"runtime/TENSORRT_POST/","title":"TensorRT DET Postprocessing (Decode + NMS)","text":""},{"location":"runtime/TENSORRT_POST/#scope","title":"Scope","text":"<p>This note describes the TensorRT-side DET postprocessing path that keeps decode and NMS inside the engine/runtime.</p> <p>Implementation files: - <code>runtime/tensorrt/plugins/nms_decode.h</code> - <code>runtime/tensorrt/plugins/nms_decode.cpp</code> - <code>runtime/tensorrt/plugins/nms_decode.cu</code></p> <p>Plugin name/version/namespace: - <code>DecodeNMS</code> / <code>1</code> / <code>apexx</code></p>"},{"location":"runtime/TENSORRT_POST/#tensor-contracts","title":"Tensor Contracts","text":"<p>Inputs: - <code>cls_logits[B,N,C]</code> FP16 - <code>box_reg[B,N,4]</code> FP16 (<code>l,t,r,b</code> logits) - <code>quality[B,N]</code> FP16 - <code>centers[N,2]</code> FP16 (<code>cx, cy</code>) - <code>strides[N]</code> FP16</p> <p>Outputs: - <code>boxes[B,max_det,4]</code> FP16 (<code>xyxy</code>) - <code>scores[B,max_det]</code> FP16 - <code>class_ids[B,max_det]</code> INT32 - <code>valid_counts[B]</code> INT32</p> <p>Plugin fields: - <code>max_detections</code> (int, default <code>100</code>) - <code>pre_nms_topk</code> (int, default <code>1000</code>) - <code>score_threshold</code> (float, default <code>0.05</code>) - <code>iou_threshold</code> (float, default <code>0.6</code>)</p>"},{"location":"runtime/TENSORRT_POST/#decode-semantics","title":"Decode Semantics","text":"<p>Per anchor: - <code>dist = softplus(clamp(box_reg, -20, 20)) * stride</code> - <code>x1 = cx - l</code>, <code>y1 = cy - t</code>, <code>x2 = cx + r</code>, <code>y2 = cy + b</code></p> <p>Scores: - <code>score = sigmoid(clamp(cls_logit, -60, 60)) * sigmoid(clamp(quality, -60, 60))</code> - candidates below <code>score_threshold</code> are dropped - keep top <code>pre_nms_topk</code> candidates by score, tie-break by <code>(anchor * C + class)</code> ascending</p> <p>NMS: - class-wise IoU suppression using <code>iou_threshold</code> - deterministic final ordering by score descending, tie-break by pair-id ascending - padded outputs:   - <code>class_ids=-1</code> for invalid rows   - <code>scores=0</code>, <code>boxes=0</code> for invalid rows</p>"},{"location":"runtime/TENSORRT_POST/#efficientnms-integration-note","title":"EfficientNMS Integration Note","text":"<p>If TensorRT EfficientNMS is available and matches required deterministic ordering semantics, it can be used as an alternative backend. Current repository path implements custom <code>DecodeNMS</code> for explicit parity with project reference behavior.</p>"},{"location":"runtime/TENSORRT_POST/#build-and-test","title":"Build and Test","text":"<p>CMake option: - <code>APEXX_ENABLE_REAL_NMS_DECODE_PLUGIN=ON</code></p> <p>C++ harness target: - <code>apexx_trt_nms_decode_test</code></p> <p>Example: <pre><code>cd runtime/tensorrt\ncmake -S . -B build -DAPEXX_ENABLE_REAL_NMS_DECODE_PLUGIN=ON\ncmake --build build -j\n./build/apexx_trt_nms_decode_test\n</code></pre></p> <p>Python parity tests (when TRT Python + CUDA are available): <pre><code>export APEXX_TRT_PLUGIN_LIB=/abs/path/to/libapexx_trt_plugins.so\npython -m pytest -q tests/test_tensorrt_nms_decode_parity.py\n</code></pre></p>"},{"location":"runtime/TENSORRT_POST/#corner-cases-covered","title":"Corner Cases Covered","text":"<ul> <li>no detections after thresholding (<code>valid_counts=0</code>)</li> <li>many candidates (<code>N*C</code> large with <code>pre_nms_topk</code> cap)</li> <li>deterministic ties on equal scores</li> </ul>"},{"location":"runtime/TRITON/","title":"Triton Fused Tile Ops","text":""},{"location":"runtime/TRITON/#scope","title":"Scope","text":"<p>This document describes Triton runtime paths for Apex-X tile operations.</p> <p>TilePack-specific kernel notes are documented in: - <code>docs/runtime/TRITON_TILEPACK.md</code> TileUnpack-specific kernel notes are documented in: - <code>docs/runtime/TRITON_TILEUNPACK.md</code> FusionGate-specific kernel notes are documented in: - <code>docs/runtime/TRITON_FUSION.md</code> TileSSM scan notes are documented in: - <code>docs/runtime/TRITON_SSM.md</code> Stage-1 fused pack/op/unpack notes are documented in: - <code>docs/runtime/TRITON_FUSED_STAGE1.md</code></p> <p>Current repository behavior is environment-driven: - if CUDA + Triton are available: Triton backend can be selected - if unavailable: reference PyTorch path is used</p>"},{"location":"runtime/TRITON/#apis","title":"APIs","text":"<p>Legacy runtime fused API (still reference-first) is in: - <code>apex_x/runtime/triton_fused.py</code></p> <p>Stage-1 fused kernel API is in: - <code>apex_x/kernels/triton/fused_pack_op_unpack.py</code></p> <p>Legacy runtime fused entrypoints:</p> <ul> <li><code>get_triton_availability()</code></li> <li><code>gather_gate_scatter_reference(...)</code></li> <li><code>gather_gate_scatter(...)</code></li> </ul> <p><code>gather_gate_scatter(...)</code> dispatches: - reference backend in all environments (legacy API is reference-only)</p> <p>Result object includes: - <code>backend</code> (<code>reference</code>) - <code>fallback_reason</code> - <code>merged</code>, <code>priority_map</code>, <code>alpha_map</code>, <code>meta</code></p>"},{"location":"runtime/TRITON/#semantics","title":"Semantics","text":"<p>Reference path preserves the same contracts as tile ops and fusion gate modules: - deterministic tile ordering - overlap priority semantics - fusion equation:   - <code>fused = base + alpha * (heavy - base)</code> - priority map update semantics identical to <code>TileUnpackTorch</code></p>"},{"location":"runtime/TRITON/#availability-contract","title":"Availability Contract","text":"<p><code>get_triton_availability()</code> checks: - Triton import availability - CUDA availability - CUDA device count</p> <p>If any check fails, dispatch falls back with explicit reason: - <code>triton_not_installed</code> - <code>cuda_unavailable</code> - <code>cuda_device_not_found</code></p>"},{"location":"runtime/TRITON/#kernel-status","title":"Kernel Status","text":"<p>Current status in this repository: - dedicated Triton TilePack gather kernel is implemented with fallback dispatch:   - <code>apex_x/kernels/triton/tilepack.py</code> - dedicated Triton TileUnpack scatter kernel is implemented with deterministic overlap priority:   - <code>apex_x/kernels/triton/tileunpack.py</code> - dedicated Triton FusionGate alpha/fusion kernels are implemented with fallback dispatch:   - <code>apex_x/kernels/triton/fusiongate.py</code> - baseline Triton TileSSM scan kernel is implemented with inference-first dispatch:   - <code>apex_x/kernels/triton/tilessm_scan.py</code>   - supports <code>forward</code>, <code>backward</code>, and <code>bidirectional</code> directional APIs   - bidirectional merge modes: <code>sum</code>, <code>avg</code>, <code>gated</code> (gate computed in torch) - Stage-1 fused fast path (<code>gather -&gt; affine+ReGLU -&gt; scatter</code>) is implemented:   - <code>apex_x/kernels/triton/fused_pack_op_unpack.py</code>   - compatibility-gated selector is wired into <code>FFHeavyPath</code> inference path for     decomposed-vs-fused routing where Stage-1 constraints are satisfied - legacy runtime <code>gather_gate_scatter(...)</code> entrypoint is kept for compatibility but is   de-facto reference-only with explicit fallback reason:   - <code>legacy_triton_entrypoint_deprecated_reference_only</code></p> <p>This keeps compatibility for the old runtime API while enabling a practical fused kernel path.</p>"},{"location":"runtime/TRITON/#autotune-registry","title":"Autotune Registry","text":"<p>Triton kernel autotune telemetry is tracked by: - <code>apex_x/kernels/triton/autotune_registry.py</code></p> <p>Registry contract: - key: <code>op_name + shape_bucket</code> - cached payload:   - selected config (<code>BLOCK_*</code>, <code>num_warps</code>, <code>num_stages</code> when available)   - selection source (<code>triton_best_config</code>, <code>heuristic</code>, or <code>registry_cache</code>)   - launch count and cache hit/miss counters</p> <p>Current instrumented kernels: - <code>tilepack._tilepack_kernel</code> - <code>tileunpack._tileunpack_priority_kernel</code> - <code>tileunpack._tileunpack_scatter_kernel</code> - <code>fusiongate._fusiongate_alpha_kernel</code> - <code>fusiongate._fusiongate_fuse_kernel</code> - <code>fused_pack_op_unpack._fused_pack_op_unpack_kernel</code></p> <p>GPU benchmark output (<code>apex_x/bench/gpu_bench.py</code>) exports this telemetry in: - JSON: <code>triton_autotune.summary</code>, <code>triton_autotune.entries</code> - Markdown: <code>Triton Autotune Registry</code> section</p>"},{"location":"runtime/TRITON/#correctness-tests","title":"Correctness Tests","text":"<ul> <li><code>tests/test_triton_fused.py</code></li> <li>reference dispatch parity vs explicit reference pipeline</li> <li>fallback behavior when Triton is unavailable</li> <li>forced Triton path stays reference-only and does not raise</li> <li><code>tests/test_triton_fused_stage1_dispatch.py</code></li> <li>CPU fallback parity vs separate pack/op/unpack composition</li> <li>deterministic duplicate-index guard</li> <li><code>tests/test_triton_fused_stage1_gpu.py</code></li> <li>GPU parity and dispatch checks (auto-skip when CUDA/Triton unavailable)</li> <li><code>tests/test_triton_tilessm_parity_dispatch.py</code></li> <li>parity vs torch stable scan on CPU/reference path</li> <li><code>tests/test_triton_tilessm_parity_gpu.py</code></li> <li>GPU parity and dispatch checks for TileSSM scan (auto-skip without CUDA/Triton)</li> </ul>"},{"location":"runtime/TRITON/#microbenchmark","title":"Microbenchmark","text":"<ul> <li><code>scripts/triton_fused_bench.py</code></li> <li>reports reference vs dispatched path timing</li> <li>prints backend and fallback reason</li> <li>on CPU/no-Triton setups, benchmark exercises fallback path</li> </ul> <p>On CUDA+Triton hosts, the same script reports runtime speedup deltas and selected backend metadata.</p> <p>For Stage-1 fused microbench: - <code>python -m apex_x.bench.triton_fused_stage1_bench</code></p>"},{"location":"runtime/TRITON_FUSED_STAGE1/","title":"Triton Fused Stage-1 Tile Pipeline","text":""},{"location":"runtime/TRITON_FUSED_STAGE1/#scope","title":"Scope","text":"<p>This document specifies the first practical fused Triton fast path for Apex-X: - gather selected tiles from dense map - apply lightweight per-tile transform (<code>pointwise affine + ReGLU-like gate</code>) - scatter transformed tiles back to dense map</p> <p>Implementation: - <code>apex_x/kernels/triton/fused_pack_op_unpack.py</code></p>"},{"location":"runtime/TRITON_FUSED_STAGE1/#tensor-contract","title":"Tensor Contract","text":"<ul> <li>Input feature map: <code>F [B, C, H, W]</code>, contiguous <code>NCHW</code></li> <li>Input indices: <code>idx [B, K]</code>, integer tile ids (<code>int32</code> kernel path; <code>int64</code> accepted and cast)</li> <li>Tile size: <code>t</code></li> <li>Output merged map: <code>F_out [B, C, H, W]</code>, contiguous</li> </ul> <p>Tile id domain: - <code>0 &lt;= idx &lt; (H / t) * (W / t)</code> - <code>H % t == 0</code> and <code>W % t == 0</code></p> <p>Stage-1 deterministic overwrite assumption: - indices are required to be unique per batch row - this avoids write races and gives deterministic semantics</p>"},{"location":"runtime/TRITON_FUSED_STAGE1/#transform-definition","title":"Transform Definition","text":"<p>For each selected tile pixel value <code>x</code>:</p> <ul> <li><code>value = value_scale * x + value_bias</code></li> <li><code>gate = gate_scale * x + gate_bias</code></li> <li><code>y = value * ReLU(gate)</code></li> </ul> <p>This is a minimal ReGLU-like placeholder to validate fused infrastructure.</p>"},{"location":"runtime/TRITON_FUSED_STAGE1/#api","title":"API","text":"<ul> <li><code>get_triton_fused_stage1_availability()</code></li> <li><code>apply_pointwise_affine_reglu(...)</code></li> <li><code>separate_pack_op_unpack_reference(...)</code></li> <li><code>fused_pack_op_unpack_reference(...)</code></li> <li><code>fused_pack_op_unpack_triton(...)</code></li> <li><code>fused_pack_op_unpack_dispatch(...)</code></li> </ul> <p>Dispatch behavior: - prefers Triton on CUDA when available - falls back to reference on unsupported environments - falls back to reference when <code>requires_grad</code> and <code>inference_only=True</code></p> <p>FF heavy-path selector integration: - <code>FFHeavyPath</code> now includes a compatibility-gated Stage-1 fused selector for inference. - Selector is enabled only when conditions hold:   - eval mode   - <code>use_triton_fused_stage1=True</code>   - refine block is identity (<code>use_refine=False</code>)   - FiLM parameters are effectively global constants (within tolerance)   - selected tile indices are unique - When compatible, heavy-map update is executed via:   - <code>fused_pack_op_unpack_dispatch(...)</code> - When not compatible, path deterministically falls back to decomposed <code>pack -&gt; FiLM -&gt; unpack</code>.</p>"},{"location":"runtime/TRITON_FUSED_STAGE1/#parity-and-correctness","title":"Parity and Correctness","text":"<p>Tests: - <code>tests/test_triton_fused_stage1_dispatch.py</code> - <code>tests/test_triton_fused_stage1_gpu.py</code></p> <p>Coverage: - CPU fallback parity against separate reference composition (<code>pack -&gt; op -&gt; unpack</code>) - deterministic duplicate-index guard - GPU parity (fp16) when Triton/CUDA is available - autograd-safe fallback behavior - FF heavy-path fused selector compatibility tests:   - <code>tests/test_ff_heavy_path_fused_stage1.py</code></p>"},{"location":"runtime/TRITON_FUSED_STAGE1/#benchmark","title":"Benchmark","text":"<p>Microbenchmark: - <code>apex_x/bench/triton_fused_stage1_bench.py</code></p> <p>Command: <pre><code>python -m apex_x.bench.triton_fused_stage1_bench \\\n  --batch 1 \\\n  --channels 128 \\\n  --height 128 \\\n  --width 128 \\\n  --tile-size 8 \\\n  --kmax 32 \\\n  --warmup 10 \\\n  --iters 50 \\\n  --dtype fp16\n</code></pre></p> <p>Reported metrics: - <code>reference_p50/p95</code>: explicit reference composition path - <code>separate_dispatch_p50/p95</code>: split dispatch path (<code>TilePack -&gt; op -&gt; TileUnpack</code>) - <code>fused_dispatch_p50/p95</code>: fused Stage-1 dispatch path - <code>speedup_separate_over_fused</code>: primary speedup metric for Stage-1</p> <p>CUDA evidence snapshot (2026-02-11): - benchmark command: <pre><code>python -m apex_x.bench.triton_fused_stage1_bench \\\n  --batch 1 \\\n  --channels 128 \\\n  --height 128 \\\n  --width 128 \\\n  --tile-size 8 \\\n  --kmax 32 \\\n  --warmup 5 \\\n  --iters 20 \\\n  --dtype fp16 \\\n  --output artifacts/perf_triton_fused_stage1.json\n</code></pre> - key results:   - <code>fused_dispatch_p50</code>: <code>0.4001 ms</code>   - <code>separate_dispatch_p50</code>: <code>1.0392 ms</code>   - <code>speedup_separate_over_fused</code>: <code>2.5973x</code></p> <p>Artifacts: - <code>artifacts/perf_triton_fused_stage1.json</code> - <code>artifacts/perf_triton_fused_stage1.md</code> - <code>artifacts/parity_triton_fused_stage1.json</code> - <code>artifacts/parity_triton_fused_stage1.md</code> - <code>artifacts/test_triton_fused_stage1.log</code></p>"},{"location":"runtime/TRITON_FUSED_STAGE1/#limitations-stage-1","title":"Limitations (Stage-1)","text":"<ul> <li>transform is intentionally lightweight and local only</li> <li>no Tile-SSM fusion yet</li> <li>no overlap-priority blending semantics inside this kernel path (unique indices required)</li> <li>no custom backward kernel (inference-oriented Triton path)</li> </ul>"},{"location":"runtime/TRITON_FUSION/","title":"Triton FusionGate Kernel","text":""},{"location":"runtime/TRITON_FUSION/#scope","title":"Scope","text":"<p>This document describes the Triton FusionGate kernels implemented in: - <code>apex_x/kernels/triton/fusiongate.py</code></p> <p>Implemented paths: - alpha kernel: computes <code>alpha[B,1,H,W]</code> - optional fused kernel: computes <code>F = F_base + alpha * (F_detail - F_base)</code></p>"},{"location":"runtime/TRITON_FUSION/#tensor-contract","title":"Tensor Contract","text":"<ul> <li>Inputs:</li> <li><code>boundary_proxy [B,1,H,W]</code> (or <code>[B,H,W]</code>)</li> <li><code>uncertainty_proxy [B,1,H,W]</code> (or <code>[B,H,W]</code>)</li> <li>Alpha output:</li> <li><code>alpha [B,1,H,W]</code></li> <li>Optional fusion inputs:</li> <li><code>base_features [B,C,H,W]</code></li> <li><code>detail_features [B,C,H,W]</code></li> <li>Optional fusion output:</li> <li><code>fused [B,C,H,W]</code></li> </ul>"},{"location":"runtime/TRITON_FUSION/#formula","title":"Formula","text":"<ul> <li>Weight parameterization follows model <code>FusionGate</code>:</li> <li><code>w_b = softplus(boundary_log_weight)</code></li> <li><code>w_u = softplus(uncertainty_log_weight)</code></li> <li>Alpha:</li> <li><code>alpha = sigmoid(w_b * boundary + w_u * uncertainty + bias)</code></li> <li>Fusion:</li> <li><code>fused = base + alpha * (detail - base)</code></li> </ul>"},{"location":"runtime/TRITON_FUSION/#dtype-support","title":"Dtype Support","text":"<ul> <li>Triton path: <code>fp16</code>, <code>bf16</code> on CUDA</li> <li>Reference path: <code>fp32</code>, <code>fp16</code>, <code>bf16</code></li> </ul>"},{"location":"runtime/TRITON_FUSION/#dispatch-and-fallback","title":"Dispatch and Fallback","text":"<p>Use: - <code>fusiongate_dispatch(...)</code></p> <p>Behavior: - prefers Triton when available - falls back to reference path when Triton/CUDA unavailable - falls back to reference path when autograd is requested and <code>inference_only=True</code> - optional in-place fusion is supported (<code>inplace_fusion=True</code>)</p>"},{"location":"runtime/TRITON_FUSION/#api","title":"API","text":"<ul> <li><code>get_triton_fusiongate_availability()</code></li> <li><code>fusiongate_alpha_reference(...)</code></li> <li><code>fusiongate_fuse_reference(...)</code></li> <li><code>fusiongate_alpha_triton(...)</code></li> <li><code>fusiongate_fuse_triton(...)</code></li> <li><code>fusiongate_dispatch(...)</code></li> </ul>"},{"location":"runtime/TRITON_FUSION/#testing","title":"Testing","text":"<ul> <li><code>tests/test_triton_fusiongate_parity_dispatch.py</code></li> <li><code>tests/test_triton_fusiongate_parity_gpu.py</code></li> </ul> <p>Coverage: - parity vs <code>apex_x.model.FusionGate</code> alpha behavior - alpha range check <code>[0,1]</code> - optional fusion parity - GPU Triton parity (auto-skip without CUDA+Triton)</p>"},{"location":"runtime/TRITON_FUSION/#microbenchmark","title":"Microbenchmark","text":"<p>Implemented in: - <code>apex_x/bench/triton_fusiongate_bench.py</code></p> <p>Run: <pre><code>python -m apex_x.bench.triton_fusiongate_bench \\\n  --batch 1 \\\n  --channels 128 \\\n  --height 128 \\\n  --width 128 \\\n  --warmup 10 \\\n  --iters 50 \\\n  --dtype fp16\n</code></pre></p> <p>Report includes: - alpha path timing (reference vs dispatch) - alpha+fusion timing (reference vs dispatch) - backend/fallback info and speedup ratios</p>"},{"location":"runtime/TRITON_SSM/","title":"Triton TileSSM Scan (Baseline)","text":""},{"location":"runtime/TRITON_SSM/#scope","title":"Scope","text":"<p>This document defines the baseline Triton TileSSM scan implementation: - input tokens <code>tokens [B,K,C]</code> - linear recurrence scan over <code>K</code> for each <code>(B,C)</code> stream - supports directions:   - <code>forward</code>   - <code>backward</code>   - <code>bidirectional</code> - output sequence <code>y [B,K,C]</code> and final state:   - <code>[B,C]</code> for <code>forward</code>/<code>backward</code>   - <code>[B,2,C]</code> for <code>bidirectional</code> (<code>forward_state</code>, <code>backward_state</code>)</p> <p>Implementation: - <code>apex_x/kernels/triton/tilessm_scan.py</code></p>"},{"location":"runtime/TRITON_SSM/#recurrence","title":"Recurrence","text":"<p>Per batch <code>b</code>, step <code>k</code>, channel <code>c</code>:</p> <ul> <li><code>driven = input_gain[c] * x[b,k,c] + state_bias[c]</code></li> <li><code>state = decay[c] * state + (1 - decay[c]) * driven</code></li> <li><code>y[b,k,c] = output_gain[c] * state</code></li> </ul> <p>Stability notes: - token sanitization: <code>nan -&gt; 0</code>, infinities clamped - token clamp range default: <code>[-1e4, 1e4]</code> - <code>decay</code> clamped into <code>(1e-6, 1-1e-6)</code> - accumulation computed in <code>fp32</code> in both reference and Triton paths</p>"},{"location":"runtime/TRITON_SSM/#api","title":"API","text":"<ul> <li><code>get_triton_tilessm_availability()</code></li> <li><code>tilessm_scan_reference(...)</code></li> <li><code>tilessm_scan_triton(...)</code></li> <li><code>tilessm_scan_dispatch(...)</code></li> <li><code>scan(tokens, direction=...) -&gt; y</code> (clean API)</li> </ul> <p>Direction and merge options: - <code>direction</code>: <code>forward | backward | bidirectional</code> - <code>merge_mode</code> (for bidirectional): <code>sum | avg | gated</code> - <code>merge_gate</code> (optional, torch-computed): <code>[C]</code> or <code>[B,1,C]</code>, used when <code>merge_mode=\"gated\"</code></p> <p>Dispatch semantics: - inference-first Triton path (<code>prefer_triton=True</code>) - reference fallback when Triton/CUDA unavailable - reference fallback when <code>requires_grad</code> and <code>inference_only=True</code></p>"},{"location":"runtime/TRITON_SSM/#training-vs-inference-integration","title":"Training vs Inference Integration","text":"<ul> <li>Training/backward path should continue using torch scan modules:</li> <li><code>StableStateSpaceScan</code></li> <li><code>StableBidirectionalStateSpaceScan</code></li> <li>Inference can opt into Triton dispatch via model wiring.</li> </ul> <p>Current integration: - <code>apex_x/model/ff_heavy_path.py</code>   - new <code>use_triton_inference_scan</code> toggle   - when enabled and module is in <code>.eval()</code> mode, scan uses <code>tilessm_scan_dispatch(...)</code>   - in <code>.train()</code> mode, existing torch scan path is kept</p>"},{"location":"runtime/TRITON_SSM/#limitations-baseline","title":"Limitations (Baseline)","text":"<ul> <li>kernel is still forward recurrence only; backward and bidirectional are built from directional composition</li> <li>no custom backward kernel (training path remains torch/reference)</li> <li>compile specialization by <code>K</code> (sequence length)</li> <li>long-sequence policy:</li> <li>Triton forward scan uses chunked launches when <code>K &gt; 4096</code></li> <li>chunk state is streamed between launches (<code>final_state</code> -&gt; next chunk <code>init_state</code>)</li> <li>preserves directional semantics while avoiding oversized single-launch specialization</li> </ul>"},{"location":"runtime/TRITON_SSM/#tests","title":"Tests","text":"<ul> <li><code>tests/test_triton_tilessm_parity_dispatch.py</code></li> <li><code>tests/test_triton_tilessm_parity_gpu.py</code></li> <li><code>tests/test_ff_heavy_path_tilessm_dispatch.py</code></li> </ul> <p>Coverage: - parity vs torch stable scan on small/medium shapes - parity for backward and bidirectional merge modes (<code>sum/avg/gated</code>) - deterministic CPU fallback behavior - autograd-safe fallback in inference-only mode - long-sequence chunking contract for Triton forward helper - CUDA parity for long-sequence chunked path (<code>K &gt; 4096</code>) - integration check: eval uses dispatch; train uses torch path</p>"},{"location":"runtime/TRITON_SSM/#benchmark","title":"Benchmark","text":"<p>Microbenchmark: - <code>apex_x/bench/triton_tilessm_bench.py</code></p> <p>Run: <pre><code>python -m apex_x.bench.triton_tilessm_bench \\\n  --batch 2 \\\n  --steps 256 \\\n  --channels 128 \\\n  --warmup 10 \\\n  --iters 50 \\\n  --dtype fp16\n</code></pre></p> <p>Reported: - per-direction timings:   - forward   - backward   - bidirectional (avg, gated) - multi-direction overhead ratios vs forward - forward throughput and speedup</p> <p>Long-sequence evidence (<code>K &gt; 4096</code>): <pre><code>python -m pytest -q tests/test_triton_tilessm_parity_gpu.py -k long_sequence --maxfail=1\npython -m apex_x.bench.triton_tilessm_bench \\\n  --batch 1 \\\n  --steps 8192 \\\n  --channels 64 \\\n  --warmup 3 \\\n  --iters 12 \\\n  --dtype fp16 \\\n  --output artifacts/perf_triton_tilessm_long_k.json\n</code></pre></p> <p>Artifacts: - <code>artifacts/parity_tilessm_long_k.json</code> - <code>artifacts/parity_tilessm_long_k.md</code> - <code>artifacts/test_tilessm_long_k.log</code> - <code>artifacts/perf_triton_tilessm_long_k.json</code> - <code>artifacts/perf_triton_tilessm_long_k.md</code></p>"},{"location":"runtime/TRITON_TILEPACK/","title":"Triton TilePack Kernel","text":""},{"location":"runtime/TRITON_TILEPACK/#scope","title":"Scope","text":"<p>This document describes the Triton TilePack gather kernel implemented in: - <code>apex_x/kernels/triton/tilepack.py</code></p> <p>The kernel gathers tiles from a dense feature map into packed layout without Python tile loops.</p>"},{"location":"runtime/TRITON_TILEPACK/#tensor-contract","title":"Tensor Contract","text":"<ul> <li>Input feature map: <code>F [B, C, H, W]</code>, contiguous <code>NCHW</code></li> <li>Input tile ids: <code>idx [B, K]</code>, integer tile ids (expected <code>int32</code>, <code>int64</code> accepted and cast)</li> <li>Output packed tensor: <code>P [B, K, C, t, t]</code>, contiguous</li> </ul> <p>Grid assumptions: - <code>H % t == 0</code> and <code>W % t == 0</code> - tile id domain: <code>[0, (H / t) * (W / t) - 1]</code></p> <p>Layout assumption: - <code>idx</code> order is consumed as-is by Triton path - no implicit ordering/reordering inside kernel</p>"},{"location":"runtime/TRITON_TILEPACK/#dtype-support","title":"Dtype Support","text":"<ul> <li>Triton kernel path: <code>fp16</code>, <code>bf16</code></li> <li>Reference path: <code>fp32</code>, <code>fp16</code>, <code>bf16</code></li> </ul>"},{"location":"runtime/TRITON_TILEPACK/#dispatch-and-fallback","title":"Dispatch and Fallback","text":"<p>Use: - <code>tilepack_dispatch(...)</code></p> <p>Behavior: - prefers Triton when available (<code>CUDA + Triton</code>) - falls back to vectorized reference PyTorch gather when unavailable - falls back to reference when <code>feature_map.requires_grad</code> and <code>inference_only=True</code>   - reason: current Triton path is inference-oriented and does not register custom backward</p>"},{"location":"runtime/TRITON_TILEPACK/#api","title":"API","text":"<ul> <li><code>get_triton_tilepack_availability()</code></li> <li><code>tilepack_reference(...)</code></li> <li><code>tilepack_triton(...)</code></li> <li><code>tilepack_dispatch(...)</code></li> </ul>"},{"location":"runtime/TRITON_TILEPACK/#testing","title":"Testing","text":"<p>Parity tests: - <code>tests/test_triton_tilepack_parity_dispatch.py</code> - <code>tests/test_triton_tilepack_parity_gpu.py</code></p> <p>Coverage: - CPU fallback correctness vs <code>TilePackTorch</code> - GPU parity vs <code>TilePackTorch</code> on multiple shapes (when Triton/CUDA available) - deterministic seed use - gradient safety via reference fallback path</p>"},{"location":"runtime/TRITON_TILEPACK/#microbenchmark","title":"Microbenchmark","text":"<p>Implemented in: - <code>apex_x/bench/triton_tilepack_bench.py</code></p> <p>Run: <pre><code>python -m apex_x.bench.triton_tilepack_bench \\\n  --batch 1 \\\n  --channels 128 \\\n  --height 128 \\\n  --width 128 \\\n  --tile-size 8 \\\n  --kmax 32 \\\n  --warmup 10 \\\n  --iters 50 \\\n  --dtype fp16\n</code></pre></p> <p>Report includes: - availability and backend selected - fallback reason (if any) - <code>reference_p50/p95</code> and <code>dispatch_p50/p95</code> - speedup ratio (<code>reference / dispatch</code>)</p>"},{"location":"runtime/TRITON_TILEUNPACK/","title":"Triton TileUnpack Kernel","text":""},{"location":"runtime/TRITON_TILEUNPACK/#scope","title":"Scope","text":"<p>This document describes the Triton TileUnpack scatter kernel implemented in: - <code>apex_x/kernels/triton/tileunpack.py</code></p> <p>Current scope: - L0 tile scatter with overlap handling - deterministic priority overwrite semantics - optional blend mode with ordered composition semantics - dedicated Triton blend-update kernel path (<code>_tileunpack_blend_update_kernel</code>)</p>"},{"location":"runtime/TRITON_TILEUNPACK/#tensor-contract","title":"Tensor Contract","text":"<ul> <li>Input base map: <code>F_base [B, C, H, W]</code>, contiguous <code>NCHW</code></li> <li>Input packed map: <code>P_out [B, K, C, t, t]</code>, contiguous</li> <li>Input indices/meta:</li> <li>either <code>idx [B,K]</code> tile ids, or</li> <li><code>meta</code> containing <code>origins [B,K,2]</code></li> <li>Output merged map: <code>F_merged [B, C, H, W]</code></li> </ul> <p>Assumptions: - <code>H % t == 0</code>, <code>W % t == 0</code> - overlaps are allowed</p> <p>Priority inputs: - <code>levels [B,K]</code> integer levels (higher wins), or - pre-sorted K-order (<code>assume_priority_sorted=True</code>) where later K wins when levels are absent</p>"},{"location":"runtime/TRITON_TILEUNPACK/#dtype-support","title":"Dtype Support","text":"<ul> <li>Triton kernel path: <code>fp16</code>, <code>bf16</code> on CUDA</li> <li>Reference path: <code>fp32</code>, <code>fp16</code>, <code>bf16</code></li> </ul>"},{"location":"runtime/TRITON_TILEUNPACK/#dispatch-and-fallback","title":"Dispatch and Fallback","text":"<p>Use: - <code>tileunpack_dispatch(...)</code></p> <p>Behavior: - prefers Triton when available (<code>CUDA + Triton</code>) - falls back to reference path when Triton is unavailable - falls back to reference when autograd is requested (<code>requires_grad</code> and <code>inference_only=True</code>) - defaults to <code>overlap_mode=\"override\"</code> (priority overwrite) - <code>overlap_mode=\"blend\"</code> is supported in dispatch without forced reference-only branch   when Triton path is selected - <code>overlap_mode=\"blend\"</code> in Triton path executes sorted-rank kernel updates on CUDA   (no Python patch loop in the fast path)</p>"},{"location":"runtime/TRITON_TILEUNPACK/#api","title":"API","text":"<ul> <li><code>get_triton_tileunpack_availability()</code></li> <li><code>tileunpack_reference(...)</code></li> <li><code>tileunpack_triton(...)</code></li> <li><code>tileunpack_dispatch(...)</code></li> </ul>"},{"location":"runtime/TRITON_TILEUNPACK/#testing","title":"Testing","text":"<p>Parity tests: - <code>tests/test_triton_tileunpack_parity_dispatch.py</code> - <code>tests/test_triton_tileunpack_parity_gpu.py</code></p> <p>Coverage: - CPU fallback parity with <code>TileUnpackTorch</code> and reference overlap semantics - GPU parity with <code>TileUnpackTorch</code> on representative shapes (auto-skip without CUDA+Triton) - blend-overlap parity contract in dispatch and GPU suites - gradient-safe fallback behavior - synthetic overlap fixtures with explicit overwrite expectations</p>"},{"location":"runtime/TRITON_TILEUNPACK/#microbenchmark","title":"Microbenchmark","text":"<p>Implemented in: - <code>apex_x/bench/triton_tileunpack_bench.py</code></p> <p>Run: <pre><code>python -m apex_x.bench.triton_tileunpack_bench \\\n  --batch 1 \\\n  --channels 128 \\\n  --height 128 \\\n  --width 128 \\\n  --tile-size 8 \\\n  --kmax 32 \\\n  --overlap-shift 4 \\\n  --overlap-mode blend \\\n  --blend-alpha 0.25 \\\n  --warmup 10 \\\n  --iters 50 \\\n  --dtype fp16\n</code></pre></p> <p>Report includes: - backend selected and fallback reason - <code>reference_p50/p95</code>, <code>dispatch_p50/p95</code> - speedup ratio (<code>reference / dispatch</code>)</p>"}]}