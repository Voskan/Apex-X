{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Checkpoint and Image Inference UI\n",
        "\n",
        "This notebook lets you choose a model checkpoint (`.pt`) and an image, run inference, and visualize detections/masks.\n",
        "\n",
        "You can provide files either by:\n",
        "- text path input (for `best.pt` or any local file), or\n",
        "- upload widget.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "\n",
        "from apex_x.config import ApexXConfig\n",
        "from apex_x.model import (\n",
        "    TeacherModel,\n",
        "    TeacherModelV3,\n",
        "    PVModule,\n",
        "    DualPathFPN,\n",
        "    DetHead,\n",
        "    TimmBackboneAdapter,\n",
        "    post_process_detections_per_class,\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "\n",
        "def _extract_uploaded_file(upload_widget):\n",
        "    value = upload_widget.value\n",
        "    if not value:\n",
        "        return None, None\n",
        "\n",
        "    # ipywidgets <8: dict[name] -> {'content': ...}\n",
        "    if isinstance(value, dict):\n",
        "        name = next(iter(value.keys()))\n",
        "        payload = value[name]\n",
        "        content = payload['content'] if isinstance(payload, dict) else payload\n",
        "    else:\n",
        "        # ipywidgets >=8: tuple/list of dicts\n",
        "        item = value[0]\n",
        "        name = item.get('name', 'uploaded.bin')\n",
        "        content = item.get('content')\n",
        "\n",
        "    if isinstance(content, memoryview):\n",
        "        content = content.tobytes()\n",
        "    return name, bytes(content)\n",
        "\n",
        "\n",
    "def _safe_torch_load(source):\n",
    "    try:\n",
    "        return torch.load(source, map_location='cpu', weights_only=True)\n",
    "    except TypeError:\n",
    "        # Backward-compatible fallback for older PyTorch without weights_only.\n",
    "        return torch.load(source, map_location='cpu')\n",
    "\n",
    "\n",
    "def _is_state_dict(candidate):\n",
    "    return isinstance(candidate, dict) and candidate and all(\n",
    "        isinstance(k, str) and isinstance(v, torch.Tensor)\n",
    "        for k, v in candidate.items()\n",
    "    )\n",
    "\n",
    "\n",
    "def _load_checkpoint_payload(checkpoint_path_text, checkpoint_upload_widget):\n",
    "    upload_name, upload_bytes = _extract_uploaded_file(checkpoint_upload_widget)\n",
    "    if upload_bytes is not None:\n",
    "        payload = _safe_torch_load(io.BytesIO(upload_bytes))\n",
    "        return payload, upload_name\n",
    "\n",
    "    checkpoint_path = Path(checkpoint_path_text).expanduser()\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f'Checkpoint not found: {checkpoint_path}')\n",
    "    payload = _safe_torch_load(checkpoint_path)\n",
    "    return payload, str(checkpoint_path)\n",
    "\n",
    "\n",
    "def _extract_state_dict(payload):\n",
    "    if _is_state_dict(payload):\n",
    "        return payload, 'raw_state_dict'\n",
    "\n",
    "    if isinstance(payload, dict):\n",
    "        candidates = (\n",
    "            ('model_state_dict', 'structured_checkpoint'),\n",
    "            ('state_dict', 'state_dict_field'),\n",
    "            ('model', 'train_checkpoint_model'),\n",
    "            ('teacher', 'teacher_field'),\n",
    "            ('ema_model', 'ema_model_field'),\n",
    "            ('ema', 'ema_field'),\n",
    "        )\n",
    "        for key, label in candidates:\n",
    "            candidate = payload.get(key)\n",
    "            if _is_state_dict(candidate):\n",
    "                return candidate, label\n",
    "\n",
    "    raise ValueError(\n",
    "        'Unsupported checkpoint format. Expected raw state_dict or dict with one of: '\n",
    "        'model_state_dict, state_dict, model, teacher, ema_model, ema.'\n",
    "    )\n",
        "\n",
        "\n",
        "def _infer_model_family(state_dict, model_hint='auto'):\n",
        "    if model_hint in {'teacher', 'teacher_v3'}:\n",
        "        return model_hint\n",
        "\n",
        "    v3_markers = ('backbone.', 'neck.', 'mask_head.', 'quality_head.', 'rpn_objectness')\n",
        "    if any(any(k.startswith(marker) for marker in v3_markers) for k in state_dict.keys()):\n",
        "        return 'teacher_v3'\n",
        "    return 'teacher'\n",
        "\n",
        "\n",
        "def _infer_num_classes(state_dict, family):\n",
        "    if family == 'teacher':\n",
        "        w = state_dict.get('det_head.cls_pred.weight')\n",
        "        if isinstance(w, torch.Tensor) and w.ndim >= 1:\n",
        "            return int(w.shape[0])\n",
        "    else:\n",
        "        w = state_dict.get('det_head.stages.0.cls_head.4.weight')\n",
        "        if isinstance(w, torch.Tensor) and w.ndim >= 1:\n",
        "            return int(w.shape[0])\n",
        "    return 3\n",
        "\n",
        "\n",
        "def _infer_teacher_backbone_type(state_dict):\n",
        "    if any(k.startswith('pv_module.backbone.blocks.') for k in state_dict):\n",
        "        return 'timm'\n",
        "    return 'pv'\n",
        "\n",
        "\n",
        "def _build_teacher_model_for_state_dict(state_dict, num_classes):\n",
        "    cfg = ApexXConfig()\n",
        "    backbone_type = _infer_teacher_backbone_type(state_dict)\n",
        "\n",
        "    if backbone_type == 'timm':\n",
        "        pv_module = TimmBackboneAdapter(\n",
        "            model_name='efficientnet_b0',\n",
        "            pretrained=False,\n",
        "            out_indices=(2, 3, 4),\n",
        "        )\n",
        "        p3_ch = pv_module.p3_channels\n",
        "        p4_ch = pv_module.p4_channels\n",
        "        p5_ch = pv_module.p5_channels\n",
        "        ff_channels = p3_ch\n",
        "    else:\n",
        "        pv_module = PVModule(\n",
        "            in_channels=3,\n",
        "            p3_channels=16,\n",
        "            p4_channels=24,\n",
        "            p5_channels=32,\n",
        "            coarse_level='P4',\n",
        "        )\n",
        "        p3_ch, p4_ch, p5_ch = 16, 24, 32\n",
        "        ff_channels = 16\n",
        "\n",
        "    fpn = DualPathFPN(\n",
        "        pv_p3_channels=p3_ch,\n",
        "        pv_p4_channels=p4_ch,\n",
        "        pv_p5_channels=p5_ch,\n",
        "        ff_channels=ff_channels,\n",
        "        out_channels=16,\n",
        "    )\n",
        "    det_head = DetHead(in_channels=16, num_classes=num_classes, hidden_channels=16, depth=1)\n",
        "\n",
        "    model = TeacherModel(\n",
        "        num_classes=num_classes,\n",
        "        config=cfg,\n",
        "        pv_module=pv_module,\n",
        "        fpn=fpn,\n",
        "        det_head=det_head,\n",
        "        feature_layers=('P3', 'P4'),\n",
        "        use_ema=True,\n",
        "        ema_decay=0.99,\n",
        "        use_ema_for_forward=False,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def _load_state_dict_non_strict(model, state_dict, strict=False):\n",
        "    if strict:\n",
        "        incompatible = model.load_state_dict(state_dict, strict=True)\n",
        "        return incompatible, []\n",
        "\n",
        "    model_state = model.state_dict()\n",
        "    filtered = {}\n",
        "    skipped = []\n",
        "    for k, v in state_dict.items():\n",
        "        expected = model_state.get(k)\n",
        "        if expected is None:\n",
        "            continue\n",
        "        if tuple(expected.shape) != tuple(v.shape):\n",
        "            skipped.append(k)\n",
        "            continue\n",
        "        filtered[k] = v\n",
        "\n",
        "    incompatible = model.load_state_dict(filtered, strict=False)\n",
        "    return incompatible, skipped\n",
        "\n",
        "\n",
        "def _load_image(image_path_text, image_upload_widget):\n",
        "    upload_name, upload_bytes = _extract_uploaded_file(image_upload_widget)\n",
        "    if upload_bytes is not None:\n",
        "        image = Image.open(io.BytesIO(upload_bytes)).convert('RGB')\n",
        "        return image, upload_name\n",
        "\n",
        "    image_path = Path(image_path_text).expanduser()\n",
        "    if not image_path.exists():\n",
        "        raise FileNotFoundError(f'Image not found: {image_path}')\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    return image, str(image_path)\n",
        "\n",
        "\n",
        "def _prepare_image_tensor(pil_image, max_side=1280, align_to=32):\n",
        "    w, h = pil_image.size\n",
        "    scale = min(1.0, float(max_side) / float(max(h, w)))\n",
        "    align_to = max(1, int(align_to))\n",
        "    new_h = max(align_to, int(round((h * scale) / float(align_to)) * align_to))\n",
        "    new_w = max(align_to, int(round((w * scale) / float(align_to)) * align_to))\n",
        "    resized = pil_image.resize((new_w, new_h))\n",
        "    image_np = np.array(resized)\n",
        "    tensor = torch.from_numpy(image_np).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
        "    return image_np, tensor.to(device), (new_h, new_w)\n",
        "\n",
        "\n",
        "def _draw_predictions(image_np, boxes, scores, classes, masks=None, max_dets=50):\n",
        "    boxes = boxes.detach().cpu() if isinstance(boxes, torch.Tensor) else torch.as_tensor(boxes)\n",
        "    scores = scores.detach().cpu() if isinstance(scores, torch.Tensor) else torch.as_tensor(scores)\n",
        "    classes = classes.detach().cpu() if isinstance(classes, torch.Tensor) else torch.as_tensor(classes)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "    ax.imshow(image_np)\n",
        "\n",
        "    n = min(int(boxes.shape[0]), int(max_dets))\n",
        "    for i in range(n):\n",
        "        x1, y1, x2, y2 = [float(v) for v in boxes[i].tolist()]\n",
        "        score = float(scores[i].item())\n",
        "        cls_id = int(classes[i].item())\n",
        "\n",
        "        rect = plt.Rectangle((x1, y1), max(1.0, x2 - x1), max(1.0, y2 - y1),\n",
        "                             fill=False, color='lime', linewidth=1.5)\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, max(0.0, y1 - 2.0), f'{cls_id}:{score:.3f}', color='yellow', fontsize=9,\n",
        "                bbox=dict(facecolor='black', alpha=0.5, pad=1))\n",
        "\n",
        "        if masks is not None and i < masks.shape[0]:\n",
        "            m = masks[i]\n",
        "            if isinstance(m, torch.Tensor):\n",
        "                m = m.detach().cpu().numpy()\n",
        "            m = (m > 0.5).astype(np.float32)\n",
        "            if m.shape[:2] != image_np.shape[:2]:\n",
        "                continue\n",
        "            overlay = np.zeros((m.shape[0], m.shape[1], 4), dtype=np.float32)\n",
        "            overlay[..., 1] = 1.0\n",
        "            overlay[..., 3] = 0.25 * m\n",
        "            ax.imshow(overlay)\n",
        "\n",
        "    ax.set_axis_off()\n",
        "    ax.set_title(f'Detections: {n}')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "checkpoint_path = widgets.Text(\n",
        "    value='artifacts/train_output/checkpoints/best.pt',\n",
        "    description='CKPT path:',\n",
        "    layout=widgets.Layout(width='900px')\n",
        ")\n",
        "checkpoint_upload = widgets.FileUpload(accept='.pt', multiple=False, description='Upload CKPT')\n",
        "\n",
        "image_path = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='optional local image path',\n",
        "    description='Image path:',\n",
        "    layout=widgets.Layout(width='900px')\n",
        ")\n",
        "image_upload = widgets.FileUpload(accept='image/*', multiple=False, description='Upload image')\n",
        "\n",
        "model_hint = widgets.Dropdown(options=['auto', 'teacher', 'teacher_v3'], value='auto', description='Model:')\n",
        "strict_load = widgets.Checkbox(value=False, description='Strict load')\n",
        "conf_threshold = widgets.FloatSlider(value=0.25, min=0.0, max=1.0, step=0.01, description='Conf:')\n",
        "nms_iou = widgets.FloatSlider(value=0.5, min=0.1, max=0.9, step=0.01, description='NMS IoU:')\n",
        "max_dets = widgets.IntSlider(value=100, min=1, max=500, step=1, description='Max dets:')\n",
        "run_button = widgets.Button(description='Run Inference', button_style='success')\n",
        "out = widgets.Output()\n",
        "\n",
        "\n",
        "def _run_inference(_):\n",
        "    with out:\n",
        "        out.clear_output(wait=True)\n",
        "        try:\n",
        "            ckpt_payload, ckpt_name = _load_checkpoint_payload(checkpoint_path.value.strip(), checkpoint_upload)\n",
        "            state_dict, ckpt_format = _extract_state_dict(ckpt_payload)\n",
        "\n",
        "            family = _infer_model_family(state_dict, model_hint.value)\n",
        "            num_classes = _infer_num_classes(state_dict, family)\n",
        "\n",
        "            if family == 'teacher_v3':\n",
        "                model = TeacherModelV3(num_classes=num_classes)\n",
        "            else:\n",
        "                model = _build_teacher_model_for_state_dict(state_dict, num_classes=num_classes)\n",
        "\n",
        "            incompatible, skipped = _load_state_dict_non_strict(model, state_dict, strict=bool(strict_load.value))\n",
        "            model = model.to(device).eval()\n",
        "\n",
        "            img_pil, image_name = _load_image(image_path.value.strip(), image_upload)\n",
        "            align_to = 14 if family == 'teacher_v3' else 32\n",
        "            image_np, image_tensor, image_size = _prepare_image_tensor(img_pil, align_to=align_to)\n",
        "\n",
        "            print(f'Checkpoint: {ckpt_name} ({ckpt_format})')\n",
        "            print(f'Model family: {family} | classes: {num_classes}')\n",
        "            print(f'Image: {image_name} | resized: {image_size[1]}x{image_size[0]} | align: {align_to}')\n",
        "            print(f\"Missing keys: {len(getattr(incompatible, 'missing_keys', []))} | Unexpected keys: {len(getattr(incompatible, 'unexpected_keys', []))} | Shape-skipped: {len(skipped)}\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if family == 'teacher_v3':\n",
        "                    outputs = model(image_tensor)\n",
        "                    boxes = outputs['boxes']\n",
        "                    score_matrix = outputs['scores']\n",
        "                    if score_matrix.ndim == 2 and score_matrix.shape[1] > 0:\n",
        "                        if float(score_matrix.min().item()) < 0.0 or float(score_matrix.max().item()) > 1.0:\n",
        "                            score_matrix = torch.sigmoid(score_matrix)\n",
        "                        scores, classes = score_matrix.max(dim=1)\n",
        "                    else:\n",
        "                        scores = torch.zeros((boxes.shape[0],), device=boxes.device)\n",
        "                        classes = torch.zeros((boxes.shape[0],), dtype=torch.int64, device=boxes.device)\n",
        "\n",
        "                    keep = scores >= float(conf_threshold.value)\n",
        "                    boxes = boxes[keep]\n",
        "                    scores = scores[keep]\n",
        "                    classes = classes[keep]\n",
        "\n",
        "                    if boxes.numel() > 0:\n",
        "                        keep_idx = torchvision.ops.nms(boxes, scores, float(nms_iou.value))\n",
        "                        keep_idx = keep_idx[: int(max_dets.value)]\n",
        "                        boxes = boxes[keep_idx]\n",
        "                        scores = scores[keep_idx]\n",
        "                        classes = classes[keep_idx]\n",
        "                    else:\n",
        "                        keep_idx = torch.zeros((0,), dtype=torch.int64, device=boxes.device)\n",
        "\n",
        "                    masks = outputs.get('masks')\n",
        "                    masks_up = None\n",
        "                    if isinstance(masks, torch.Tensor) and masks.numel() > 0 and keep_idx.numel() > 0:\n",
        "                        masks_sel = masks[keep][keep_idx]\n",
        "                        if masks_sel.ndim == 4 and masks_sel.shape[1] == 1:\n",
        "                            masks_sel = masks_sel[:, 0]\n",
        "                        if masks_sel.ndim == 3:\n",
        "                            masks_up = F.interpolate(\n",
        "                                masks_sel.unsqueeze(1),\n",
        "                                size=(image_np.shape[0], image_np.shape[1]),\n",
        "                                mode='bilinear',\n",
        "                                align_corners=False,\n",
        "                            ).squeeze(1)\n",
        "\n",
        "                else:\n",
        "                    outputs = model(image_tensor, use_ema=False)\n",
        "                    dets = post_process_detections_per_class(\n",
        "                        outputs.logits_by_level,\n",
        "                        outputs.boxes_by_level,\n",
        "                        outputs.quality_by_level,\n",
        "                        conf_threshold=float(conf_threshold.value),\n",
        "                        nms_threshold=float(nms_iou.value),\n",
        "                        max_detections=int(max_dets.value),\n",
        "                    )[0]\n",
        "                    boxes = dets['boxes']\n",
        "                    scores = dets['scores']\n",
        "                    classes = dets['classes']\n",
        "\n",
        "                    masks_up = None\n",
        "                    if isinstance(outputs.masks, torch.Tensor) and outputs.masks.numel() > 0:\n",
        "                        masks = outputs.masks[0]\n",
        "                        if masks.ndim == 4 and masks.shape[1] == 1:\n",
        "                            masks = masks[:, 0]\n",
        "                        if masks.ndim == 3:\n",
        "                            masks_up = F.interpolate(\n",
        "                                masks.unsqueeze(1),\n",
        "                                size=(image_np.shape[0], image_np.shape[1]),\n",
        "                                mode='bilinear',\n",
        "                                align_corners=False,\n",
        "                            ).squeeze(1)\n",
        "                            if masks_up.shape[0] > boxes.shape[0]:\n",
        "                                masks_up = masks_up[: boxes.shape[0]]\n",
        "\n",
        "            _draw_predictions(image_np, boxes, scores, classes, masks=masks_up, max_dets=int(max_dets.value))\n",
        "\n",
        "        except Exception as exc:\n",
        "            print(f'Error: {exc}')\n",
        "\n",
        "\n",
        "run_button.on_click(_run_inference)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML('<b>Checkpoint</b>'),\n",
        "    checkpoint_path,\n",
        "    checkpoint_upload,\n",
        "    widgets.HTML('<b>Image</b>'),\n",
        "    image_path,\n",
        "    image_upload,\n",
        "    widgets.HBox([model_hint, strict_load]),\n",
        "    conf_threshold,\n",
        "    nms_iou,\n",
        "    max_dets,\n",
        "    run_button,\n",
        "    out,\n",
        "]))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
