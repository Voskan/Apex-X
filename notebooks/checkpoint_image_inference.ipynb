{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint and Image Inference UI\n",
    "\n",
    "This notebook lets you choose a model checkpoint (`.pt`) and an image, run inference, and visualize detections/masks.\n",
    "\n",
    "You can provide files either by:\n",
    "- text path input (for `best.pt` or any local file), or\n",
    "- upload widget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "from apex_x.config import ApexXConfig\n",
    "from apex_x.model import (\n",
    "    TeacherModel,\n",
    "    TeacherModelV3,\n",
    "    PVModule,\n",
    "    DualPathFPN,\n",
    "    DetHead,\n",
    "    TimmBackboneAdapter,\n",
    "    post_process_detections_per_class,\n",
    ")\n",
    "from apex_x.model.image_enhancer import LearnableImageEnhancer\n",
    "from apex_x.model.pv_dinov2 import PVModuleDINOv2\n",
    "from apex_x.train.checkpoint import extract_model_state_dict, is_tensor_state_dict, safe_torch_load\n",
    "from apex_x.model.worldclass_deps import missing_worldclass_dependencies, worldclass_install_hint\n",
    "\n",
    "\n",
    "def _robust_reshape_vit_output(self, x, image_hw=None):\n",
    "    if x.ndim != 3:\n",
    "        raise ValueError('ViT hidden-state must be [B, N_tokens, D]')\n",
    "\n",
    "    bsz, token_count, dim = x.shape\n",
    "    patch_size = max(1, int(getattr(self, 'patch_size', 14)))\n",
    "\n",
    "    num_register_tokens = 0\n",
    "    try:\n",
    "        num_register_tokens = int(getattr(self.dinov2.config, 'num_register_tokens', 0) or 0)\n",
    "    except Exception:\n",
    "        num_register_tokens = 0\n",
    "\n",
    "    candidates = [1]\n",
    "    if num_register_tokens > 0:\n",
    "        candidates.append(1 + num_register_tokens)\n",
    "    candidates.append(0)\n",
    "\n",
    "    special = None\n",
    "    if image_hw is not None:\n",
    "        exp_h = max(1, int(image_hw[0]) // patch_size)\n",
    "        exp_w = max(1, int(image_hw[1]) // patch_size)\n",
    "        exp_tokens = exp_h * exp_w\n",
    "        for s in candidates:\n",
    "            if token_count - s == exp_tokens:\n",
    "                special = s\n",
    "                break\n",
    "\n",
    "    if special is None:\n",
    "        for s in candidates:\n",
    "            if token_count - s > 0:\n",
    "                special = s\n",
    "                break\n",
    "\n",
    "    if special is None:\n",
    "        raise ValueError(f'Invalid token count {token_count}: cannot strip special tokens')\n",
    "\n",
    "    x = x[:, special:, :]\n",
    "    num_tokens = int(x.shape[1])\n",
    "\n",
    "    ratio = None\n",
    "    if image_hw is not None and int(image_hw[1]) > 0:\n",
    "        ratio = float(image_hw[0]) / float(image_hw[1])\n",
    "\n",
    "    best = None\n",
    "    root = int(math.isqrt(num_tokens))\n",
    "    for h in range(root, 0, -1):\n",
    "        if num_tokens % h != 0:\n",
    "            continue\n",
    "        w = num_tokens // h\n",
    "        for cand_h, cand_w in ((h, w), (w, h)):\n",
    "            if ratio is None:\n",
    "                score = abs(cand_h - cand_w)\n",
    "            else:\n",
    "                score = abs((float(cand_h) / float(max(1, cand_w))) - ratio)\n",
    "            if best is None or score < best[0]:\n",
    "                best = (score, cand_h, cand_w)\n",
    "\n",
    "    if best is None:\n",
    "        raise ValueError(f'Cannot factor token grid for num_tokens={num_tokens}')\n",
    "\n",
    "    h_patches, w_patches = best[1], best[2]\n",
    "    if h_patches * w_patches != num_tokens:\n",
    "        raise ValueError(\n",
    "            f'Cannot reshape {num_tokens} tokens into a 2-D grid '\n",
    "            f'({h_patches}x{w_patches} != {num_tokens})'\n",
    "        )\n",
    "\n",
    "    return x.reshape(bsz, h_patches, w_patches, dim).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "def _patch_pvmodule_dinov2_rect_tokens():\n",
    "    # Runtime compatibility patch for environments with older pv_dinov2 reshape logic.\n",
    "    PVModuleDINOv2._reshape_vit_output = _robust_reshape_vit_output\n",
    "\n",
    "\n",
    "_patch_pvmodule_dinov2_rect_tokens()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "missing_deps = missing_worldclass_dependencies()\n",
    "if missing_deps:\n",
    "    print(f\"Worldclass deps missing: {missing_deps}\")\n",
    "    print(worldclass_install_hint())\n",
    "\n",
    "\n",
    "def _extract_uploaded_file(upload_widget):\n",
    "    value = upload_widget.value\n",
    "    if not value:\n",
    "        return None, None\n",
    "\n",
    "    # ipywidgets <8: dict[name] -> {'content': ...}\n",
    "    if isinstance(value, dict):\n",
    "        name = next(iter(value.keys()))\n",
    "        payload = value[name]\n",
    "        content = payload['content'] if isinstance(payload, dict) else payload\n",
    "    else:\n",
    "        # ipywidgets >=8: tuple/list of dicts\n",
    "        item = value[0]\n",
    "        name = item.get('name', 'uploaded.bin')\n",
    "        content = item.get('content')\n",
    "\n",
    "    if isinstance(content, memoryview):\n",
    "        content = content.tobytes()\n",
    "    return name, bytes(content)\n",
    "\n",
    "\n",
    "def _safe_torch_load(source):\n",
    "    return safe_torch_load(source, map_location='cpu')\n",
    "\n",
    "\n",
    "def _is_state_dict(candidate):\n",
    "    return is_tensor_state_dict(candidate)\n",
    "\n",
    "\n",
    "def _load_checkpoint_payload(checkpoint_path_text, checkpoint_upload_widget):\n",
    "    upload_name, upload_bytes = _extract_uploaded_file(checkpoint_upload_widget)\n",
    "    if upload_bytes is not None:\n",
    "        payload = _safe_torch_load(io.BytesIO(upload_bytes))\n",
    "        return payload, upload_name\n",
    "\n",
    "    checkpoint_path = Path(checkpoint_path_text).expanduser()\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f'Checkpoint not found: {checkpoint_path}')\n",
    "    payload = _safe_torch_load(checkpoint_path)\n",
    "    return payload, str(checkpoint_path)\n",
    "\n",
    "\n",
    "def _extract_state_dict(payload):\n",
    "    state_dict, fmt = extract_model_state_dict(payload)\n",
    "    fmt_map = {\n",
    "        'model_state_dict': 'structured_checkpoint',\n",
    "        'state_dict': 'state_dict_field',\n",
    "        'model': 'train_checkpoint_model',\n",
    "        'teacher': 'teacher_field',\n",
    "        'ema_model': 'ema_model_field',\n",
    "        'ema': 'ema_field',\n",
    "        'raw_state_dict': 'raw_state_dict',\n",
    "    }\n",
    "    return state_dict, fmt_map.get(fmt, fmt)\n",
    "\n",
    "\n",
    "def _infer_model_family(state_dict, model_hint='auto'):\n",
    "    if model_hint in {'teacher', 'teacher_v3'}:\n",
    "        return model_hint\n",
    "\n",
    "    v3_markers = ('backbone.', 'neck.', 'mask_head.', 'quality_head.', 'rpn_objectness')\n",
    "    if any(any(k.startswith(marker) for marker in v3_markers) for k in state_dict.keys()):\n",
    "        return 'teacher_v3'\n",
    "    return 'teacher'\n",
    "\n",
    "\n",
    "def _infer_num_classes(state_dict, family):\n",
    "    if family == 'teacher':\n",
    "        w = state_dict.get('det_head.cls_pred.weight')\n",
    "        if isinstance(w, torch.Tensor) and w.ndim >= 1:\n",
    "            return int(w.shape[0])\n",
    "    else:\n",
    "        w = state_dict.get('det_head.stages.0.cls_head.4.weight')\n",
    "        if isinstance(w, torch.Tensor) and w.ndim >= 1:\n",
    "            return int(w.shape[0])\n",
    "    return 3\n",
    "\n",
    "\n",
    "def _infer_teacher_backbone_type(state_dict):\n",
    "    if any(k.startswith('pv_module.backbone.blocks.') for k in state_dict):\n",
    "        return 'timm'\n",
    "    return 'pv'\n",
    "\n",
    "\n",
    "def _build_teacher_model_for_state_dict(state_dict, num_classes):\n",
    "    cfg = ApexXConfig()\n",
    "    backbone_type = _infer_teacher_backbone_type(state_dict)\n",
    "\n",
    "    if backbone_type == 'timm':\n",
    "        pv_module = TimmBackboneAdapter(\n",
    "            model_name='efficientnet_b0',\n",
    "            pretrained=False,\n",
    "            out_indices=(2, 3, 4),\n",
    "        )\n",
    "        p3_ch = pv_module.p3_channels\n",
    "        p4_ch = pv_module.p4_channels\n",
    "        p5_ch = pv_module.p5_channels\n",
    "        ff_channels = p3_ch\n",
    "    else:\n",
    "        pv_module = PVModule(\n",
    "            in_channels=3,\n",
    "            p3_channels=16,\n",
    "            p4_channels=24,\n",
    "            p5_channels=32,\n",
    "            coarse_level='P4',\n",
    "        )\n",
    "        p3_ch, p4_ch, p5_ch = 16, 24, 32\n",
    "        ff_channels = 16\n",
    "\n",
    "    fpn = DualPathFPN(\n",
    "        pv_p3_channels=p3_ch,\n",
    "        pv_p4_channels=p4_ch,\n",
    "        pv_p5_channels=p5_ch,\n",
    "        ff_channels=ff_channels,\n",
    "        out_channels=16,\n",
    "    )\n",
    "    det_head = DetHead(in_channels=16, num_classes=num_classes, hidden_channels=16, depth=1)\n",
    "\n",
    "    model = TeacherModel(\n",
    "        num_classes=num_classes,\n",
    "        config=cfg,\n",
    "        pv_module=pv_module,\n",
    "        fpn=fpn,\n",
    "        det_head=det_head,\n",
    "        feature_layers=('P3', 'P4'),\n",
    "        use_ema=True,\n",
    "        ema_decay=0.99,\n",
    "        use_ema_for_forward=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def _load_state_dict_non_strict(model, state_dict, strict=False):\n",
    "    if strict:\n",
    "        incompatible = model.load_state_dict(state_dict, strict=True)\n",
    "        return incompatible, []\n",
    "\n",
    "    model_state = model.state_dict()\n",
    "    filtered = {}\n",
    "    skipped = []\n",
    "    for k, v in state_dict.items():\n",
    "        expected = model_state.get(k)\n",
    "        if expected is None:\n",
    "            continue\n",
    "        if tuple(expected.shape) != tuple(v.shape):\n",
    "            skipped.append(k)\n",
    "            continue\n",
    "        filtered[k] = v\n",
    "\n",
    "    incompatible = model.load_state_dict(filtered, strict=False)\n",
    "    return incompatible, skipped\n",
    "\n",
    "\n",
    "def _load_image(image_path_text, image_upload_widget):\n",
    "    upload_name, upload_bytes = _extract_uploaded_file(image_upload_widget)\n",
    "    if upload_bytes is not None:\n",
    "        image = Image.open(io.BytesIO(upload_bytes)).convert('RGB')\n",
    "        return image, upload_name\n",
    "\n",
    "    image_path = Path(image_path_text).expanduser()\n",
    "    if not image_path.exists():\n",
    "        raise FileNotFoundError(f'Image not found: {image_path}')\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return image, str(image_path)\n",
    "\n",
    "\n",
    "def _prepare_image_tensor(\n",
    "    pil_image,\n",
    "    target_size=1024,\n",
    "    keep_aspect=False,\n",
    "    align_to=32,\n",
    "):\n",
    "    w, h = pil_image.size\n",
    "    target_size = int(max(64, target_size))\n",
    "\n",
    "    if keep_aspect:\n",
    "        scale = float(target_size) / float(max(h, w))\n",
    "        align_to = max(1, int(align_to))\n",
    "        new_h = max(align_to, int(round((h * scale) / float(align_to)) * align_to))\n",
    "        new_w = max(align_to, int(round((w * scale) / float(align_to)) * align_to))\n",
    "    else:\n",
    "        new_h = target_size\n",
    "        new_w = target_size\n",
    "\n",
    "    resized = pil_image.resize((new_w, new_h))\n",
    "    image_np = np.array(resized)\n",
    "    tensor = torch.from_numpy(image_np).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "    return image_np, tensor.to(device), (new_h, new_w)\n",
    "\n",
    "\n",
    "def _clip_boxes_to_image(boxes, image_h, image_w):\n",
    "    boxes = boxes.clone()\n",
    "    boxes[:, 0::2] = boxes[:, 0::2].clamp(0, max(0, image_w - 1))\n",
    "    boxes[:, 1::2] = boxes[:, 1::2].clamp(0, max(0, image_h - 1))\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def _postprocess_teacher_v3_outputs(outputs, conf_threshold, nms_iou, max_dets, image_hw):\n",
    "    image_h, image_w = int(image_hw[0]), int(image_hw[1])\n",
    "\n",
    "    boxes = outputs.get('boxes')\n",
    "    score_matrix = outputs.get('scores')\n",
    "    if not isinstance(boxes, torch.Tensor) or boxes.ndim != 2 or boxes.shape[1] != 4:\n",
    "        raise ValueError('TeacherModelV3 output has invalid boxes tensor')\n",
    "    if not isinstance(score_matrix, torch.Tensor) or score_matrix.ndim != 2:\n",
    "        raise ValueError('TeacherModelV3 output has invalid scores tensor')\n",
    "\n",
    "    if score_matrix.shape[0] != boxes.shape[0]:\n",
    "        n = min(score_matrix.shape[0], boxes.shape[0])\n",
    "        boxes = boxes[:n]\n",
    "        score_matrix = score_matrix[:n]\n",
    "\n",
    "    # If scores look like logits, map to probabilities.\n",
    "    if float(score_matrix.min().item()) < 0.0 or float(score_matrix.max().item()) > 1.0:\n",
    "        score_matrix = torch.sigmoid(score_matrix)\n",
    "\n",
    "    scores, classes = score_matrix.max(dim=1)\n",
    "    boxes = _clip_boxes_to_image(boxes, image_h=image_h, image_w=image_w)\n",
    "\n",
    "    keep = scores >= float(conf_threshold)\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    classes = classes[keep]\n",
    "\n",
    "    if boxes.numel() > 0:\n",
    "        keep_idx = torchvision.ops.batched_nms(\n",
    "            boxes,\n",
    "            scores,\n",
    "            classes,\n",
    "            iou_threshold=float(nms_iou),\n",
    "        )\n",
    "        keep_idx = keep_idx[: int(max_dets)]\n",
    "        boxes = boxes[keep_idx]\n",
    "        scores = scores[keep_idx]\n",
    "        classes = classes[keep_idx]\n",
    "    else:\n",
    "        keep_idx = torch.zeros((0,), dtype=torch.int64, device=boxes.device)\n",
    "\n",
    "    masks = outputs.get('masks')\n",
    "    masks_up = None\n",
    "    if isinstance(masks, torch.Tensor) and masks.numel() > 0 and keep_idx.numel() > 0:\n",
    "        masks_sel = masks[keep][keep_idx]\n",
    "        if masks_sel.ndim == 4 and masks_sel.shape[1] == 1:\n",
    "            masks_sel = masks_sel[:, 0]\n",
    "        if masks_sel.ndim == 3:\n",
    "            masks_up = F.interpolate(\n",
    "                masks_sel.unsqueeze(1),\n",
    "                size=(image_h, image_w),\n",
    "                mode='bilinear',\n",
    "                align_corners=False,\n",
    "            ).squeeze(1)\n",
    "\n",
    "    return boxes, scores, classes, masks_up\n",
    "\n",
    "\n",
    "def _draw_predictions(image_np, boxes, scores, classes, masks=None, max_dets=50, mask_threshold=0.5):\n",
    "    boxes = boxes.detach().cpu() if isinstance(boxes, torch.Tensor) else torch.as_tensor(boxes)\n",
    "    scores = scores.detach().cpu() if isinstance(scores, torch.Tensor) else torch.as_tensor(scores)\n",
    "    classes = classes.detach().cpu() if isinstance(classes, torch.Tensor) else torch.as_tensor(classes)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "    ax.imshow(image_np)\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "    n = min(int(boxes.shape[0]), int(max_dets))\n",
    "    for i in range(n):\n",
    "        x1, y1, x2, y2 = [float(v) for v in boxes[i].tolist()]\n",
    "        score = float(scores[i].item())\n",
    "        cls_id = int(classes[i].item())\n",
    "\n",
    "        color = tuple((rng.rand(3) * 0.8 + 0.2).tolist())\n",
    "        rect = plt.Rectangle(\n",
    "            (x1, y1),\n",
    "            max(1.0, x2 - x1),\n",
    "            max(1.0, y2 - y1),\n",
    "            fill=False,\n",
    "            color=color,\n",
    "            linewidth=1.5,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "            x1,\n",
    "            max(0.0, y1 - 2.0),\n",
    "            f'{cls_id}:{score:.3f}',\n",
    "            color='white',\n",
    "            fontsize=9,\n",
    "            bbox=dict(facecolor='black', alpha=0.6, pad=1),\n",
    "        )\n",
    "\n",
    "        if masks is not None and i < masks.shape[0]:\n",
    "            m = masks[i]\n",
    "            if isinstance(m, torch.Tensor):\n",
    "                m = m.detach().cpu().numpy()\n",
    "            m = (m > float(mask_threshold)).astype(np.float32)\n",
    "            if m.shape[:2] != image_np.shape[:2]:\n",
    "                continue\n",
    "            overlay = np.zeros((m.shape[0], m.shape[1], 4), dtype=np.float32)\n",
    "            overlay[..., 0] = color[0]\n",
    "            overlay[..., 1] = color[1]\n",
    "            overlay[..., 2] = color[2]\n",
    "            overlay[..., 3] = 0.25 * m\n",
    "            ax.imshow(overlay)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(f'Detections: {n}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "checkpoint_path = widgets.Text(\n",
    "    value='outputs/a100_v3_1024px/best_1024.pt',\n",
    "    description='CKPT path:',\n",
    "    layout=widgets.Layout(width='900px')\n",
    ")\n",
    "checkpoint_upload = widgets.FileUpload(accept='.pt', multiple=False, description='Upload CKPT')\n",
    "\n",
    "image_path = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='optional local image path',\n",
    "    description='Image path:',\n",
    "    layout=widgets.Layout(width='900px')\n",
    ")\n",
    "image_upload = widgets.FileUpload(accept='image/*', multiple=False, description='Upload image')\n",
    "\n",
    "model_hint = widgets.Dropdown(options=['auto', 'teacher', 'teacher_v3'], value='auto', description='Model:')\n",
    "strict_load = widgets.Checkbox(value=False, description='Strict load')\n",
    "use_ckpt_enhancer = widgets.Checkbox(value=True, description='Use enhancer')\n",
    "keep_aspect = widgets.Checkbox(value=False, description='Keep aspect')\n",
    "align_mode = widgets.Dropdown(\n",
    "    options=[('auto', 'auto'), ('14', '14'), ('16', '16'), ('28', '28'), ('32', '32')],\n",
    "    value='auto',\n",
    "    description='Align:',\n",
    ")\n",
    "inference_size = widgets.IntSlider(value=1024, min=256, max=2048, step=32, description='Infer size:')\n",
    "conf_threshold = widgets.FloatSlider(value=0.25, min=0.0, max=1.0, step=0.01, description='Conf:')\n",
    "nms_iou = widgets.FloatSlider(value=0.5, min=0.1, max=0.9, step=0.01, description='NMS IoU:')\n",
    "mask_threshold = widgets.FloatSlider(value=0.5, min=0.05, max=0.95, step=0.01, description='Mask thr:')\n",
    "max_dets = widgets.IntSlider(value=100, min=1, max=500, step=1, description='Max dets:')\n",
    "run_button = widgets.Button(description='Run Inference', button_style='success')\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def _run_inference(_):\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        try:\n",
    "            ckpt_payload, ckpt_name = _load_checkpoint_payload(checkpoint_path.value.strip(), checkpoint_upload)\n",
    "            state_dict, ckpt_format = _extract_state_dict(ckpt_payload)\n",
    "\n",
    "            family = _infer_model_family(state_dict, model_hint.value)\n",
    "            num_classes = _infer_num_classes(state_dict, family)\n",
    "\n",
    "            if family == 'teacher_v3':\n",
    "                model = TeacherModelV3(num_classes=num_classes)\n",
    "            else:\n",
    "                model = _build_teacher_model_for_state_dict(state_dict, num_classes=num_classes)\n",
    "\n",
    "            incompatible, skipped = _load_state_dict_non_strict(model, state_dict, strict=bool(strict_load.value))\n",
    "            model = model.to(device).eval()\n",
    "\n",
    "            enhancer = None\n",
    "            enhancer_status = 'not used'\n",
    "            if bool(use_ckpt_enhancer.value) and isinstance(ckpt_payload, dict):\n",
    "                enhancer_state = ckpt_payload.get('enhancer')\n",
    "                if _is_state_dict(enhancer_state):\n",
    "                    enhancer = LearnableImageEnhancer()\n",
    "                    enh_incompat, enh_skipped = _load_state_dict_non_strict(enhancer, enhancer_state, strict=False)\n",
    "                    enhancer = enhancer.to(device).eval()\n",
    "                    enhancer_status = (\n",
    "                        f'loaded (missing={len(getattr(enh_incompat, \"missing_keys\", []))}, '\n",
    "                        f'unexpected={len(getattr(enh_incompat, \"unexpected_keys\", []))}, '\n",
    "                        f'shape_skipped={len(enh_skipped)})'\n",
    "                    )\n",
    "                else:\n",
    "                    enhancer_status = 'checkpoint has no enhancer state'\n",
    "\n",
    "            img_pil, image_name = _load_image(image_path.value.strip(), image_upload)\n",
    "            align_to = 14 if family == 'teacher_v3' else 32\n",
    "            if align_mode.value != 'auto':\n",
    "                align_to = int(align_mode.value)\n",
    "            image_np, image_tensor, image_size = _prepare_image_tensor(\n",
    "                img_pil,\n",
    "                target_size=int(inference_size.value),\n",
    "                keep_aspect=bool(keep_aspect.value),\n",
    "                align_to=align_to,\n",
    "            )\n",
    "\n",
    "            print(f'Checkpoint: {ckpt_name} ({ckpt_format})')\n",
    "            print(f'Model family: {family} | classes: {num_classes}')\n",
    "            print(f'Image: {image_name} | resized: {image_size[1]}x{image_size[0]} | align: {align_to} | keep_aspect: {bool(keep_aspect.value)}')\n",
    "            print(f\"Missing keys: {len(getattr(incompatible, 'missing_keys', []))} | Unexpected keys: {len(getattr(incompatible, 'unexpected_keys', []))} | Shape-skipped: {len(skipped)}\")\n",
    "            print(f'Enhancer: {enhancer_status}')\n",
    "\n",
    "            def _forward_once(model_input, image_hw):\n",
    "                if family == 'teacher_v3':\n",
    "                    outputs = model(model_input)\n",
    "                    return _postprocess_teacher_v3_outputs(\n",
    "                        outputs,\n",
    "                        conf_threshold=float(conf_threshold.value),\n",
    "                        nms_iou=float(nms_iou.value),\n",
    "                        max_dets=int(max_dets.value),\n",
    "                        image_hw=image_hw,\n",
    "                    )\n",
    "\n",
    "                outputs = model(model_input, use_ema=False)\n",
    "                dets = post_process_detections_per_class(\n",
    "                    outputs.logits_by_level,\n",
    "                    outputs.boxes_by_level,\n",
    "                    outputs.quality_by_level,\n",
    "                    conf_threshold=float(conf_threshold.value),\n",
    "                    nms_threshold=float(nms_iou.value),\n",
    "                    max_detections=int(max_dets.value),\n",
    "                )[0]\n",
    "                boxes = dets['boxes']\n",
    "                scores = dets['scores']\n",
    "                classes = dets['classes']\n",
    "\n",
    "                masks_up = None\n",
    "                if isinstance(outputs.masks, torch.Tensor) and outputs.masks.numel() > 0:\n",
    "                    masks = outputs.masks[0]\n",
    "                    if masks.ndim == 4 and masks.shape[1] == 1:\n",
    "                        masks = masks[:, 0]\n",
    "                    if masks.ndim == 3:\n",
    "                        masks_up = F.interpolate(\n",
    "                            masks.unsqueeze(1),\n",
    "                            size=(image_hw[0], image_hw[1]),\n",
    "                            mode='bilinear',\n",
    "                            align_corners=False,\n",
    "                        ).squeeze(1)\n",
    "                        if masks_up.shape[0] > boxes.shape[0]:\n",
    "                            masks_up = masks_up[: boxes.shape[0]]\n",
    "\n",
    "                return boxes, scores, classes, masks_up\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_input = image_tensor\n",
    "                if enhancer is not None:\n",
    "                    model_input = enhancer(model_input)\n",
    "\n",
    "                try:\n",
    "                    boxes, scores, classes, masks_up = _forward_once(\n",
    "                        model_input,\n",
    "                        image_hw=(image_np.shape[0], image_np.shape[1]),\n",
    "                    )\n",
    "                except ValueError as exc:\n",
    "                    message = str(exc)\n",
    "                    can_retry = family == 'teacher_v3' and 'Cannot reshape' in message\n",
    "                    if not can_retry:\n",
    "                        raise\n",
    "                    print(f'Warning: {message}')\n",
    "                    print('Retrying with square resize to stabilize token grid...')\n",
    "                    image_np, image_tensor, image_size = _prepare_image_tensor(\n",
    "                        img_pil,\n",
    "                        target_size=int(inference_size.value),\n",
    "                        keep_aspect=False,\n",
    "                        align_to=14,\n",
    "                    )\n",
    "                    model_input = image_tensor\n",
    "                    if enhancer is not None:\n",
    "                        model_input = enhancer(model_input)\n",
    "                    boxes, scores, classes, masks_up = _forward_once(\n",
    "                        model_input,\n",
    "                        image_hw=(image_np.shape[0], image_np.shape[1]),\n",
    "                    )\n",
    "                    print(\n",
    "                        f'Retry image resized to: {image_size[1]}x{image_size[0]} '\n",
    "                        '| align: 14 | keep_aspect: false'\n",
    "                    )\n",
    "\n",
    "            _draw_predictions(\n",
    "                image_np,\n",
    "                boxes,\n",
    "                scores,\n",
    "                classes,\n",
    "                masks=masks_up,\n",
    "                max_dets=int(max_dets.value),\n",
    "                mask_threshold=float(mask_threshold.value),\n",
    "            )\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f'Error: {exc}')\n",
    "\n",
    "\n",
    "run_button.on_click(_run_inference)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML('<b>Checkpoint</b>'),\n",
    "    checkpoint_path,\n",
    "    checkpoint_upload,\n",
    "    widgets.HTML('<b>Image</b>'),\n",
    "    image_path,\n",
    "    image_upload,\n",
    "    widgets.HBox([model_hint, strict_load, use_ckpt_enhancer, keep_aspect, align_mode]),\n",
    "    inference_size,\n",
    "    conf_threshold,\n",
    "    nms_iou,\n",
    "    mask_threshold,\n",
    "    max_dets,\n",
    "    run_button,\n",
    "    out,\n",
    "]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}