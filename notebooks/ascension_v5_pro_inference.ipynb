{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Apex-X Ascension Inference Dashboard (Compatible Mode)\n",
    "\n",
    "**Infinite Resolution & SOTA Benchmarking**\n",
    "\n",
    "This dashboard is designed for high-fidelity evaluation of the model.\n",
    "**NOTE:** Automatically detected `TeacherModel` (V1/Base) checkpoint. Running in compatibility mode.\n",
    "1. **Sub-pixel Boundary UI**: Interactive zoom into mask edges.\n",
    "2. **Confidence Scores**: Top-5 analysis.\n",
    "3. **Production Benchmarking**: Latency/FPS profiles for Cloud & Edge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d7135",
   "metadata": {},
   "source": [
    "## 1. System Setup & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1260831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='IPython')\n",
    "\n",
    "# 1. Install critical system dependencies first\n",
    "!pip install pickleshare structlog -q\n",
    "\n",
    "if not os.path.exists('Apex-X'):\n",
    "    !git clone https://github.com/Voskan/Apex-X.git\n",
    "    print('âœ… Repository cloned')\n",
    "else:\n",
    "    !cd Apex-X && git pull\n",
    "    print('âœ… Repository updated')\n",
    "\n",
    "%cd Apex-X\n",
    "!pip install -e . -q\n",
    "!pip install pycocotools albumentations matplotlib seaborn tqdm transformers timm peft -q\n",
    "print('\\nâœ… Environment Ready')\n",
    "\n",
    "import torch, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from apex_x.model import TeacherModel\n",
    "from apex_x.train.checkpoint import safe_torch_load, extract_model_state_dict\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_PATH = '/workspace/Apex-X/artifacts/best.pt' # Change to your checkpoint\n",
    "IMAGE_PATH = '/workspace/test.png' # Change to your input image\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def load_model(path):\n",
    "    # Initialize Base TeacherModel (matches checkpoint keys: pv_module, seg_head, etc.)\n",
    "    # We need to guess standard config channels or load from config if available.\n",
    "    # Default fallback: p3=16, p4=24, p5=32 (from trainer.py defaults)\n",
    "    # But trainer.py uses ApexXConfig. \n",
    "    # Let's verify channels from checkpoint if possible, but 16/24/32 is standard small default.\n",
    "    \n",
    "    print(f\"ðŸ”¹ Loading TeacherModel (V1/Base) from {path}...\")\n",
    "    model = TeacherModel(\n",
    "        num_classes=23,  # Verified from user requests\n",
    "        feature_layers=(\"P3\", \"P4\", \"P5\"),\n",
    "        enable_seg_head=True,\n",
    "        seg_num_instances=100, # Request up to 100 masks\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            payload = safe_torch_load(path, map_location=DEVICE)\n",
    "            state_dict, _ = extract_model_state_dict(payload)\n",
    "            \n",
    "            # DEBUG: Check key matching\n",
    "            model_keys = set(model.state_dict().keys())\n",
    "            ckpt_keys = set(state_dict.keys())\n",
    "            missing = model_keys - ckpt_keys\n",
    "            unexpected = ckpt_keys - model_keys\n",
    "            print(f\"ðŸ” Weight mismatch analysis:\")\n",
    "            print(f\"   - Missing keys in checkpoint: {len(missing)}\")\n",
    "            if len(missing) > 0:\n",
    "                print(f\"   - Example missing: {list(missing)[:5]}\")\n",
    "            print(f\"   - Unexpected keys in checkpoint: {len(unexpected)}\")\n",
    "            \n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"âœ… Checkpoint loaded: {path}\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"âŒ Error loading weights: {e}\")\n",
    "            print(\"âš ï¸ Model running with RANDOM weights!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Checkpoint not found. Running with random weights.\")\n",
    "    return model.eval()\n",
    "\n",
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7849f",
   "metadata": {},
   "source": [
    "## 2. Advanced Inference & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(img_path, zoom_region=None):\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"âŒ Image not found: {img_path}\")\n",
    "        return\n",
    "        \n",
    "    import torchvision.transforms as T\n",
    "    import torchvision.ops as ops\n",
    "    \n",
    "    # 1. Standard ImageNet Normalization\n",
    "    transform = T.Compose([\n",
    "        T.Resize((1024, 1024)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img_orig = Image.open(img_path).convert('RGB')\n",
    "    img_vis = img_orig.resize((1024, 1024))\n",
    "    \n",
    "    x = transform(img_orig).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        # TeacherModel returns TeacherDistillOutput\n",
    "        out = model(x)\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    # Extract Prototypical Masks\n",
    "    # Out.masks is [B, N, H, W] if segmentations were proposed\n",
    "    masks = out.masks\n",
    "    if masks is None:\n",
    "        print(\"âš ï¸ No masks generated by the model (low confidence or disabled).\")\n",
    "        masks = torch.zeros((1, 0, 1024, 1024), device=DEVICE)\n",
    "    \n",
    "    # We need scores to filter. \n",
    "    # TeacherDistillOutput has 'logits' [B, L] flattened.\n",
    "    # We need structured instance scores. \n",
    "    # TeacherModel._propose_segmentation_boxes calculates them internally but doesn't return them directly in 'out'.\n",
    "    # However, 'masks' contains the top-K instances selected by score.\n",
    "    # We can infer valid masks by checking if they are not all zero?\n",
    "    # Or we assume all returned masks are valid candidates.\n",
    "    \n",
    "    # Let's visualize ALL returned masks for now.\n",
    "    print(f\"ðŸ” Generated {masks.shape[1]} mask candidates.\")\n",
    "    \n",
    "    # Since we don't have explicit scores for the *returned* masks (internal logic),\n",
    "    # we will trust the model's internal top-k selection.\n",
    "    \n",
    "    masks_np = masks[0].cpu().numpy() # [N, H, W]\n",
    "    \n",
    "    combined_mask = np.zeros((1024, 1024), dtype=bool)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(masks_np.shape[0]):\n",
    "        m = masks_np[i]\n",
    "        # Threshold mask (sigmoid output)\n",
    "        m_bin = m > 0.5\n",
    "        \n",
    "        if m_bin.sum() > 10: # Min pixel area\n",
    "            combined_mask = np.logical_or(combined_mask, m_bin)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"âœ… Visualizing {count} valid masks.\")\n",
    "    \n",
    "    # --- VISUALIZATION UPDATE: Overlay on Original Image ---\n",
    "    img_np = np.array(img_vis)\n",
    "    overlay = np.zeros_like(img_np)\n",
    "    overlay[combined_mask] = [255, 0, 0] # Red color for mask\n",
    "    \n",
    "    mask_layer = Image.new(\"RGBA\", (1024, 1024), (255, 0, 0, 0))\n",
    "    mask_draw = np.array(mask_layer)\n",
    "    mask_draw[combined_mask, 3] = 128 # 50% opacity\n",
    "    mask_layer = Image.fromarray(mask_draw)\n",
    "    \n",
    "    img_vis_rgba = img_vis.convert(\"RGBA\")\n",
    "    final_comp = Image.alpha_composite(img_vis_rgba, mask_layer)\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_vis); plt.title(f\"Input | Latency: {latency:.1f}ms\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(final_comp)\n",
    "    plt.title(f\"Model Predictions ({count} objects)\")\n",
    "    \n",
    "    if zoom_region:\n",
    "        zx, zy, zs = zoom_region\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(final_comp.crop((zx, zy, zx+zs, zy+zs)))\n",
    "        plt.title(\"Sub-pixel Integrity Detail\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "run_inference(IMAGE_PATH, zoom_region=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a397d0",
   "metadata": {},
   "source": [
    "## 4. Production Benchmarking (Cloud/Edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(resolutions=[512, 1024, 1536]):\n",
    "    print(f\"Benchmarking on {torch.cuda.get_device_name(0)}...\")\n",
    "    for res in resolutions:\n",
    "        x = torch.randn(1, 3, res, res).to(DEVICE)\n",
    "        # Warmup\n",
    "        for _ in range(10): _ = model(x)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        for _ in range(50): _ = model(x)\n",
    "        torch.cuda.synchronize()\n",
    "        avg_ms = ((time.time() - t0) / 50) * 1000\n",
    "        fps = 1000 / avg_ms\n",
    "        print(f\"Resolution: {res}x{res} | Latency: {avg_ms:.2f}ms | Throughput: {fps:.1f} FPS\")\n",
    "\n",
    "# benchmark_performance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
