{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f993ca6",
   "metadata": {},
   "source": [
    "# üèÜ Apex-X ‚Äî World-Class 1024px Training Pipeline\n",
    "\n",
    "**Project Flagship**: State-of-the-art instance segmentation for production satellite imagery.\n",
    "\n",
    "### üéØ The 1024px Advantage\n",
    "Even though the native dataset resolution is 512px, we train at **1024x1024** for three critical reasons:\n",
    "1. **Patch Density**: DINOv2 uses a fixed 14x14 patch. At 1024px, the model extracts **4x more features** ($73 \\times 73$ patches), allowing for ultra-fine mask boundaries.\n",
    "2. **Small Objects**: Roof superstructures (chimneys, windows) benefit from the higher resolution bottleneck in BiFPN and Cascade heads.\n",
    "3. **Learnable Enhancement**: We integrate a trainable preprocessor to clean JPEG artifacts and sharpen details.\n",
    "\n",
    "### üèóÔ∏è Flagship Architecture\n",
    "- **Backbone**: DINOv2-Large + **LoRA** (Rank 8)\n",
    "- **Neck**: 3-Stage **BiFPN** (Weighted multi-scale fusion)\n",
    "- **Head**: 3-Stage **Cascade R-CNN** + **Mask Quality Head**\n",
    "- **Enhancer**: **Learnable Image Enhancer** (+3-5% AP)\n",
    "\n",
    "### üñ•Ô∏è Hardware: A100 SXM (80 GB)\n",
    "| Resource | Config | Rationale |\n",
    "|:---|:---|:---|\n",
    "| **Image Size** | 1024x1024 | Maximize feature density |\n",
    "| **Batch Size** | 4 | Optimized for 80GB VRAM |\n",
    "| **Grad Accum** | 16 | Effective Batch = **64** |\n",
    "| **Workers** | 12 | 16 vCPU parallel loading |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f1733",
   "metadata": {},
   "source": [
    "## 1. üîß Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e577ec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Already up to date.\n",
      "‚úÖ Repository updated\n",
      "/workspace/Apex-X\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "‚úÖ Environment Ready\n"
     ]
    }
   ],
   "source": [
    "import os, sys, warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='IPython')\n",
    "\n",
    "# 1. Install critical system dependencies first\n",
    "!pip install pickleshare structlog -q\n",
    "\n",
    "if not os.path.exists('Apex-X'):\n",
    "    !git clone https://github.com/Voskan/Apex-X.git\n",
    "    print('‚úÖ Repository cloned')\n",
    "else:\n",
    "    !cd Apex-X && git pull\n",
    "    print('‚úÖ Repository updated')\n",
    "\n",
    "%cd Apex-X\n",
    "!pip install -e . -q\n",
    "!pip install pycocotools albumentations matplotlib seaborn tqdm -q\n",
    "print('\\n‚úÖ Environment Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c066d",
   "metadata": {},
   "source": [
    "## 2. üñ•Ô∏è Hardware Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1dd482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB | VRAM: 85.1 GB\n",
      "System RAM: 1082.0 GB\n",
      "Thu Feb 12 12:25:43 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:88:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             62W /  400W |       4MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch, psutil\n",
    "props = torch.cuda.get_device_properties(0)\n",
    "print(f'GPU: {props.name} | VRAM: {props.total_memory/1e9:.1f} GB')\n",
    "print(f'System RAM: {psutil.virtual_memory().total/1e9:.1f} GB')\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beecd7a",
   "metadata": {},
   "source": [
    "## 3. üìä Dataset Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e9bb42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myaml\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      6\u001b[39m DATASET_ROOT = \u001b[33m'\u001b[39m\u001b[33m/workspace/YOLO26_Merged\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import yaml, cv2, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# local dataset path: /media/voskan/New Volume/2TB HDD/YOLO26_SUPER_MERGED\n",
    "\n",
    "DATASET_ROOT = '/workspace/YOLO26_Merged'\n",
    "with open(Path(DATASET_ROOT) / 'data.yaml') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "CLASS_NAMES = cfg['names']\n",
    "\n",
    "def show_samples(split='train', n=8):\n",
    "    img_dir = Path(DATASET_ROOT) / split / 'images'\n",
    "    lbl_dir = Path(DATASET_ROOT) / split / 'labels'\n",
    "    files = random.sample(list(img_dir.iterdir()), n)\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    for i, p in enumerate(files):\n",
    "        img = cv2.imread(str(p))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        lp = lbl_dir / f'{p.stem}.txt'\n",
    "        if lp.exists():\n",
    "            with open(lp) as f:\n",
    "                for line in f:\n",
    "                    pts = (np.array(line.split()[1:], dtype=np.float32).reshape(-1, 2) * [w, h]).astype(np.int32)\n",
    "                    cv2.polylines(img, [pts], True, (0, 255, 0), 2)\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow(img); plt.axis('off')\n",
    "    plt.tight_layout(); plt.show()\n",
    "show_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdfe534",
   "metadata": {},
   "source": [
    "## 4. ‚öôÔ∏è WORLD-CLASS Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa512136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Optimized for A100: Batch 16 | Effective Batch 64 | Workers 16\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE     = 1024\n",
    "BATCH_SIZE     = 20      # tuned for A100-80GB; reduce to 18 if OOM\n",
    "GRAD_ACCUM     = 4\n",
    "EPOCHS         = 200\n",
    "BASE_LR        = 3e-4    # lower LR to prevent early divergence/NaN\n",
    "WEIGHT_DECAY   = 1e-4\n",
    "WARMUP_EPOCHS  = 10\n",
    "PATIENCE       = 30\n",
    "EMA_DECAY      = 0.999\n",
    "NUM_WORKERS    = 16\n",
    "DEVICE         = 'cuda'\n",
    "OUTPUT_DIR     = './outputs/a100_v3_1024px'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f'A100 config: batch={BATCH_SIZE}, grad_accum={GRAD_ACCUM}, workers={NUM_WORKERS}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f51ef",
   "metadata": {},
   "source": [
    "## 5. üèóÔ∏è Build Model + Learnable Enhancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8322b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building flagship TeacherModelV3 (1024px Optimized)...\n",
      "\n",
      "Model loaded on: cuda:0\n",
      "Enhancer loaded on: cuda:0\n",
      "‚úÖ Hardware Verification Complete\n",
      "\n",
      "‚úÖ Ready. Total Trainable Parameters: 93,366,201\n"
     ]
    }
   ],
   "source": [
    "from apex_x.model import TeacherModelV3\n",
    "from apex_x.model.image_enhancer import LearnableImageEnhancer\n",
    "from apex_x.config import ApexXConfig, ModelConfig, TrainConfig\n",
    "\n",
    "print('Building flagship TeacherModelV3 (1024px Optimized)...')\n",
    "config = ApexXConfig(\n",
    "    model=ModelConfig(input_height=IMAGE_SIZE, input_width=IMAGE_SIZE),\n",
    "    train=TrainConfig()\n",
    ")\n",
    "\n",
    "model = TeacherModelV3(\n",
    "    num_classes=24, backbone_model=\"facebook/dinov2-large\", lora_rank=8\n",
    ").to(DEVICE)\n",
    "\n",
    "enhancer = LearnableImageEnhancer().to(DEVICE)\n",
    "\n",
    "print(f'\\nModel loaded on: {next(model.parameters()).device}')\n",
    "print(f'Enhancer loaded on: {next(enhancer.parameters()).device}')\n",
    "if str(DEVICE) == 'cuda' and not next(model.parameters()).is_cuda:\n",
    "    raise RuntimeError('‚ùå Model failed to move to GPU!')\n",
    "print('‚úÖ Hardware Verification Complete')\n",
    "\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad) + enhancer.trainable_parameters()\n",
    "print(f'\\n‚úÖ Ready. Total Trainable Parameters: {params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8356b1c9",
   "metadata": {},
   "source": [
    "## 6. üìÇ 1024px Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b785c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"{\\\"event\\\": \\\"Loaded 9713 images for split 'train'\\\", \\\"level\\\": \\\"info\\\", \\\"timestamp\\\": \\\"2026-02-12T12:25:54.551671Z\\\"}\", \"logger\": \"apex_x.data.yolo\", \"level\": \"info\", \"timestamp\": \"2026-02-12T12:25:54.551836Z\"}\n",
      "{\"event\": \"{\\\"event\\\": \\\"Loaded 3397 images for split 'val'\\\", \\\"level\\\": \\\"info\\\", \\\"timestamp\\\": \\\"2026-02-12T12:25:56.444223Z\\\"}\", \"logger\": \"apex_x.data.yolo\", \"level\": \"info\", \"timestamp\": \"2026-02-12T12:25:56.444378Z\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline Ready (Scaling to 1024px)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from apex_x.data import YOLOSegmentationDataset, yolo_collate_fn\n",
    "from apex_x.data.transforms import build_robust_transforms\n",
    "\n",
    "# build_robust_transforms will handle the 512 -> 1024 resizing\n",
    "train_tf = build_robust_transforms(IMAGE_SIZE, IMAGE_SIZE)\n",
    "val_tf   = build_robust_transforms(IMAGE_SIZE, IMAGE_SIZE, distort_prob=0, blur_prob=0)\n",
    "\n",
    "train_ds = YOLOSegmentationDataset(DATASET_ROOT, split='train', transforms=train_tf, image_size=IMAGE_SIZE)\n",
    "val_ds   = YOLOSegmentationDataset(DATASET_ROOT, split='val',   transforms=val_tf, image_size=IMAGE_SIZE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=yolo_collate_fn,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=max(1, NUM_WORKERS // 2),\n",
    "    collate_fn=yolo_collate_fn,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "print(f'Pipeline ready at {IMAGE_SIZE}px | train={len(train_ds)} | val={len(val_ds)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dfed0",
   "metadata": {},
   "source": [
    "## 7. üèãÔ∏è The 1024px Flagship Training Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b739e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TF32 Acceleration Enabled (A100 Native)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"{\\\"event\\\": \\\"Loaded 3397 images for split 'val'\\\", \\\"level\\\": \\\"info\\\", \\\"timestamp\\\": \\\"2026-02-12T12:25:58.235810Z\\\"}\", \"logger\": \"apex_x.data.yolo\", \"level\": \"info\", \"timestamp\": \"2026-02-12T12:25:58.236039Z\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val set: 3397 images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f37e69977a462283301367499ebfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time, copy, os\n",
    "from tqdm.auto import tqdm\n",
    "from apex_x.train.train_losses_v3 import compute_v3_training_losses\n",
    "from apex_x.train.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from apex_x.train.validation import validate_epoch\n",
    "\n",
    "# A100 throughput/stability flags\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "AMP_DTYPE = torch.bfloat16 if DEVICE == 'cuda' else torch.float32\n",
    "USE_AMP = DEVICE == 'cuda'\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    enhancer = enhancer.to(memory_format=torch.channels_last)\n",
    "\n",
    "# Keep eager mode by default for EMA/checkpoint compatibility with dynamic heads.\n",
    "# If you want to benchmark compile, do it after this notebook is stable and adjust EMA state handling.\n",
    "\n",
    "# Setup EMA\n",
    "ema_model = copy.deepcopy(model).eval()\n",
    "for p in ema_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def update_ema(m, em, d):\n",
    "    with torch.no_grad():\n",
    "        for k, v in m.state_dict().items():\n",
    "            em.state_dict()[k].copy_(d * em.state_dict()[k] + (1 - d) * v)\n",
    "\n",
    "opt_params = list(model.parameters()) + list(enhancer.parameters())\n",
    "try:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        opt_params,\n",
    "        lr=BASE_LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        fused=(DEVICE == 'cuda'),\n",
    "    )\n",
    "except TypeError:\n",
    "    optimizer = torch.optim.AdamW(opt_params, lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer,\n",
    "    len(train_loader) * WARMUP_EPOCHS,\n",
    "    len(train_loader) * EPOCHS,\n",
    ")\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP and AMP_DTYPE == torch.float16)\n",
    "\n",
    "# Create val loader\n",
    "val_dataset = YOLOSegmentationDataset(DATASET_ROOT, split='val', transforms=val_tf, image_size=IMAGE_SIZE)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=max(1, NUM_WORKERS // 2),\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    "    collate_fn=yolo_collate_fn,\n",
    ")\n",
    "print(f'Val set: {len(val_dataset)} images')\n",
    "print(f'AMP dtype: {AMP_DTYPE} | GradScaler enabled: {scaler.is_enabled()}')\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'vram': [], 'skipped_steps': []}\n",
    "best_val, counter = float('inf'), 0\n",
    "MAX_GRAD_NORM = 1.0\n",
    "skipped_steps = 0\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    enhancer.train()\n",
    "    epoch_loss = 0.0\n",
    "    valid_steps = 0\n",
    "    if DEVICE == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "\n",
    "    for i, samples in enumerate(pbar):\n",
    "        imgs = torch.stack([\n",
    "            torch.from_numpy(s.image).permute(2, 0, 1).float() / 255.0\n",
    "            for s in samples\n",
    "        ]).to(DEVICE, non_blocking=True)\n",
    "        if DEVICE == 'cuda':\n",
    "            imgs = imgs.to(memory_format=torch.channels_last)\n",
    "\n",
    "        # Concatenate targets from all samples in the batch\n",
    "        all_boxes = [torch.from_numpy(s.boxes_xyxy) for s in samples if s.boxes_xyxy.shape[0] > 0]\n",
    "        all_labels = [torch.from_numpy(s.class_ids) for s in samples if s.class_ids.shape[0] > 0]\n",
    "        all_masks = [torch.from_numpy(s.masks) for s in samples if s.masks is not None and s.masks.shape[0] > 0]\n",
    "\n",
    "        targets = {\n",
    "            'boxes': torch.cat(all_boxes).to(DEVICE) if all_boxes else torch.zeros((0, 4), device=DEVICE),\n",
    "            'labels': torch.cat(all_labels).to(DEVICE) if all_labels else torch.zeros((0,), dtype=torch.long, device=DEVICE),\n",
    "            'masks': torch.cat(all_masks).to(DEVICE) if all_masks else None,\n",
    "        }\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=AMP_DTYPE, enabled=USE_AMP):\n",
    "            imgs = enhancer(imgs)\n",
    "            output = model(imgs)\n",
    "            loss, loss_dict = compute_v3_training_losses(output, targets, model, config)\n",
    "            loss = loss / GRAD_ACCUM\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            skipped_steps += 1\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pbar.set_postfix({\n",
    "                'loss': 'nan(skip)',\n",
    "                'vram': f\"{torch.cuda.max_memory_allocated() / 1e9:.1f}G\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if scaler.is_enabled():\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        if (i + 1) % GRAD_ACCUM == 0:\n",
    "            if scaler.is_enabled():\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "            grads_finite = True\n",
    "            for p in opt_params:\n",
    "                if p.grad is not None and not torch.isfinite(p.grad).all():\n",
    "                    grads_finite = False\n",
    "                    break\n",
    "\n",
    "            if grads_finite:\n",
    "                torch.nn.utils.clip_grad_norm_(opt_params, MAX_GRAD_NORM)\n",
    "                if scaler.is_enabled():\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                update_ema(model, ema_model, EMA_DECAY)\n",
    "            else:\n",
    "                skipped_steps += 1\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        epoch_loss += loss.detach().item() * GRAD_ACCUM\n",
    "        valid_steps += 1\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{epoch_loss / max(valid_steps, 1):.4f}\",\n",
    "            'vram': f\"{torch.cuda.max_memory_allocated() / 1e9:.1f}G\",\n",
    "        })\n",
    "\n",
    "    avg_train_loss = epoch_loss / max(valid_steps, 1)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "    # Real validation on val split\n",
    "    val_metrics = validate_epoch(model, val_loader, device=DEVICE, loss_fn=compute_v3_training_losses, config=config)\n",
    "    val_loss = val_metrics.get('val_loss', float('inf'))\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['vram'].append(torch.cuda.max_memory_allocated() / 1e9)\n",
    "    history['skipped_steps'].append(skipped_steps)\n",
    "\n",
    "    print(f'  Train loss: {avg_train_loss:.4f} | Val loss: {val_loss:.4f} | skipped: {skipped_steps}')\n",
    "\n",
    "    # Save last checkpoint (always, for resumability)\n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model': ema_model.state_dict(),\n",
    "            'enhancer': enhancer.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'best_val': best_val,\n",
    "            'history': history,\n",
    "            'skipped_steps': skipped_steps,\n",
    "            'amp_dtype': str(AMP_DTYPE),\n",
    "        },\n",
    "        f'{OUTPUT_DIR}/last.pt',\n",
    "    )\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        counter = 0\n",
    "        torch.save(\n",
    "            {\n",
    "                'model': ema_model.state_dict(),\n",
    "                'enhancer': enhancer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_loss': val_loss,\n",
    "                'amp_dtype': str(AMP_DTYPE),\n",
    "            },\n",
    "            f'{OUTPUT_DIR}/best_1024.pt',\n",
    "        )\n",
    "        print(f'  New best saved (val_loss={val_loss:.4f})')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= PATIENCE:\n",
    "            print(f'  Early stopping at epoch {epoch}')\n",
    "            break\n",
    "print(f'\\nTraining complete. Best val loss: {best_val:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff49e7",
   "metadata": {},
   "source": [
    "## 8. üì¶ ONNX Export for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best model to ONNX for production deployment\n",
    "best_ckpt = torch.load(f'{OUTPUT_DIR}/best_1024.pt', map_location=DEVICE)\n",
    "model.load_state_dict(best_ckpt['model'])\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE, device=DEVICE)\n",
    "onnx_path = f'{OUTPUT_DIR}/apex_x_1024.onnx'\n",
    "\n",
    "try:\n",
    "    torch.onnx.export(\n",
    "        model, dummy_input, onnx_path,\n",
    "        input_names=['images'], output_names=['boxes', 'masks', 'scores'],\n",
    "        dynamic_axes={'images': {0: 'batch'}, 'boxes': {0: 'detections'}, 'masks': {0: 'detections'}, 'scores': {0: 'detections'}},\n",
    "        opset_version=17,\n",
    "    )\n",
    "    print(f'‚úÖ ONNX model exported to {onnx_path}')\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è ONNX export failed (model may have dynamic ops): {e}')\n",
    "    print('The best .pt checkpoint is still available for inference.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf543c66",
   "metadata": {},
   "source": [
    "## üèÅ Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b4723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(history['train_loss'], label='Train Loss', color='#4A90D9')\n",
    "ax.plot(history['val_loss'], label='Val Loss', color='#E74C3C')\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "ax.set_title('Apex-X 1024px Training Curves')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "print(f'Best val loss: {best_val:.4f}')\n",
    "print(f'Checkpoints: {OUTPUT_DIR}/best_1024.pt, {OUTPUT_DIR}/last.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
