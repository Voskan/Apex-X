{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸš€ Apex-X Training on A100 SXM (80GB)\n",
                "\n",
                "This notebook is optimized for training Apex-X on large-scale datasets using **NVIDIA A100 80GB**. \n",
                "\n",
                "### ðŸ—ï¸ Hardware Specs:\n",
                "- **GPU**: 1x A100 SXM (80 GB VRAM)\n",
                "- **RAM**: 117 GB\n",
                "- **CPU**: 16 vCPU\n",
                "\n",
                "### ðŸŽ¯ Dataset:\n",
                "- **Path**: `/media/voskan/New Volume/2TB HDD/YOLO26_SUPER_MERGED`\n",
                "- **Format**: YOLO Segmentation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repository if not already present\n",
                "import os\n",
                "if not os.path.exists('Apex-X'):\n",
                "    !git clone https://github.com/Voskan/Apex-X.git\n",
                "\n",
                "%cd Apex-X\n",
                "\n",
                "# Install dependencies\n",
                "!pip install -e .\n",
                "!pip install pycocotools albumentations wandb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration\n",
                "\n",
                "Optimized for 80GB VRAM. We can use a large batch size and high-resolution images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from apex_x.data import YOLOSegmentationDataset, build_robust_transforms, MosaicAugmentation, MixUpAugmentation\n",
                "from apex_x.model import TeacherModel\n",
                "from apex_x.config import ApexXConfig\n",
                "from apex_x.train import ApexXTrainer, LinearWarmupCosineAnnealingLR\n",
                "from torch.utils.data import DataLoader\n",
                "from pathlib import Path\n",
                "\n",
                "# --- Hyperparameters ---\n",
                "DATASET_ROOT = \"/media/voskan/New Volume/2TB HDD/YOLO26_SUPER_MERGED\"\n",
                "IMAGE_SIZE = 1024  # High res for satellite imagery\n",
                "BATCH_SIZE = 64    # Optimized for A100 80GB\n",
                "EPOCHS = 300\n",
                "BASE_LR = 0.01\n",
                "WEIGHT_DECAY = 0.0005\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "OUTPUT_DIR = \"./outputs/a100_training\"\n",
                "\n",
                "print(f\"Device: {DEVICE}\")\n",
                "if DEVICE == \"cuda\":\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading\n",
                "\n",
                "Using the native `YOLOSegmentationDataset` with Mosaic and MixUp augmentations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build Base Dataset\n",
                "train_dataset = YOLOSegmentationDataset(\n",
                "    root=DATASET_ROOT, \n",
                "    split=\"train\",\n",
                "    image_size=IMAGE_SIZE\n",
                ")\n",
                "\n",
                "# Apply Advanced Augmentations\n",
                "print(\"Enabling Mosaic & MixUp...\")\n",
                "train_dataset.transforms = MosaicAugmentation(\n",
                "    dataset=train_dataset,\n",
                "    output_size=IMAGE_SIZE,\n",
                "    mosaic_prob=0.5\n",
                ")\n",
                "\n",
                "# Standard Albumentations Pipeline\n",
                "standard_transforms = build_robust_transforms(image_size=IMAGE_SIZE, is_training=True)\n",
                "\n",
                "# Patch-based training (Optional but recommended for memory efficiency if increasing resolution further)\n",
                "# For 1024px on A100, we can stay full-frame or use patches.\n",
                "\n",
                "val_dataset = YOLOSegmentationDataset(\n",
                "    root=DATASET_ROOT,\n",
                "    split=\"val\",\n",
                "    image_size=IMAGE_SIZE,\n",
                "    transforms=build_robust_transforms(image_size=IMAGE_SIZE, is_training=False)\n",
                ")\n",
                "\n",
                "train_loader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=True,\n",
                "    num_workers=16, # Optimized for 16 vCPUs\n",
                "    pin_memory=True\n",
                ")\n",
                "\n",
                "val_loader = DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=False,\n",
                "    num_workers=8,\n",
                "    pin_memory=True\n",
                ")\n",
                "\n",
                "print(f\"Train batches: {len(train_loader)}\")\n",
                "print(f\"Val batches: {len(val_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model & Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = ApexXConfig(\n",
                "    num_classes=train_dataset.num_classes,\n",
                "    image_size=IMAGE_SIZE\n",
                ")\n",
                "\n",
                "model = TeacherModel(config).to(DEVICE)\n",
                "\n",
                "trainer = ApexXTrainer(\n",
                "    teacher=model,\n",
                "    device=DEVICE,\n",
                "    checkpoint_dir=Path(OUTPUT_DIR) / \"checkpoints\"\n",
                ")\n",
                "\n",
                "optimizer = torch.optim.AdamW(\n",
                "    model.parameters(),\n",
                "    lr=BASE_LR,\n",
                "    weight_decay=WEIGHT_DECAY\n",
                ")\n",
                "\n",
                "scheduler = LinearWarmupCosineAnnealingLR(\n",
                "    optimizer=optimizer,\n",
                "    warmup_epochs=10,\n",
                "    total_epochs=EPOCHS,\n",
                "    eta_min=BASE_LR * 0.01\n",
                ")\n",
                "\n",
                "# Mixed Precision Training (Automatic)\n",
                "scaler = torch.cuda.amp.GradScaler() if DEVICE == \"cuda\" else None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import wandb\n",
                "wandb.init(project=\"Apex-X-A100-SXM\", config={\n",
                "    \"batch_size\": BATCH_SIZE,\n",
                "    \"epochs\": EPOCHS,\n",
                "    \"lr\": BASE_LR,\n",
                "    \"image_size\": IMAGE_SIZE\n",
                "})\n",
                "\n",
                "best_map = 0.0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    # Train phase\n",
                "    train_metrics = trainer.train_epoch(\n",
                "        train_loader,\n",
                "        optimizer,\n",
                "        epoch=epoch,\n",
                "        total_epochs=EPOCHS,\n",
                "        scaler=scaler  # Enables FP16\n",
                "    )\n",
                "    \n",
                "    scheduler.step()\n",
                "    \n",
                "    # Validate\n",
                "    if (epoch + 1) % 5 == 0:\n",
                "        val_metrics = trainer.validate(val_loader, compute_map=True)\n",
                "        current_map = val_metrics.get(\"mAP_bbox\", 0.0)\n",
                "        \n",
                "        if current_map > best_map:\n",
                "            best_map = current_map\n",
                "            trainer.save_checkpoint(\n",
                "                Path(OUTPUT_DIR) / \"checkpoints\" / \"best.pt\",\n",
                "                epoch=epoch,\n",
                "                metrics=val_metrics\n",
                "            )\n",
                "        \n",
                "        wandb.log({\"val/mAP\": current_map, \"val/loss\": val_metrics.get(\"val_loss\")})\n",
                "    \n",
                "    wandb.log(train_metrics)\n",
                "\n",
                "wandb.finish()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}