{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Apex-X Ascension V5 Professional Training Suite\n",
    "\n",
    "**The production-grade engine for world-class segmentation.**\n",
    "\n",
    "This notebook provides a complete E2E pipeline for:\n",
    "1. **Dataset Exploratory Data Analysis (EDA)**: Understand your data bias and statistics.\n",
    "2. **SOTA Ascension V5 Training**: Stable, high-precision training loop.\n",
    "3. **Production Export (ONNX)**: Deploy your model with dynamic axes support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762b43de",
   "metadata": {},
   "source": [
    "## 1. System Setup & Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='IPython')\n",
    "\n",
    "# 1. Install critical system dependencies first\n",
    "!pip install pickleshare structlog -q\n",
    "\n",
    "if not os.path.exists('Apex-X'):\n",
    "    !git clone https://github.com/Voskan/Apex-X.git\n",
    "    print('‚úÖ Repository cloned')\n",
    "else:\n",
    "    !cd Apex-X && git pull\n",
    "    print('‚úÖ Repository updated')\n",
    "\n",
    "%cd Apex-X\n",
    "!pip install -e . -q\n",
    "!pip install pycocotools albumentations matplotlib seaborn tqdm transformers timm peft -q\n",
    "print('\\n‚úÖ Environment Ready')\n",
    "\n",
    "import torch, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "try:\n",
    "    import triton\n",
    "    print(f\"‚úÖ Triton: {triton.__version__}\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Triton not found. Using CPU fallback for Geometrical Branch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Statistical Intelligence (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex_x.data import YOLOSegmentationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DATASET_ROOT = '/workspace/YOLO26_Merged'\n",
    "IMAGE_SIZE = 1024\n",
    "\n",
    "ds = YOLOSegmentationDataset(DATASET_ROOT, split='train', image_size=IMAGE_SIZE)\n",
    "print(f\"Analyzing {len(ds)} images...\")\n",
    "\n",
    "class_counts = {}\n",
    "instance_counts = []\n",
    "mask_areas = []\n",
    "\n",
    "# Sample 10% for speed if massive, else full\n",
    "sample_indices = np.random.choice(len(ds), min(500, len(ds)), replace=False)\n",
    "\n",
    "for idx in tqdm(sample_indices):\n",
    "    sample = ds[idx]\n",
    "    cids = sample.class_ids\n",
    "    instance_counts.append(len(cids))\n",
    "    for cid in cids:\n",
    "        class_counts[cid] = class_counts.get(cid, 0) + 1\n",
    "    \n",
    "    if sample.masks is not None:\n",
    "        # Calculate relative area\n",
    "        areas = sample.masks.sum(axis=(1, 2)) / (IMAGE_SIZE**2)\n",
    "        mask_areas.extend(areas.tolist())\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.title(\"Class Distribution\"); plt.xlabel(\"Class ID\"); plt.ylabel(\"Instances\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(instance_counts, bins=20)\n",
    "plt.title(\"Instances per Image\"); plt.xlabel(\"Count\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(mask_areas, bins=50, color='orange')\n",
    "plt.title(\"Relative Mask Area\"); plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced SOTA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex_x.config import ApexXConfig, ModelConfig, TrainConfig, LossConfig\n",
    "from apex_x.data import standard_collate_fn\n",
    "from apex_x.data.transforms import build_robust_transforms\n",
    "from apex_x.model import TeacherModelV5\n",
    "\n",
    "# Hyperparameters\n",
    "config = ApexXConfig(\n",
    "    model=ModelConfig(input_height=IMAGE_SIZE, input_width=IMAGE_SIZE),\n",
    "    train=TrainConfig(\n",
    "        batch_size=16, \n",
    "        epochs=200, \n",
    "        lr=1e-4, \n",
    "        weight_decay=0.05,\n",
    "        grad_accum=16\n",
    "    ),\n",
    "    loss=LossConfig(\n",
    "        topological_persistence=True,   # Stability Fix Applied\n",
    "        flow_symmetry=True,             # Physics-informed boundaries\n",
    "        self_distillation=True          # Recursive refinement\n",
    "    )\n",
    ")\n",
    "\n",
    "train_tf = build_robust_transforms(IMAGE_SIZE, IMAGE_SIZE)\n",
    "train_loader = DataLoader(ds, batch_size=config.train.batch_size, shuffle=True, \n",
    "                          collate_fn=standard_collate_fn, num_workers=4)\n",
    "\n",
    "model = TeacherModelV5(num_classes=ds.num_classes).to('cuda')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.train.lr, weight_decay=config.train.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.train.epochs)\n",
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Professional Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex_x.train.train_losses_v5 import compute_v5_training_losses\n",
    "from apex_x.train.validation import validate_epoch\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(config.train.epochs):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.train.epochs}\")\n",
    "    \n",
    "    for i, batch in enumerate(pbar):\n",
    "        imgs = batch['images'].to('cuda')\n",
    "        targets = {k: v.to('cuda') if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(imgs)\n",
    "            loss, loss_dict = compute_v5_training_losses(outputs, targets, model, config)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i+1) % config.train.grad_accum == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        pbar.set_postfix({k: f\"{v.item():.3f}\" for k, v in loss_dict.items() if 'loss' not in k})\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation\n",
    "    # Note: val_loader needs standard_collate_fn too!\n",
    "    with torch.no_grad():\n",
    "        # Assuming you have a val_loader check step 3\n",
    "        # val_metrics = validate_epoch(model, val_loader, device='cuda', loss_fn=compute_v5_training_losses, config=config)\n",
    "        # print(f\"\\n‚≠ê Epoch {epoch+1} Results | Val Loss: {val_metrics['val_loss']:.4f}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Production Handoff: ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex_x.export.onnx_export import export_to_onnx, verify_onnx_model\n",
    "\n",
    "onnx_path = \"artifacts/ascension_v5_flagship.onnx\"\n",
    "\n",
    "class ONNXWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        # Flatten DICT to tuple for ONNX compatibility\n",
    "        return out['boxes'], out['scores'], out['masks']\n",
    "\n",
    "wrapper = ONNXWrapper(model).cpu().eval()\n",
    "\n",
    "export_to_onnx(\n",
    "    wrapper,\n",
    "    onnx_path,\n",
    "    input_shape=(1, 3, IMAGE_SIZE, IMAGE_SIZE),\n",
    "    opset_version=17,\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size', 2: 'height', 3: 'width'},\n",
    "        'output1': {0: 'batch_size'}, # boxes\n",
    "        'output2': {0: 'batch_size'}, # scores\n",
    "        'output3': {0: 'batch_size'}, # masks\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\nüéÅ Ascension V5 ONNX Suite Ready at {onnx_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
